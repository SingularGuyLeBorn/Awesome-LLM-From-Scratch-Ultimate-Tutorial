# 理论解析: 字节对编码 (Byte Pair Encoding, BPE)

## 1. 为什么需要分词 (Tokenization)？

大语言模型（LLM）无法直接理解人类的文本。它们处理的是数字，即Token ID。分词就是将原始文本字符串转换为模型可以理解的Token ID序列的过程。选择合适的分词粒度至关重要：

-   **词级别 (Word-level)**: 
    -   优点: 语义信息丰富。
    -   缺点: 词表会变得异常庞大（数百万），且无法处理词表中没有的词（Out-of-Vocabulary, OOV），例如新词、拼写错误或专有名词。
-   **字符级别 (Char-level)**:
    -   优点: 词表极小（几百个），没有OOV问题。
    -   缺点: 单个字符语义信息稀疏，会导致输入序列过长，极大增加计算负担。

## 2. 子词分词 (Subword Tokenization)

BPE是一种**子词分词**算法，它巧妙地平衡了词级别和字符级别的优缺点。其核心思想是：**高频词应该被保留为完整的Token，而低频词则应被拆分为更有意义的子词单元**。这样既控制了词表大小，又避免了OOV问题。

## 3. BPE 算法核心步骤

BPE是一种基于数据压缩思想的贪心算法。其训练过程可以概括为：**不断地将文本中出现频率最高的相邻字节对（或字符对）合并，形成一个新的、更长的Token**。

**算法流程:**

1.  **初始化**:
    -   准备一个大型的文本语料库。
    -   将初始词表设置为语料库中出现的所有基本单元（例如，UTF-8字节或字符）。
2.  **迭代合并**:
    -   **统计**: 统计语料库中所有相邻Token对的出现频率。
    -   **找出最优对**: 找到频率最高的Token对（例如 `('t', 'h')`）。
    -   **合并**: 将这个最优对合并成一个新的Token（例如 `'th'`）。
    -   **更新词表**: 将新生成的Token添加到词表中。
    -   **更新语料**: 在语料库中，将所有出现的 `('t', 'h')` 替换为 `'th'`。
3.  **终止**: 重复第2步，直到词表大小达到预设的 `vocab_size`。

### 示例演示

假设我们的初始语料是 `"aaabdaaabac"`，目标词表大小为10。

-   **初始词表**: `['a', 'b', 'c', 'd']` (大小为4)

**第1次合并:**
-   相邻对频率: `('a', 'a')`: 5次, `('a', 'b')`: 2次, `('b', 'd')`: 1次, ...
-   最优对: `('a', 'a')`
-   新Token: `'aa'`
-   更新词表: `['a', 'b', 'c', 'd', 'aa']` (大小为5)
-   更新语料: `"aabdaabac"` -> `"aaabdaaabac"`

**第2次合并:**
-   当前语料: `"aaabdaaabac"`
-   相邻对频率: `('a', 'a')`: 2次, `('a', 'b')`: 2次, ...
-   最优对: `('a', 'b')`
-   新Token: `'ab'`
-   更新词表: `['a', 'b', 'c', 'd', 'aa', 'ab']` (大小为6)
-   更新语料: `"aaabd aaab ac"` -> `"aa ab d aa ab ac"`

...持续这个过程，直到词表大小达到10。

## 4. BPE 的优缺点

**优点**:
-   **有效平衡**: 完美地平衡了词表大小和序列长度。
-   **无OOV问题**: 任何未见过的词最终都可以被拆分为字符或字节级别的Token。
-   **效率高**: 算法实现简单，训练和编码速度快。

**缺点**:
-   **贪心算法**: 每次合并都是局部最优，不保证全局最优。
-   **非语义驱动**: 合并纯粹基于频率，可能产生没有实际语义的子词（例如 `'ĠIt'`）。

## 5. BPE vs. WordPiece vs. Unigram

| 算法 | 合并/拆分标准 | 核心思想 | 代表模型 |
| :--- | :--- | :--- | :--- |
| **BPE** | **频率最高**的相邻对 | 数据压缩 | GPT系列, LLaMA |
| **WordPiece** | 使语料**似然度增加最大**的对 | 语言模型概率 | BERT, T5 |
| **Unigram** | 从大词表开始，**移除**对语料似然度**影响最小**的Token | 概率模型优化 | ALBERT, T5 |

## 6. 参考文献

-   [Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)