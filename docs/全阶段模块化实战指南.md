# â™¾ï¸ LLM Zero-to-Hero: å…¨é˜¶æ®µæ¨¡å—åŒ–å®æˆ˜æŒ‡å—

æœ¬é¡¹ç›®çš„è®¾è®¡å“²å­¦æ˜¯ **â€œä¸‰é˜¶æ®µè‡ªç”±ç»„åˆâ€**ã€‚
ä½ å¯ä»¥æŠŠå®ƒæƒ³è±¡æˆè£…é…ä¸€æ¡æµæ°´çº¿ï¼šæ— è®ºä½ åœ¨ç¬¬ä¸€é˜¶æ®µé€ å‡ºäº†ä»€ä¹ˆæ ·çš„â€œå¼•æ“â€ï¼ˆæ¨¡å‹æ¶æ„ï¼‰ï¼Œç¬¬äºŒé˜¶æ®µéƒ½å¯ä»¥ç”¨ä»»æ„ä¸€ç§â€œå·¥è‰ºâ€ï¼ˆå¾®è°ƒæ–¹æ³•ï¼‰è¿›è¡Œæ‰“ç£¨ï¼Œç¬¬ä¸‰é˜¶æ®µå†ç”¨ä»»æ„ä¸€ç§â€œæ•™å…·â€ï¼ˆå¯¹é½ç®—æ³•ï¼‰è¿›è¡Œæ•™è‚²ã€‚

---

## ğŸ—‚ï¸ é˜¶æ®µ 0: æ•°æ®å‡†å¤‡ (åªéœ€è¿è¡Œä¸€æ¬¡)
*æ— è®ºåç»­å¦‚ä½•ç»„åˆï¼Œè¿™éƒ½æ˜¯å¿…é¡»çš„èµ·ç‚¹ã€‚*

```bash
python data_pipeline/download/download_tinystories.py && python data_pipeline/tokenizer/train_tokenizer.py --vocab_size 4096 --data_limit_mb 100 && python data_pipeline/processing/encode_stories.py && python data_pipeline/processing/build_pretrain_bins.py && python data_pipeline/processing/build_sft_bins.py && python data_pipeline/processing/build_preference_bins.py && python data_pipeline/download/download_prompts.py
```

---

## ğŸ—ï¸ é˜¶æ®µ 1: é¢„è®­ç»ƒ (Pre-training) â€”â€” é€‰æ‹©ä½ çš„â€œå¤§è„‘â€

**è¯·ä»ä¸‹æ–¹ä»»é€‰ä¸€ç§æ¶æ„è¿›è¡Œè®­ç»ƒã€‚**
*æ— è®ºä½ é€‰äº†å“ªä¸ªï¼Œäº§å‡ºçš„æ¨¡å‹éƒ½ä¼šä¿å­˜åœ¨ `runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth`ï¼Œå¹¶è¢«åç»­é˜¶æ®µè‡ªåŠ¨åŠ è½½ã€‚*

### é€‰é¡¹ A: æ ‡å‡†æ¶æ„ (Standard)
*æœ€ç¨³å¥ï¼Œå…¼å®¹æ€§æœ€å¥½ (Llama2/3 é£æ ¼)ã€‚*
```bash
python pretrain/train.py --config_path configs/pretrain/1.4M_pretrain_fast.yaml --fast_dev_run
```

### é€‰é¡¹ B: DeepSeek-V2 (MLA + MoE)
*æ¨èã€‚æ¨ç†æ˜¾å­˜æä½ï¼Œè®­ç»ƒæˆæœ¬ä½ã€‚*
```bash
python pretrain/train.py --config_path configs/pretrain/variants/deepseek_v2_nano.yaml --fast_dev_run
```

### é€‰é¡¹ C: æ ‡å‡† MoE (Mixtral Style)
*ç¨€ç–æ¿€æ´»ï¼Œå‰å‘è®¡ç®—å¿«ã€‚*
```bash
python pretrain/train.py --config_path configs/pretrain/1.4M_moe_fast.yaml --fast_dev_run
```

### é€‰é¡¹ D: ç¨€ç–æ³¨æ„åŠ› (MoBA / NSA)
*é•¿æ–‡æœ¬ä¼˜åŒ–æ¶æ„ã€‚*
```bash
python pretrain/train.py --config_path configs/pretrain/variants/moba_nano.yaml --fast_dev_run
```
*(æˆ–è€…ä½¿ç”¨ `nsa_nano.yaml` / `moba_moe_nano.yaml`)*

### é€‰é¡¹ E: æè‡´æ˜¾å­˜ä¼˜åŒ– (MQA)
*ç±»ä¼¼ Gemma/Falconï¼Œæ¨ç†æ˜¾å­˜å ç”¨æä½ã€‚*
```bash
python pretrain/train.py --config_path configs/pretrain/variants/mqa_nano.yaml --fast_dev_run
```

---

## ğŸ”§ é˜¶æ®µ 2: ç›‘ç£å¾®è°ƒ (SFT) â€”â€” é€‰æ‹©ä½ çš„â€œå·¥è‰ºâ€

**ç°åœ¨ï¼Œæˆ‘ä»¬è¦æ•™æ¨¡å‹å¬æ‡‚æŒ‡ä»¤ã€‚è¯·ä»ä¸‹æ–¹ä»»é€‰ä¸€ç§å¾®è°ƒæ–¹å¼ã€‚**
*è¿™äº›è„šæœ¬ä¼šè‡ªåŠ¨é€‚é…é˜¶æ®µ 1 äº§å‡ºçš„ä»»ä½•æ¨¡å‹æ¶æ„ï¼ˆè‡ªåŠ¨æ¢æµ‹å±‚åã€è‡ªåŠ¨é€‚é…å‚æ•°ï¼‰ã€‚*

### å·¥è‰º A: QLoRA (4-bit é‡åŒ–å¾®è°ƒ) â€”â€” [å¼ºçƒˆæ¨è]
*æ˜¾å­˜å ç”¨æœ€ä½ï¼Œå¯åœ¨æ¶ˆè´¹çº§æ˜¾å¡å¾®è°ƒå¤§æ¨¡å‹ã€‚ä¼šè‡ªåŠ¨å°†åŸºåº§é‡åŒ–ä¸º NF4 æ ¼å¼ã€‚*
```bash
python finetune/peft/qlora/sft_qlora_train.py --config_path configs/sft/peft/qlora/1.4M_sft_qlora.yaml --fast_dev_run
```

### å·¥è‰º B: LoRA (æ ‡å‡†ä½ç§©å¾®è°ƒ)
*ç»å…¸çš„å‚æ•°é«˜æ•ˆå¾®è°ƒï¼Œé€Ÿåº¦å¿«ï¼Œæ•ˆæœå¥½ã€‚*
```bash
python finetune/peft/lora/sft_lora_train.py --config_path configs/sft/peft/lora/1.4M_sft_lora.yaml --fast_dev_run
```

### å·¥è‰º C: Full Fine-tuning (å…¨é‡å¾®è°ƒ)
*æ›´æ–°æ¨¡å‹æ‰€æœ‰å‚æ•°ã€‚æ˜¾å­˜è¦æ±‚æœ€é«˜ï¼Œä½†ç†è®ºä¸Šé™æœ€é«˜ã€‚*
```bash
python finetune/full/sft_train.py --config_path configs/sft/full/1.4M_sft_full.yaml --fast_dev_run
```

---

## ğŸ’¬ ä¸­åœºä¼‘æ¯: éªŒè¯å¯¹è¯ (Chat)

åœ¨è¿›å…¥ RL ä¹‹å‰ï¼Œå…ˆè¯•è¯•ä½ çš„ SFT æ¨¡å‹æ•ˆæœã€‚

**é€šç”¨å¯¹è¯å‘½ä»¤:**
*(è„šæœ¬ä¼šè‡ªåŠ¨è¯†åˆ«æ˜¯å¦éœ€è¦åŠ è½½ Adapterï¼Œä¹Ÿä¼šè‡ªåŠ¨åº”ç”¨ MLA/MoE çš„æ¨ç†ä¼˜åŒ–)*

*   **å¦‚æœä½ ç”¨äº† QLoRA / LoRA:**
    ```bash
    python inference/chat.py --config_path configs/sft/peft/qlora/1.4M_sft_qlora.yaml --checkpoint_path runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth --adapter_path runs/sft/peft/qlora/fast-dev-run/checkpoints/ckpt_best.pth
    ```
    *(è¯·æ ¹æ®ä½ å®é™…è·‘çš„ SFT ç±»å‹ï¼Œä¿®æ”¹ config_path å’Œ adapter_path)*

*   **å¦‚æœä½ ç”¨äº† Full SFT:**
    ```bash
    python inference/chat.py --config_path configs/sft/full/1.4M_sft_full.yaml --checkpoint_path runs/sft/full/fast-dev-run/checkpoints/ckpt_best.pth
    ```

---

## âš–ï¸ é˜¶æ®µ 3: ä»·å€¼è§‚å¯¹é½ (RLHF) â€”â€” é€‰æ‹©ä½ çš„â€œæ•™å…·â€

**æœ€åï¼Œè®©æ¨¡å‹æ›´ç¬¦åˆäººç±»åå¥½ã€‚è¯·ä»»é€‰ä¸€ç§å¯¹é½ç®—æ³•ã€‚**

### å‰ç½®: è®­ç»ƒå¥–åŠ±æ¨¡å‹ (Reward Model)
*å¤§å¤šæ•° RL ç®—æ³•éœ€è¦ä¸€ä¸ª RMã€‚*
```bash
python align/rm_train.py --config_path configs/rlhf/rm/1.4M_rm.yaml --fast_dev_run
```

### è·¯çº¿ A: ç¦»çº¿å¯¹é½ (Offline RL)
*æœ€ç®€å•ï¼Œä¸éœ€è¦å¤æ‚çš„é‡‡æ ·ç¯å¢ƒï¼Œ**æ”¯æŒæ‰€æœ‰æ¶æ„**ã€‚*

*   **DPO (Direct Preference Optimization):**
    ```bash
    python align/train_offline.py --config_path configs/rlhf/offline/dpo/1.4M_dpo.yaml --fast_dev_run
    ```
*   **ORPO (Odds Ratio Preference Optimization):**
    ```bash
    python align/train_offline.py --config_path configs/rlhf/offline/orpo/1.4M_orpo.yaml --fast_dev_run
    ```

### è·¯çº¿ B: åœ¨çº¿å¯¹é½ (Online RL)
*æ•ˆæœé€šå¸¸æ›´å¥½ï¼Œä½†è®­ç»ƒæ›´æ…¢ã€‚**æ³¨æ„ï¼šLinear Attention å’Œ NSA æ¶æ„æš‚ä¸æ”¯æŒåœ¨çº¿ RLã€‚***

*   **GRPO (Group Relative Policy Optimization - æ¨è):**
    *DeepSeekMath ä½¿ç”¨çš„ç®—æ³•ï¼Œä¸éœ€è¦ Critic æ¨¡å‹ï¼Œçœæ˜¾å­˜ã€‚*
    ```bash
    python align/train_online.py --config_path configs/rlhf/online/grpo/1.4M_grpo.yaml --fast_dev_run
    ```
*   **PPO (Proximal Policy Optimization):**
    *ç»å…¸çš„ RLHF ç®—æ³•ã€‚*
    ```bash
    python align/train_online.py --config_path configs/rlhf/online/ppo/1.4M_ppo.yaml --fast_dev_run
    ```
*   **GSPO (Group Sequence Policy Optimization):**
    ```bash
    python align/train_online.py --config_path configs/rlhf/online/gspo/1.4M_gspo.yaml --fast_dev_run
    ```

---

## ğŸ“Š æ€»ç»“ï¼šä»»æ„ç»„åˆç¤ºä¾‹

*   **DeepSeek å¤åˆ»æµ**:
    *   Phase 1: `deepseek_v2_nano.yaml`
    *   Phase 2: `sft_qlora_train.py` (QLoRA)
    *   Phase 3: `grpo` (GRPO)

*   **æç®€ Gemma æµ**:
    *   Phase 1: `mqa_nano.yaml`
    *   Phase 2: `sft_lora_train.py` (LoRA)
    *   Phase 3: `dpo` (DPO)

*   **é•¿æ–‡æœ¬æ¢ç´¢æµ**:
    *   Phase 1: `moba_moe_nano.yaml`
    *   Phase 2: `sft_full.py` (Full SFT)
    *   Phase 3: `dpo` (DPO - æ³¨æ„ MoBA ä¸æ”¯æŒåœ¨çº¿ PPO)

ç°åœ¨ï¼Œå°½æƒ…æ¢ç´¢å§ï¼
```