# 理论解析: 位置编码技术全景 (从绝对PE到RoPE与ALiBi)

## 1. 核心问题：如何让Transformer理解顺序？

Transformer的核心——自注意力机制，本质上是“无序”的。它将输入视为一个“集合”，`"I love you"` 和 `"You love I"` 在它看来可能没有区别。位置编码（Positional Encoding, PE）就是为了解决这个问题，它向模型注入关于token顺序的信息。

---

## 2. 绝对位置编码 (Absolute PE)

为序列中的每个绝对位置（第1个、第2个...）分配一个唯一的向量。

### 2.1 Sinusoidal PE (三角函数编码)
-   **出处**: "Attention Is All You Need" (原版Transformer)
-   **思想**: 使用不同频率的`sin`和`cos`函数组合来为每个位置生成一个唯一的、确定性的向量。对于位置`pos`和维度`i`，其编码值为：
    $$ PE_{(pos, 2i)} = \sin(pos / 10000^{2i/d_{\text{model}}}) $$
    $$ PE_{(pos, 2i+1)} = \cos(pos / 10000^{2i/d_{\text{model}}}) $$
-   **优点**: 无需学习，理论上可以外推到无限长。
-   **缺点**: 外推性能在实践中很差；与词嵌入的简单相加可能不是最优的融合方式。

### 2.2 Learned PE (可学习编码)
-   **出处**: BERT, GPT-2/3 等
-   **思想**: 创建一个位置嵌入矩阵 `E_pos`，大小为 `(max_seq_len, hidden_dim)`。第 `i` 个token的位置编码就是这个矩阵的第 `i` 行。这个矩阵像词嵌入一样，在训练中被学习。
-   **优点**: 简单直观，能自适应数据。
-   **缺点**: **完全无法外推**。一旦推理长度超过预训练时的 `max_seq_len`，模型就不知道该用哪个向量了。

---

## 3. 相对位置编码 (Relative PE)

核心思想是，模型真正需要知道的不是一个token的绝对位置，而是**token之间的相对距离**。

### 3.1 RoPE (Rotary Position Embedding)
-   **出处**: LLaMA, Qwen, Yi 等几乎所有现代开源LLM
-   **思想**: **当前主流！** 它不改变词向量的长度，而是将词向量视为复数，并根据其绝对位置 `m`，将其乘以一个旋转矩阵 `e^(i*m*θ)`。
-   **关键特性**: 两个分别位于`m`和`n`位置的向量`q`和`k`，经过RoPE旋转后，它们的内积 `<q_m, k_n>` 只与它们的**相对位置 `(m-n)`** 有关，而与绝对位置`m`和`n`无关。这就巧妙地将绝对位置信息编码为了相对位置信息。
-   **优点**: 性能强大，完美地将相对位置信息融入了自注意力机制。
-   **缺点**: 基础版本的长度外推能力有限，需要YaRN等技术来优化。

### 3.2 ALiBi (Attention with Linear Biases)
-   **出处**: BLOOM, MPT 等
-   **思想**: **极其优雅**！它完全抛弃了位置嵌入向量，也不修改Q和K。而是在计算Attention分数后，直接给分数矩阵加上一个“惩罚项”（偏置）。两个token距离越远，惩罚越大（偏置为负且绝对值越大），注意力分数就越低。
    $$ \text{AttentionScores}(q_i, k_j) = \frac{q_i k_j^T}{\sqrt{d_k}} + m \cdot |i-j| $$
    其中 `m` 是一个预设的、每个头都不同的斜率。
-   **优点**: 实现简单，**天然具备优秀的长度外推能力**，因为距离惩罚可以应用到任意长度。
-   **缺点**: 可能不如RoPE在复杂相对位置建模上精确。

---

## 4. 总结对比

| 方法 | 类型 | 作用方式 | 长度外推能力 | 现代应用 |
| :--- | :--- | :--- | :--- | :--- |
| **Sinusoidal PE** | 绝对 | 加到词嵌入上 | 差 | 较少 |
| **Learned PE** | 绝对 | 加到词嵌入上 | **无** | 较少 (e.g., ViT) |
| **RoPE** | 相对 | **旋转**Q和K向量 | 有限 (需YaRN等) | **主流** (LLaMA) |
| **ALiBi** | 相对 | **偏置**注意力分数 | **强** | 广泛 (BLOOM) |

我们的代码库同时实现了以上所有方法，允许通过配置灵活切换，以进行研究和学习。
