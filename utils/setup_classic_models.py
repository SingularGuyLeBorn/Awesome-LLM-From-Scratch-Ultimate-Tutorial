# FILE: utils/setup_classic_models.py
# -*- coding: utf-8 -*-
"""
ã€é…ç½®å·¥å‚ v3.1 - æ·±åº¦æ–‡æ¡£ç‰ˆã€‘
åŠŸèƒ½ï¼šè‡ªåŠ¨ç”Ÿæˆç»å…¸æ¨¡å‹ (DeepSeek, Llama, Gemma) çš„å…¨ç”Ÿå‘½å‘¨æœŸé…ç½®ã€‚
å‡çº§ï¼š
1. æ•°æ®å¤„ç†æµç¨‹åˆ†æ­¥è¯¦è§£ã€‚
2. è¯„ä¼°ç¯èŠ‚è¦†ç›– Pretrain, SFT, RLHF å…¨é˜¶æ®µã€‚
"""
import os
import yaml
from pathlib import Path

# --- 1. æ¨¡å‹æ¶æ„å®šä¹‰ (The Blueprint) ---

ARCHITECTURES = {
    "deepseek": {
        "v3_nano": {
            "title": "DeepSeek-V3 Nano (The Sparse Giant)",
            "description": "åŸºäº DeepSeek-V3 æ¶æ„çš„å¾®ç¼©ç‰ˆã€‚æ ¸å¿ƒç‰¹æ€§åŒ…æ‹¬ MLA (å¤šå¤´æ½œå˜é‡æ³¨æ„åŠ›) å’Œ DeepSeekMoE (ç»†ç²’åº¦æ··åˆä¸“å®¶)ã€‚è¿™æ˜¯ç›®å‰æœ€é«˜æ•ˆã€æœ€å¤æ‚çš„æ¶æ„ä¹‹ä¸€ã€‚",
            "support_paged_attention": False,  # MLA æš‚ä¸æ”¯æŒ PagedAttention
            "model_config": {
                "dim": 64,
                "n_layers": 2,
                "n_heads": 4,
                "n_kv_heads": 4,
                "vocab_size": 4096,
                "multiple_of": 16,
                "norm_eps": 1.0e-5,
                "max_seq_len": 128,
                "dropout": 0.0,
                # DeepSeek Specifics
                "attention_variant": "mla",
                "q_lora_rank": 16,
                "kv_lora_rank": 16,
                "v_head_dim": 16,
                "rope_head_dim": 8,
                "nope_head_dim": 8,
                "num_experts": 4,
                "num_shared_experts": 1,
                "num_experts_per_tok": 2,
                "use_aux_free_lb": True,
                "use_activation_checkpointing": True
            },
            "target_modules": "auto"  # MLA ç»“æ„å¤æ‚ï¼Œå¼ºçƒˆå»ºè®®è‡ªåŠ¨æ¢æµ‹
        }
    },
    "llama": {
        "v3_nano": {
            "title": "Llama-3 Nano (The Robust Standard)",
            "description": "åŸºäº Meta Llama-3 æ¶æ„çš„å¾®ç¼©ç‰ˆã€‚é‡‡ç”¨ GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)ã€RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) å’Œ SwiGLUã€‚è¿™æ˜¯ç›®å‰å…¼å®¹æ€§æœ€å¥½ã€ç”Ÿæ€æœ€ä¸°å¯Œçš„æ¶æ„ã€‚",
            "support_paged_attention": True,
            "model_config": {
                "dim": 128,
                "n_layers": 4,
                "n_heads": 4,
                "n_kv_heads": 2,  # GQA: 4 Query Heads, 2 KV Heads
                "vocab_size": 4096,
                "multiple_of": 32,
                "norm_eps": 1.0e-5,
                "max_seq_len": 256,
                "dropout": 0.0,
                "attention_variant": "mha",  # ä»£ç ä¸­ mha å…¼å®¹ GQA
                "rope_base": 10000,
                "num_experts": 0,
                "use_activation_checkpointing": True
            },
            "target_modules": ["wq", "wk", "wv", "wo", "w_gate", "w_up", "w_down"]
        }
    },
    "gemma": {
        "v2_nano": {
            "title": "Gemma-2 Nano (The Efficient Speeder)",
            "description": "åŸºäº Google Gemma-2 æ¶æ„çš„å¾®ç¼©ç‰ˆã€‚é‡‡ç”¨ MQA (å¤šæŸ¥è¯¢æ³¨æ„åŠ›ï¼ŒKVå¤´æ•°ä¸º1)ï¼Œæå¤§åœ°å‡å°‘äº†æ¨ç†æ—¶çš„ KV Cache å ç”¨ï¼Œæ¨ç†é€Ÿåº¦æå¿«ã€‚",
            "support_paged_attention": True,
            "model_config": {
                "dim": 128,
                "n_layers": 4,
                "n_heads": 4,
                "n_kv_heads": 1,  # MQA: All heads share 1 KV head
                "vocab_size": 4096,
                "multiple_of": 32,
                "norm_eps": 1.0e-6,  # Gemma uses smaller eps
                "max_seq_len": 256,
                "dropout": 0.0,
                "attention_variant": "mha",
                "rope_base": 10000,
                "num_experts": 0,
                "use_activation_checkpointing": True
            },
            "target_modules": ["wq", "wk", "wv", "wo", "w_gate", "w_up", "w_down"]
        }
    }
}


# --- 2. é…ç½®ç”Ÿæˆå™¨ (The Config Factory) ---

def get_base_config(run_name, output_dir="./runs/", device="cpu"):
    return {
        "run_name": run_name,
        "output_dir": output_dir,
        "device": device,
        "console": {"verbose": True}
    }


def get_data_config(stage="pretrain"):
    base = {
        "tokenizer_name": "./data_pipeline/processed_data/tinystories_project_vs4096.json"
    }
    if stage == "pretrain":
        base["data_dir"] = "./data_pipeline/processed_data/"
        base["train_data_limit"] = 5000
        base["val_data_limit"] = 200
    elif stage == "sft":
        base["sft_data_path"] = "./data_pipeline/processed_data/sft_data.bin"
    elif stage in ["rm", "dpo", "orpo"]:
        base["data_dir"] = "./data_pipeline/processed_data/"  # Expects preference bins
    elif stage in ["ppo", "grpo", "gspo"]:
        base["prompt_data_path"] = "./data_pipeline/prompts/h4_prompts.txt"
    return base


def get_training_config(stage="pretrain"):
    # åŸºç¡€è®­ç»ƒå‚æ•°
    cfg = {
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "max_epochs": 1,
        "weight_decay": 0.01,
        "clip_grad_norm": 1.0,
        "loss_spike_threshold": 10.0,
        "use_activation_checkpointing": True
    }

    # é’ˆå¯¹ä¸åŒé˜¶æ®µçš„è¶…å‚å¾®è°ƒ
    if stage == "pretrain":
        cfg["learning_rate"] = 5.0e-4
        cfg["warmup_ratio"] = 0.1
        cfg["min_lr_ratio"] = 0.1
        cfg["max_epochs"] = 2  # é¢„è®­ç»ƒå¤šè·‘å‡ è½®
    elif stage == "sft":
        cfg["learning_rate"] = 2.0e-5  # SFT éœ€è¦æ›´å°çš„ LR
    elif stage == "lora":
        cfg["learning_rate"] = 2.0e-4  # LoRA å¯ä»¥æ‰¿å—è¾ƒå¤§çš„ LR
    elif stage == "qlora":
        cfg["learning_rate"] = 1.5e-4
        cfg["clip_grad_norm"] = 0.3  # QLoRA æ¢¯åº¦è£å‰ªæ›´ä¸¥æ ¼
    elif stage == "rm":
        cfg["batch_size"] = 2
        cfg["learning_rate"] = 1.0e-5
    elif stage in ["dpo", "grpo", "ppo"]:
        cfg["batch_size"] = 1  # RL åœ¨ CPU ä¸Šé€šå¸¸åªèƒ½è·‘å° Batch
        cfg["learning_rate"] = 5.0e-7  # RL éœ€è¦æå°çš„ LR
        cfg["max_epochs"] = 1
        cfg["weight_decay"] = 0.0  # RL é€šå¸¸ä¸åŠ  weight decay

    return cfg


# --- 3. ä¸»é€»è¾‘ (The Builder) ---

def generate_configs():
    root_dir = Path(__file__).parent.parent / "configs" / "classic_reproductions"
    root_dir.mkdir(parents=True, exist_ok=True)

    for family, variants in ARCHITECTURES.items():
        for version, details in variants.items():
            # 1. åˆ›å»ºç›®å½•ç»“æ„
            model_dir = root_dir / family / version
            model_dir.mkdir(parents=True, exist_ok=True)

            model_key = f"{family}-{version}"
            title = details["title"]
            desc = details["description"]
            support_api = details["support_paged_attention"]
            model_cfg = details["model_config"]
            target_modules = details["target_modules"]

            print(f"ğŸ› ï¸  Constructing Suite for: {title} ...")

            # --- ç”Ÿæˆ YAML é…ç½®æ–‡ä»¶ ---

            # 0. Pretrain
            pretrain_cfg = get_base_config(f"pretrain-{model_key}-{{timestamp}}")
            pretrain_cfg["data"] = get_data_config("pretrain")
            pretrain_cfg["model"] = model_cfg
            pretrain_cfg["training"] = get_training_config("pretrain")
            pretrain_cfg["logging"] = {"log_interval": 5}
            pretrain_cfg["checkpointing"] = {"save_interval": 200, "resume_from": "none"}
            with open(model_dir / "0_pretrain.yaml", "w", encoding="utf-8") as f:
                yaml.dump(pretrain_cfg, f, sort_keys=False, allow_unicode=True)

            # 1. SFT (Full)
            sft_cfg = get_base_config(f"sft-full-{model_key}-{{timestamp}}")
            sft_cfg["data"] = get_data_config("sft")
            sft_cfg["model"] = model_cfg
            sft_cfg["sft"] = {"base_model_checkpoint": "will_be_overridden_by_fast_dev_run"}
            sft_cfg["training"] = get_training_config("sft")
            sft_cfg["logging"] = {"log_interval": 1}
            sft_cfg["checkpointing"] = {"save_interval": 100}
            with open(model_dir / "1_sft_full.yaml", "w", encoding="utf-8") as f:
                yaml.dump(sft_cfg, f, sort_keys=False, allow_unicode=True)

            # 2. SFT (LoRA)
            lora_cfg = get_base_config(f"sft-lora-{model_key}-{{timestamp}}")
            lora_cfg["data"] = get_data_config("sft")
            lora_cfg["model"] = model_cfg
            lora_cfg["sft"] = {"base_model_checkpoint": "will_be_overridden_by_fast_dev_run"}
            lora_cfg["lora"] = {"r": 16, "alpha": 32, "dropout": 0.05, "target_modules": target_modules}
            lora_cfg["training"] = get_training_config("lora")
            lora_cfg["logging"] = {"log_interval": 1}
            lora_cfg["checkpointing"] = {"save_interval": 100}
            with open(model_dir / "1_sft_lora.yaml", "w", encoding="utf-8") as f:
                yaml.dump(lora_cfg, f, sort_keys=False, allow_unicode=True)

            # 3. SFT (QLoRA)
            qlora_cfg = get_base_config(f"sft-qlora-{model_key}-{{timestamp}}")
            qlora_cfg["data"] = get_data_config("sft")
            qlora_cfg["model"] = model_cfg
            qlora_cfg["sft"] = {"base_model_checkpoint": "will_be_overridden_by_fast_dev_run"}
            qlora_cfg["qlora"] = {"r": 16, "alpha": 32, "dropout": 0.05, "target_modules": target_modules,
                                  "compute_dtype": "float32"}
            qlora_cfg["training"] = get_training_config("qlora")
            qlora_cfg["logging"] = {"log_interval": 1}
            qlora_cfg["checkpointing"] = {"save_interval": 100}
            with open(model_dir / "1_sft_qlora.yaml", "w", encoding="utf-8") as f:
                yaml.dump(qlora_cfg, f, sort_keys=False, allow_unicode=True)

            # 4. Reward Model
            rm_cfg = get_base_config(f"rm-{model_key}-{{timestamp}}")
            rm_cfg["data"] = get_data_config("rm")
            rm_cfg["model"] = model_cfg
            rm_cfg["rm"] = {"sft_model_checkpoint": "will_be_overridden_by_fast_dev_run"}
            rm_cfg["training"] = get_training_config("rm")
            rm_cfg["logging"] = {"log_interval": 1}
            with open(model_dir / "2_rm.yaml", "w", encoding="utf-8") as f:
                yaml.dump(rm_cfg, f, sort_keys=False, allow_unicode=True)

            # 5. RLHF (DPO)
            dpo_cfg = get_base_config(f"dpo-{model_key}-{{timestamp}}")
            dpo_cfg["data"] = get_data_config("dpo")
            dpo_cfg["model"] = model_cfg
            dpo_cfg["offline"] = {"algorithm": "dpo", "sft_model_checkpoint": "will_be_overridden", "beta": 0.1,
                                  "label_smoothing": 0.0}
            dpo_cfg["training"] = get_training_config("dpo")
            dpo_cfg["logging"] = {"log_interval": 1}
            with open(model_dir / "3_rlhf_dpo.yaml", "w", encoding="utf-8") as f:
                yaml.dump(dpo_cfg, f, sort_keys=False, allow_unicode=True)

            # 6. RLHF (GRPO)
            grpo_cfg = get_base_config(f"grpo-{model_key}-{{timestamp}}")
            grpo_cfg["data"] = get_data_config("grpo")
            grpo_cfg["model"] = model_cfg
            grpo_cfg["rl"] = {
                "algorithm": "grpo", "sft_model_checkpoint": "will_be_overridden",
                "reward_model_checkpoint": "will_be_overridden",
                "kl_coeff": 0.04, "group_size": 4, "max_prompt_len": 32, "max_gen_len": 64,
                "generate": {"temperature": 0.7, "top_k": 20},
                "rollout_batches": 2, "update_epochs": 1, "update_batch_size": 4, "clip_epsilon": 0.2
            }
            grpo_cfg["training"] = get_training_config("grpo")
            with open(model_dir / "3_rlhf_grpo.yaml", "w", encoding="utf-8") as f:
                yaml.dump(grpo_cfg, f, sort_keys=False, allow_unicode=True)

            # 7. RLHF (PPO)
            ppo_cfg = get_base_config(f"ppo-{model_key}-{{timestamp}}")
            ppo_cfg["data"] = get_data_config("ppo")
            ppo_cfg["model"] = model_cfg
            ppo_cfg["rl"] = {
                "algorithm": "ppo", "sft_model_checkpoint": "will_be_overridden",
                "reward_model_checkpoint": "will_be_overridden",
                "kl_coeff": 0.02, "group_size": 1, "max_prompt_len": 32, "max_gen_len": 64,
                "generate": {"temperature": 0.7, "top_k": 20},
                "rollout_batches": 4, "update_epochs": 1, "update_batch_size": 4,
                "clip_epsilon": 0.2, "gamma": 0.99, "lambda_gae": 0.95, "value_loss_coef": 0.5, "entropy_coef": 0.01
            }
            ppo_cfg["training"] = get_training_config("ppo")
            with open(model_dir / "3_rlhf_ppo.yaml", "w", encoding="utf-8") as f:
                yaml.dump(ppo_cfg, f, sort_keys=False, allow_unicode=True)

            # --- ç”Ÿæˆæ–‡æ¡£ (Story Mode) ---

            api_section = ""
            if support_api:
                api_section = f"""
## ğŸŒ ç¬¬äº”ç« ï¼šäº‘ç«¯éƒ¨ç½² (Serving)
è®©ä½ çš„æ¨¡å‹åƒ ChatGPT ä¸€æ ·æä¾› API æœåŠ¡ï¼Œæ”¯æŒé«˜å¹¶å‘å’Œ PagedAttention åŠ é€Ÿã€‚

**1. å¯åŠ¨ API æœåŠ¡å™¨**:
```bash
python inference/api_server.py --config_path configs/classic_reproductions/{family}/{version}/0_pretrain.yaml --checkpoint_path runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth
```

**2. å‘é€è¯·æ±‚æµ‹è¯•**:
*(è¯·å¦å¼€ä¸€ä¸ªç»ˆç«¯è¿è¡Œ)*
```bash
python -c "import requests; print(requests.post('http://127.0.0.1:8000/v1/chat/completions', json={{'model': 'test', 'messages': [{{'role': 'user', 'content': 'Hello!'}}]}}).json())"
```
"""
            else:
                api_section = f"""
## ğŸŒ ç¬¬äº”ç« ï¼šäº‘ç«¯éƒ¨ç½² (Serving)
*(âš ï¸ æ³¨æ„ï¼šå½“å‰æ¨¡å‹æ¶æ„ `{model_cfg['attention_variant'].upper()}` é‡‡ç”¨éæ ‡å‡† KV ç»“æ„ï¼Œæš‚æœªé€‚é… PagedAttention å¼•æ“ã€‚è¯·ä½¿ç”¨ Chat æ¨¡å¼è¿›è¡Œæœ¬åœ°äº¤äº’ã€‚)*
"""

            # å®Œæ•´çš„ README å†…å®¹
            readme_content = f"""# ğŸ“˜ ç‚¼ä¸¹æ‰‹è®°: {title}

> **"{desc}"**

æ¬¢è¿æ¥åˆ° LLM å®æˆ˜å¤ç°å¥—ä»¶ã€‚æœ¬æŒ‡å—å°†å¸¦é¢†ä½ å®Œæˆä»**æ¨¡å‹å‡ºç”Ÿ**åˆ°**å¯¹é½äººç±»ä»·å€¼è§‚**çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚

---

## ğŸ› ï¸ åºç« ï¼šå‡†å¤‡å·¥ä½œ (Prerequisites)
ä¿—è¯è¯´â€œç£¨åˆ€ä¸è¯¯ç æŸ´å·¥â€ã€‚åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡†å¤‡å¥½æ•°æ®æµæ°´çº¿ã€‚
è¯·æŒ‰é¡ºåºæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ã€‚æ¯ä¸€æ­¥éƒ½è‡³å…³é‡è¦ã€‚

**1. ä¸‹è½½åŸå§‹æ•°æ®**
*ä» HuggingFace ä¸‹è½½ TinyStories æ•°æ®é›†ã€‚è¿™æ˜¯æˆ‘ä»¬æ¨¡å‹çš„â€œè¯¾æœ¬â€ã€‚*
```bash
python data_pipeline/download/download_tinystories.py
```

**2. è®­ç»ƒåˆ†è¯å™¨ (Tokenizer)**
*è®­ç»ƒä¸€ä¸ªä¸“é—¨ç”¨äºå¤„ç†è¿™äº›æ•°æ®çš„ BPE åˆ†è¯å™¨ã€‚å®ƒå†³å®šäº†æ¨¡å‹å¦‚ä½•â€œé˜…è¯»â€æ–‡æœ¬ã€‚*
```bash
python data_pipeline/tokenizer/train_tokenizer.py --vocab_size 4096 --data_limit_mb 100
```

**3. ç¼–ç æ•°æ® (Encode)**
*å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—åºåˆ— (Token IDs)ã€‚è¿™æ˜¯æ¨¡å‹å”¯ä¸€èƒ½ç†è§£çš„è¯­è¨€ã€‚*
```bash
python data_pipeline/processing/encode_stories.py
```

**4. æ„å»ºé¢„è®­ç»ƒæ•°æ® (Pretrain Bins)**
*å°† Token åºåˆ—æ‰“åŒ…æˆé«˜æ•ˆçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œä¾›é¢„è®­ç»ƒä½¿ç”¨ã€‚*
```bash
python data_pipeline/processing/build_pretrain_bins.py
```

**5. æ„å»ºæŒ‡ä»¤å¾®è°ƒæ•°æ® (SFT Bins)**
*å‡†å¤‡é—®ç­”å¯¹æ•°æ®ï¼Œç”¨äºæ•™æ¨¡å‹å¬ä»æŒ‡ä»¤ã€‚*
```bash
python data_pipeline/processing/build_sft_bins.py
```

**6. æ„å»ºåå¥½æ•°æ® (Preference Bins)**
*å‡†å¤‡â€œå¥½å›ç­” vs åå›ç­”â€çš„å¯¹æ¯”æ•°æ®ï¼Œç”¨äºå¥–åŠ±æ¨¡å‹ (RM) å’Œ DPO è®­ç»ƒã€‚*
```bash
python data_pipeline/processing/build_preference_bins.py
```

**7. ä¸‹è½½è¯„ä¼°æç¤ºè¯ (Prompts)**
*ä¸‹è½½ç”¨äºåœ¨çº¿å¼ºåŒ–å­¦ä¹  (PPO/GRPO) çš„ Prompt é›†åˆã€‚*
```bash
python data_pipeline/download/download_prompts.py
```

---

## ğŸ§  ç¬¬ä¸€ç« ï¼šå¤§è„‘çš„è¯ç”Ÿ (Pre-training)
**ç›®æ ‡**ï¼šåœ¨ä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„ç½‘ç»œä¸­æ¶Œç°å‡ºè¯­è¨€èƒ½åŠ›ã€‚
**æ ¸å¿ƒ**ï¼šæ¨¡å‹é˜…è¯»å¤§é‡æ–‡æœ¬ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ (Next Token Prediction)ã€‚

**å¯åŠ¨å‘½ä»¤**:
```bash
python pretrain/train.py --config_path configs/classic_reproductions/{family}/{version}/0_pretrain.yaml --fast_dev_run --compile
```
*   `--fast_dev_run`: å¿«é€Ÿè·‘é€šæµç¨‹ï¼Œåªè®­ç»ƒå°‘é‡æ­¥æ•°ã€‚**æ­£å¼è®­ç»ƒæ—¶è¯·å»æ‰æ­¤å‚æ•°**ã€‚
*   `--compile`: å°è¯•ä½¿ç”¨ `torch.compile` åŠ é€Ÿã€‚

---

## ğŸ“ ç¬¬äºŒç« ï¼šå­¦ä¼šå¬è¯ (Supervised Fine-Tuning)
**ç›®æ ‡**ï¼šå°†é¢„è®­ç»ƒæ¨¡å‹çš„â€œç»­å†™èƒ½åŠ›â€è½¬å˜ä¸ºâ€œæŒ‡ä»¤éµå¾ªèƒ½åŠ›â€ã€‚

### ğŸ‘‘ é€‰é¡¹ A: å…¨é‡å¾®è°ƒ (Full SFT)
*æ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œæ•ˆæœæœ€å¥½ã€‚*
```bash
python finetune/full/sft_train.py --config_path configs/classic_reproductions/{family}/{version}/1_sft_full.yaml --fast_dev_run
```

### ğŸ—¡ï¸ é€‰é¡¹ B: LoRA å¾®è°ƒ
*åªæ›´æ–° 1% çš„å‚æ•° (Adapter)ã€‚*
```bash
python finetune/peft/lora/sft_lora_train.py --config_path configs/classic_reproductions/{family}/{version}/1_sft_lora.yaml --fast_dev_run
```

### ğŸ¤ é€‰é¡¹ C: QLoRA (4-bit)
*å°†å¤§è„‘å‹ç¼©åˆ° 4-bit å†å¾®è°ƒã€‚æåº¦èŠ‚çœå†…å­˜ã€‚*
```bash
python finetune/peft/qlora/sft_qlora_train.py --config_path configs/classic_reproductions/{family}/{version}/1_sft_qlora.yaml --fast_dev_run
```

---

## âš–ï¸ ç¬¬ä¸‰ç« ï¼šæ³¨å…¥çµé­‚ (RLHF & Alignment)
**ç›®æ ‡**ï¼šè®©æ¨¡å‹çš„å›ç­”æ›´ç¬¦åˆäººç±»åå¥½ã€‚

### 3.1 åŸ¹å…»è£åˆ¤ (Reward Model)
*æ•™ä¼šä¸€ä¸ªæ¨¡å‹å»åˆ¤æ–­å›ç­”çš„å¥½åã€‚*
```bash
python align/rm_train.py --config_path configs/classic_reproductions/{family}/{version}/2_rm.yaml --fast_dev_run
```

### 3.2 å¼ºåŒ–å­¦ä¹  (Alignment)
*ä»»é€‰ä¸€ç§æµæ´¾:*

*   **DPO (ç¦»çº¿)**: *ç›´æ¥åœ¨åå¥½æ•°æ®ä¸Šä¼˜åŒ–ï¼Œæ— éœ€ RMã€‚*
    ```bash
    python align/train_offline.py --config_path configs/classic_reproductions/{family}/{version}/3_rlhf_dpo.yaml --fast_dev_run
    ```

*   **GRPO (åœ¨çº¿)**: *DeepSeek æ ¸å¿ƒç§‘æŠ€ã€‚æ—  Criticï¼Œè‡ªåšå¼ˆè¿›åŒ–ã€‚*
    ```bash
    python align/train_online.py --config_path configs/classic_reproductions/{family}/{version}/3_rlhf_grpo.yaml --fast_dev_run
    ```

*   **PPO (åœ¨çº¿)**: *ç»å…¸ RLHF ç®—æ³•ã€‚*
    ```bash
    python align/train_online.py --config_path configs/classic_reproductions/{family}/{version}/3_rlhf_ppo.yaml --fast_dev_run
    ```

---

## ğŸ“ ç¬¬å››ç« ï¼šæœŸæœ«è€ƒè¯• (Evaluation)
æ˜¯æ—¶å€™çœ‹çœ‹æˆ‘ä»¬çš„â€œå­©å­â€å­¦å¾—æ€ä¹ˆæ ·äº†ã€‚æˆ‘ä»¬åœ¨ **GSM8K** (æ•°å­¦) æ•°æ®é›†ä¸Šè¿›è¡Œæµ‹è¯•ã€‚

**1. è€ƒæ ¸é¢„è®­ç»ƒæ¨¡å‹ (Base Model)**
*çœ‹çœ‹æ²¡å—è¿‡æ•™è‚²çš„åŸå§‹å¤§è„‘èƒ½å¾—å‡ åˆ†ã€‚*
```bash
python evaluation/run_leaderboard.py --config_path configs/classic_reproductions/{family}/{version}/0_pretrain.yaml --checkpoint_path runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth --tasks gsm8k --limit 20
```

**2. è€ƒæ ¸ Full SFT æ¨¡å‹**
*çœ‹çœ‹ç»è¿‡æŒ‡ä»¤å¾®è°ƒåï¼Œå®ƒçš„é€»è¾‘èƒ½åŠ›æ˜¯å¦æœ‰æå‡ã€‚*
```bash
python evaluation/run_leaderboard.py --config_path configs/classic_reproductions/{family}/{version}/1_sft_full.yaml --checkpoint_path runs/sft/full/fast-dev-run/checkpoints/ckpt_best.pth --tasks gsm8k --limit 20
```

**3. è€ƒæ ¸ RLHF æ¨¡å‹ (ä»¥ GRPO ä¸ºä¾‹)**
*çœ‹çœ‹å¯¹é½åçš„æ¨¡å‹è¡¨ç°å¦‚ä½•ã€‚*
```bash
python evaluation/run_leaderboard.py --config_path configs/classic_reproductions/{family}/{version}/3_rlhf_grpo.yaml --checkpoint_path runs/rlhf/online/grpo-fast-dev-run/checkpoints/ckpt_best.pth --tasks gsm8k --limit 20
```

---

## ğŸ’¬ ç»ˆç« ï¼šä¸å®ƒå¯¹è¯ (Chat Inference)
æ­å–œï¼ä½ å·²ç»èµ°å®Œäº†å…¨ç¨‹ã€‚

**åŠ è½½ Base æ¨¡å‹:**
```bash
python inference/chat.py --config_path configs/classic_reproductions/{family}/{version}/0_pretrain.yaml --checkpoint_path runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth --quantize
```

**åŠ è½½ QLoRA æ¨¡å‹:**
```bash
python inference/chat.py --config_path configs/classic_reproductions/{family}/{version}/1_sft_qlora.yaml --checkpoint_path runs/pretrain/fast-dev-run/checkpoints/ckpt_best.pth --adapter_path runs/sft/peft/qlora/fast-dev-run/checkpoints/ckpt_best.pth
```
{api_section}

---
*Generated by Project Codex Builder*
"""
            # å†™å…¥ README.md
            with open(model_dir / "README.md", "w", encoding="utf-8") as f:
                f.write(readme_content)

    print("\nâœ… Classic Reproduction Suites generated successfully!")
    print(f"ğŸ“‚ Explore them at: {root_dir}")


if __name__ == "__main__":
    generate_configs()
# END OF FILE: utils/setup_classic_models.py
