# FILE: evaluation/benchmarks/run_gsm8k.py
# -*- coding: utf-8 -*-
"""
[æ–°å¢] GSM8K æ•°å­¦æ¨ç†èƒ½åŠ›è¯„æµ‹è„šæœ¬ (Lightweightç‰ˆ)ã€‚

åŠŸèƒ½:
1. åŠ è½½æŒ‡å®šçš„ Hugging Face æ•°æ®é›† (GSM8K)ã€‚
2. ä½¿ç”¨æˆ‘ä»¬çš„ InferenceEngine è¿›è¡Œå°‘æ ·æœ¬ (Few-shot) æˆ–é›¶æ ·æœ¬ (Zero-shot) æ¨ç†ã€‚
3. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ¨¡å‹ç­”æ¡ˆä¸­çš„æ•°å­—ã€‚
4. ä¸æ ‡å‡†ç­”æ¡ˆè¿›è¡Œæ¯”è¾ƒå¹¶è®¡ç®—å‡†ç¡®ç‡ã€‚
"""
import argparse
import torch
import re
import sys
from pathlib import Path
from tqdm import tqdm
import json

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from inference.engine.engine import InferenceEngine
from tokenizers import Tokenizer

try:
    from datasets import load_dataset
except ImportError:
    print("è¯·å…ˆå®‰è£… datasets åº“: pip install datasets")
    sys.exit(1)


def extract_answer(completion: str) -> str:
    """
    ä» GSM8K çš„å›ç­”ä¸­æå–æœ€ç»ˆçš„æ•°å€¼ç­”æ¡ˆã€‚
    GSM8K çš„æ ‡å‡†å›ç­”é€šå¸¸ä»¥ "#### " ç»“å°¾ï¼Œåè·Ÿæ•°å­—ã€‚
    å¦‚æœæ¨¡å‹æ²¡æœ‰éµå¾ªæ­¤æ ¼å¼ï¼Œæˆ‘ä»¬å°è¯•æå–æœ€åä¸€ä¸ªæ•°å­—ã€‚
    """
    # 1. å°è¯•æ ‡å‡†çš„ "#### " æ ¼å¼
    if "####" in completion:
        answer = completion.split("####")[1].strip()
        # ç§»é™¤å¯èƒ½å­˜åœ¨çš„é€—å·ï¼Œä¾‹å¦‚ 1,234 -> 1234
        return answer.replace(",", "")

    # 2. å¤‡é€‰ï¼šæå–æ–‡æœ¬ä¸­çš„æœ€åä¸€ä¸ªæ•°å­—
    # åŒ¹é…æ•´æ•°æˆ–å°æ•°
    numbers = re.findall(r"-?\d+(?:\.\d+)?", completion)
    if numbers:
        return numbers[-1]

    return ""


def is_correct(model_answer: str, ground_truth: str) -> bool:
    """æ¯”è¾ƒæ¨¡å‹ç­”æ¡ˆå’Œæ ‡å‡†ç­”æ¡ˆæ˜¯å¦æ•°å€¼ç›¸ç­‰ã€‚"""
    try:
        return float(model_answer) == float(ground_truth)
    except ValueError:
        return False


def main():
    parser = argparse.ArgumentParser(description="GSM8K Benchmark è¯„æµ‹")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml)")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth)")
    parser.add_argument("--limit", type=int, default=100, help="è¯„æµ‹æ ·æœ¬æ•°é‡é™åˆ¶ (ä¸ºäº†é€Ÿåº¦)")
    parser.add_argument("--quantize", action="store_true", help="ä½¿ç”¨ Int8 é‡åŒ–åŠ é€Ÿè¯„æµ‹")
    parser.add_argument("--shot", type=int, default=0, help="Few-shot æ ·æœ¬æ•° (0=Zero-shot)")
    args = parser.parse_args()

    # 1. åŠ è½½é…ç½®å’Œæ¨¡å‹
    print("ğŸš€ [GSM8K] åˆå§‹åŒ–...")
    cfg = load_config(args.config_path, Path(project_root))
    tokenizer = Tokenizer.from_file(cfg.data.tokenizer_name)

    # å¼ºåˆ¶å°† pad token è®¾ç½®ä¸º eos tokenï¼Œä»¥ä¾¿è¿›è¡Œ batch padding
    tokenizer.enable_padding(pad_id=tokenizer.token_to_id("<|endoftext|>"), pad_token="<|endoftext|>")

    model = build_model(cfg.model)
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
    model.load_state_dict(checkpoint['model_state_dict'])

    # åˆå§‹åŒ–æ¨ç†å¼•æ“ (æ”¯æŒ batch)
    engine = InferenceEngine(model, tokenizer, quantize=args.quantize)

    # 2. åŠ è½½æ•°æ®
    print("ğŸ“š æ­£åœ¨åŠ è½½ GSM8K æ•°æ®é›† (split='test')...")
    # ä½¿ç”¨ 'main' é…ç½®
    dataset = load_dataset("gsm8k", "main", split=f"test[:{args.limit}]")

    print(f"ğŸ”¥ å¼€å§‹è¯„æµ‹ (Samples: {len(dataset)}, Quantize: {args.quantize})...")

    correct_count = 0
    results = []

    # å®šä¹‰ Prompt æ¨¡æ¿
    # å¯¹äº Base æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨ç»­å†™é£æ ¼ï¼›å¯¹äº Instruct æ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨ Chat æ¨¡æ¿
    # è¿™é‡Œå‡è®¾æ˜¯ Instruct/Chat æ¨¡å‹
    prompt_template = "<|im_start|>user\nQuestion: {question}\nLet's think step by step.<|im_end|>\n<|im_start|>assistant\n"

    # 3. é€ä¸ªè¯„æµ‹ (æœªæ¥å¯ä»¥ä¼˜åŒ–ä¸º Batch è¯„æµ‹)
    pbar = tqdm(dataset)
    for sample in pbar:
        question = sample['question']
        # æå– Ground Truth (GSM8K æ ¼å¼: ".... #### 42")
        ground_truth = extract_answer(sample['answer'])

        prompt = prompt_template.format(question=question)

        # ç”Ÿæˆ
        try:
            # è·å–ç”Ÿæˆçš„å›ç­”éƒ¨åˆ†
            # generate è¿”å›çš„æ˜¯ prompt + completionï¼Œæˆ‘ä»¬éœ€è¦æˆªå–
            full_output = engine.generate([prompt], max_new_tokens=256, temperature=0.0)[0]  # Greedy decoding
            completion = full_output[len(prompt):]

            model_val = extract_answer(completion)

            if is_correct(model_val, ground_truth):
                correct_count += 1
                res_str = "âœ… Correct"
            else:
                res_str = "âŒ Wrong"

            pbar.set_postfix(acc=f"{correct_count / (pbar.n + 1):.2%}")

            results.append({
                "question": question,
                "ground_truth": ground_truth,
                "model_answer": model_val,
                "completion": completion,
                "correct": is_correct(model_val, ground_truth)
            })

        except Exception as e:
            print(f"\nError processing sample: {e}")

    accuracy = correct_count / len(dataset)
    print("\n" + "=" * 50)
    print(f"ğŸ“Š GSM8K Evaluation Result")
    print(f"Samples: {len(dataset)}")
    print(f"Accuracy: {accuracy:.2%}")
    print("=" * 50)

    # ä¿å­˜ç»“æœ
    output_file = Path(project_root) / "evaluation" / f"gsm8k_results_{accuracy:.2f}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"è¯¦ç»†ç»“æœå·²ä¿å­˜è‡³: {output_file}")


if __name__ == "__main__":
    main()
# END OF FILE: evaluation/benchmarks/run_gsm8k.py