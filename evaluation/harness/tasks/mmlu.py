# FILE: evaluation/harness/tasks/mmlu.py
import torch
from tqdm import tqdm
from datasets import load_dataset
from ..base import Benchmark
import numpy as np


class MMLUBenchmark(Benchmark):
    def __init__(self, shot_num: int = 0):
        super().__init__("cais/mmlu", shot_num)
        self.subsets = ["abstract_algebra", "anatomy", "astronomy", "business_ethics", "clinical_knowledge",
                        "college_computer_science"]
        # ä¸ºäº†æ¼”ç¤ºï¼Œåªé€‰äº†å‡ ä¸ªå­é›†ï¼Œå®žé™… MMLU æœ‰ 57 ä¸ª

    def load_data(self):
        # MMLU éœ€è¦æŒ‰ subset åŠ è½½ï¼Œè¿™é‡Œæˆ‘ä»¬ç®€åŒ–é€»è¾‘ï¼ŒåŠ¨æ€åŠ è½½
        pass

    def make_prompt(self, sample) -> str:
        # MMLU Prompt æ ¼å¼:
        # Question: ...
        # A. ...
        # B. ...
        # C. ...
        # D. ...
        # Answer:
        question = sample['question']
        options = sample['choices']
        prompt = f"Question: {question}\n"
        for i, opt in enumerate(options):
            prompt += f"{chr(65 + i)}. {opt}\n"
        prompt += "Answer:"
        return prompt

    def evaluate(self, model, tokenizer, limit: int = None) -> dict:
        print(f"ðŸ“š Evaluating MMLU ({len(self.subsets)} subsets)...")
        model.eval()
        total_correct = 0
        total_samples = 0

        # ç›®æ ‡ Token çš„ ID (A, B, C, D)
        # æ³¨æ„ï¼šåˆ†è¯å™¨ä¸åŒï¼ŒID å¯èƒ½ä¸åŒï¼Œä¸”å¯èƒ½å¸¦ç©ºæ ¼å‰ç¼€ã€‚è¿™é‡Œåšç®€åŒ–å¤„ç†ã€‚
        target_tokens = [
            tokenizer.encode("A").ids[-1],
            tokenizer.encode("B").ids[-1],
            tokenizer.encode("C").ids[-1],
            tokenizer.encode("D").ids[-1]
        ]

        for subset in self.subsets:
            try:
                # åŠ è½½ test split
                ds = load_dataset(self.dataset_name, subset, split="test")
                if limit: ds = ds.select(range(min(limit, len(ds))))
            except Exception as e:
                print(f"âš ï¸ Skipping subset {subset}: {e}")
                continue

            subset_correct = 0

            for sample in tqdm(ds, desc=f"MMLU-{subset}", leave=False):
                prompt = self.make_prompt(sample)
                inputs = tokenizer.encode(prompt)
                input_tensor = torch.tensor([inputs.ids], device=model.device)

                with torch.no_grad():
                    logits = model(input_tensor)
                    # å–æœ€åŽä¸€ä¸ª token çš„ logits
                    next_token_logits = logits[0, -1, :]

                    # åªæ¯”è¾ƒ A, B, C, D çš„æ¦‚çŽ‡
                    target_logits = next_token_logits[target_tokens]
                    pred_idx = torch.argmax(target_logits).item()
                    pred_char = chr(65 + pred_idx)

                    if list("ABCD").index(pred_char) == sample['answer']:
                        subset_correct += 1

            total_correct += subset_correct
            total_samples += len(ds)
            print(f"   > {subset}: {subset_correct}/{len(ds)} ({subset_correct / len(ds):.2%})")

        acc = total_correct / total_samples if total_samples > 0 else 0.0
        print(f"âœ… MMLU Overall Accuracy: {acc:.2%}")
        return {"mmlu_accuracy": acc}
# END OF FILE: evaluation/harness/tasks/mmlu.py