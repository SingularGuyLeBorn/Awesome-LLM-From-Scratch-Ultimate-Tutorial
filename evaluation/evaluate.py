# FILE: evaluation/evaluate.py
# -*- coding: utf-8 -*-
"""
[æ–°å¢] é€šç”¨æ¨¡å‹è¯„ä¼°è„šæœ¬ã€‚
åŠŸèƒ½:
- åŠ è½½æŒ‡å®šçš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚
- åœ¨éªŒè¯é›†ä¸Šè®¡ç®—å¹³å‡æŸå¤±ã€‚
- è®¡ç®—å¹¶æŠ¥å‘Šå›°æƒ‘åº¦ (Perplexity)ã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
from tqdm import tqdm
import torch.nn.functional as F

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from pretrain.data_loader import get_pretrain_loaders
from evaluation.metrics.perplexity import calculate_perplexity


@torch.no_grad()
def run_evaluation(model, val_loader, device):
    """åœ¨éªŒè¯é›†ä¸Šè¿è¡Œè¯„ä¼°ã€‚"""
    model.eval()
    total_loss = 0
    pbar = tqdm(val_loader, desc="[Evaluating]")

    for x, y, loss_mask in pbar:
        x, y, loss_mask = x.to(device), y.to(device), loss_mask.to(device)

        with torch.autocast(device_type=device, dtype=torch.bfloat16 if device == 'cpu' else torch.float16,
                            enabled=True):
            logits = model(x)

            # è®¡ç®—æŸå¤±
            logits_flat = logits.view(-1, logits.size(-1))
            y_flat = y.view(-1)
            loss_mask_flat = loss_mask.view(-1)

            loss = F.cross_entropy(logits_flat, y_flat, reduction='none', ignore_index=-1)
            masked_loss = loss * loss_mask_flat
            avg_loss = masked_loss.sum() / (loss_mask_flat.sum() + 1e-9)

        total_loss += avg_loss.item()
        pbar.set_postfix(loss=f"{avg_loss.item():.4f}")

    avg_val_loss = total_loss / len(val_loader)
    return avg_val_loss


def main():
    parser = argparse.ArgumentParser(description="æ¨¡å‹è¯„ä¼°è„šæœ¬")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--eval_data_limit", type=int, default=1000, help="ç”¨äºè¯„ä¼°çš„éªŒè¯é›†æ–‡æ¡£æ•°é‡ä¸Šé™ã€‚")
    args = parser.parse_args()

    # --- 1. åŠ è½½é…ç½®ã€æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½é…ç½®å’Œæ¨¡å‹...")
    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)

    checkpoint = torch.load(args.checkpoint_path, map_location=cfg.device)
    model.load_state_dict(checkpoint['model_state_dict'])
    print(f"âœ… æˆåŠŸä» '{args.checkpoint_path}' åŠ è½½æ¨¡å‹æƒé‡ã€‚")

    model.to(cfg.device)

    # --- 2. å‡†å¤‡æ•°æ®åŠ è½½å™¨ ---
    print("\nğŸ“š æ­£åœ¨å‡†å¤‡éªŒè¯æ•°æ®åŠ è½½å™¨...")
    _, val_loader = get_pretrain_loaders(
        tokenizer_name=cfg.data.tokenizer_name,
        data_dir=Path(cfg.data.data_dir),
        block_size=cfg.model.max_seq_len,
        batch_size=cfg.training.batch_size,
        train_data_limit=0,  # æˆ‘ä»¬ä¸éœ€è¦è®­ç»ƒé›†
        val_data_limit=args.eval_data_limit,
        return_train_loader=False  # åªè¿”å›éªŒè¯åŠ è½½å™¨
    )

    # --- 3. è¿è¡Œè¯„ä¼° ---
    print("\nğŸ”¬ å¼€å§‹è¯„ä¼°...")
    avg_loss = run_evaluation(model, val_loader, cfg.device)
    perplexity = calculate_perplexity(avg_loss)

    # --- 4. æŠ¥å‘Šç»“æœ ---
    print("\n" + "=" * 50)
    print(f"{'è¯„ä¼°ç»“æœ':^50}")
    print("=" * 50)
    print(f"  - æ¨¡å‹æ£€æŸ¥ç‚¹: {args.checkpoint_path}")
    print(f"  - éªŒè¯é›†æ–‡æ¡£æ•°: {args.eval_data_limit}")
    print("-" * 50)
    print(f"  - å¹³å‡éªŒè¯æŸå¤± (Avg. Loss): {avg_loss:.4f}")
    print(f"  - å›°æƒ‘åº¦ (Perplexity): {perplexity:.2f}")
    print("=" * 50)


if __name__ == "__main__":
    main()
# END OF FILE: evaluation/evaluate.py