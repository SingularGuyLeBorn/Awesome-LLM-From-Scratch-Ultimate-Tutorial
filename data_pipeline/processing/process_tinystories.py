# FILE: data_pipeline/processing/process_tinystories.py
"""
ã€å·¥ä¸šçº§æ€§èƒ½ç‰ˆã€‘å¤„ç†ä¸‹è½½å¥½çš„TinyStoriesæ•°æ®é›†:
é‡‡ç”¨ä¸¤éæ‰«ææ³•ï¼Œå®ç°å†…å­˜é«˜æ•ˆã€çº¿æ€§çš„Tokenæ‹¼æ¥ã€‚
"""
import os
from pathlib import Path
from datasets import load_from_disk
from tqdm import tqdm
import numpy as np
from tokenizers import Tokenizer
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def process_tinystories():
    # --- é…ç½® ---
    TOKENIZER_NAME = "tinystories_project_vs4096.json"

    # å®šä¹‰æ•°æ®è·¯å¾„
    raw_data_path = Path(__file__).parent.parent / "downloaded_data" / "TinyStories"
    processed_data_path = Path(__file__).parent.parent / "processed_data"
    tokenizer_path = processed_data_path / TOKENIZER_NAME

    # æ£€æŸ¥
    if not raw_data_path.exists():
        logging.error(f"åŸå§‹æ•°æ®ç›®å½• '{raw_data_path}' ä¸å­˜åœ¨ã€‚")
        logging.error("è¯·å…ˆè¿è¡Œ 'data_pipeline/download/download_tinystories.py' ä¸‹è½½æ•°æ®ã€‚")
        return
    if not tokenizer_path.exists():
        logging.error(f"åˆ†è¯å™¨ '{tokenizer_path}' ä¸å­˜åœ¨ã€‚")
        logging.error("è¯·å…ˆè¿è¡Œ 'data_pipeline/tokenizer/train_tokenizer.py' è®­ç»ƒåˆ†è¯å™¨ã€‚")
        return

    processed_data_path.mkdir(parents=True, exist_ok=True)
    train_file = processed_data_path / "train.bin"
    val_file = processed_data_path / "val.bin"

    if train_file.exists() and val_file.exists():
        logging.info("âœ… é¢„å¤„ç†åçš„ .bin æ–‡ä»¶å·²å­˜åœ¨ã€‚è·³è¿‡å¤„ç†ã€‚")
        return

    # 1. åŠ è½½æ•°æ®é›†å’Œåˆ†è¯å™¨
    logging.info("ğŸ’¿ æ­£åœ¨ä»æœ¬åœ°ç£ç›˜åŠ è½½æ•°æ®é›†...")
    dataset = load_from_disk(str(raw_data_path))

    logging.info(f"ğŸ¤– æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {TOKENIZER_NAME}...")
    tokenizer = Tokenizer.from_file(str(tokenizer_path))
    eos_token_id = tokenizer.token_to_id("<|endoftext|>")
    if eos_token_id is None:
        logging.error("âŒ é”™è¯¯: åˆ†è¯å™¨ä¸­æœªæ‰¾åˆ° <|endoftext|> tokenã€‚")
        return

    # 2. å®šä¹‰ç¼–ç å‡½æ•°
    def encode_batch(batch):
        texts = batch['text']
        encodings = tokenizer.encode_batch(texts)
        all_ids = [encoding.ids + [eos_token_id] for encoding in encodings]
        return {"ids": all_ids}

    # 3. ä½¿ç”¨.map()è¿›è¡Œå¤šè¿›ç¨‹å¹¶è¡Œç¼–ç 
    logging.info("ğŸ“– æ­£åœ¨ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œç¼–ç æ‰€æœ‰æ•…äº‹æ–‡æœ¬...")
    num_proc = max(1, os.cpu_count() // 2)
    logging.info(f"   ä½¿ç”¨ {num_proc} ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç†ã€‚")

    encoded_dataset = dataset.map(
        encode_batch,
        batched=True,
        num_proc=num_proc,
        remove_columns=dataset.column_names
    )

    # --- æ ¸å¿ƒä¿®å¤ï¼šå·¥ä¸šçº§æ‹¼æ¥æ–¹æ¡ˆ ---
    logging.info("âœ¨ ç¼–ç å®Œæˆï¼Œå¼€å§‹é«˜æ•ˆæ‹¼æ¥...")

    # 4. ç¬¬ä¸€éæ‰«æï¼šè®¡ç®—æ€»Tokenæ•°
    total_tokens_count = sum(len(ids) for ids in tqdm(encoded_dataset['ids'], desc="Pass 1: è®¡ç®—æ€»é•¿åº¦"))
    logging.info(f"   è®¡ç®—å¾—å‡ºæ€»tokensæ•°: {total_tokens_count:,}")

    # 5. ç¬¬äºŒéæ‰«æï¼šä¸€æ¬¡æ€§åˆ†é…å†…å­˜å¹¶å¡«å……
    all_tokens = np.empty(total_tokens_count, dtype=np.uint16)
    current_position = 0
    for ids in tqdm(encoded_dataset['ids'], desc="Pass 2: å¡«å……æ•°æ®"):
        length = len(ids)
        all_tokens[current_position: current_position + length] = ids
        current_position += length

    assert current_position == total_tokens_count, "æ‹¼æ¥åé•¿åº¦ä¸ä¸€è‡´ï¼Œå‡ºç°é”™è¯¯ï¼"

    # 6. åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split_ratio = 0.9
    split_index = int(len(all_tokens) * split_ratio)

    train_tokens = all_tokens[:split_index]
    val_tokens = all_tokens[split_index:]

    logging.info(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†å®Œæ¯•:")
    logging.info(f"   - è®­ç»ƒé›†tokensæ•°: {len(train_tokens):,}")
    logging.info(f"   - éªŒè¯é›†tokensæ•°: {len(val_tokens):,}")

    # 7. ä¿å­˜ä¸ºäºŒè¿›åˆ¶æ–‡ä»¶
    logging.info(f"\nğŸ’¾ æ­£åœ¨å°†è®­ç»ƒé›†ä¿å­˜åˆ° '{train_file}'...")
    train_tokens.tofile(train_file)
    logging.info(f"ğŸ’¾ æ­£åœ¨å°†éªŒè¯é›†ä¿å­˜åˆ° '{val_file}'...")
    val_tokens.tofile(val_file)

    logging.info("\nâœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    from multiprocessing import freeze_support

    freeze_support()

    process_tinystories()
# END OF FILE: data_pipeline/processing/process_tinystories.py