# FILE: build_pretrain_bins.py
"""
ã€æ­¥éª¤äºŒï¼šæ„å»ºäºŒè¿›åˆ¶æ–‡ä»¶ - ç»ˆæåŠ é€Ÿç‰ˆã€‘
çœŸæ­£çš„é›¶æ‹·è´å®ç°ï¼šç›´æ¥æ“ä½œ PyArrow çš„åº•å±‚ Buffer
"""
from pathlib import Path
from datasets import load_from_disk
import numpy as np
import logging
import pyarrow as pa
import pyarrow.compute as pc

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def build_bins():
    SPLIT_RATIO = 0.9

    processed_data_path = Path(__file__).parent.parent / "processed_data"
    encoded_dataset_path = processed_data_path / "TinyStories_encoded"
    train_file = processed_data_path / "train.bin"
    val_file = processed_data_path / "val.bin"

    if not encoded_dataset_path.exists():
        logging.error(f"å·²ç¼–ç çš„æ•°æ®é›†ç›®å½• '{encoded_dataset_path}' ä¸å­˜åœ¨ã€‚")
        logging.error("è¯·å…ˆè¿è¡Œ 'encode_stories.py'ã€‚")
        return
    if train_file.exists() and val_file.exists():
        logging.info("âœ… æœ€ç»ˆçš„ .bin æ–‡ä»¶å·²å­˜åœ¨ã€‚è·³è¿‡æ„å»ºæ­¥éª¤ã€‚")
        return

    logging.info(f"ğŸ’¿ æ­£åœ¨ä» '{encoded_dataset_path}' åŠ è½½å·²ç¼–ç çš„æ•°æ®é›†...")
    encoded_dataset = load_from_disk(str(encoded_dataset_path))

    logging.info("ğŸš€ å¼€å§‹ç»ˆæåŠ é€Ÿæ‹¼æ¥ï¼ˆçœŸé›¶æ‹·è´ï¼‰...")

    # æ–¹æ¡ˆï¼šç›´æ¥ä»æ¯ä¸ª chunk æå–æ‰å¹³åŒ–æ•°æ®ï¼Œç„¶åç”¨ concatenate
    chunked_array = encoded_dataset.data.column('ids')
    logging.info(f"   PyArrow ChunkedArray åŒ…å« {chunked_array.num_chunks} ä¸ªæ•°æ®å—")

    # æ”¶é›†æ‰€æœ‰æ‰å¹³åŒ–çš„ chunk
    flat_chunks = []
    total_tokens = 0

    for i, chunk in enumerate(chunked_array.chunks):
        # chunk æ˜¯ä¸€ä¸ª ListArrayï¼Œæˆ‘ä»¬è¦å®ƒçš„ .values (æ‰å¹³çš„ Array)
        if hasattr(chunk, 'values'):
            flat_chunk = chunk.values
        elif hasattr(chunk, 'flatten'):
            # å¦‚æœæ˜¯ ListArrayï¼Œflatten ä¼šç»™æˆ‘ä»¬æ‰å¹³æ•°æ®
            flat_chunk = chunk.flatten()
            if isinstance(flat_chunk, list):
                # è¿˜æ˜¯ listï¼Ÿè½¬æ¢ä¸º PyArrow Array
                flat_chunk = pa.array(flat_chunk, type=pa.uint16())
        else:
            # æœ€åçš„å…œåº•æ–¹æ¡ˆ
            logging.warning(f"   è­¦å‘Šï¼šchunk {i} ç±»å‹å¼‚å¸¸ï¼Œä½¿ç”¨æ…¢é€Ÿè½¬æ¢")
            flat_chunk = pa.array([item for sublist in chunk.to_pylist() for item in sublist],
                                  type=pa.uint16())

        flat_chunks.append(flat_chunk)
        total_tokens += len(flat_chunk)

        if (i + 1) % 500 == 0:
            logging.info(f"   å·²å¤„ç† {i + 1}/{chunked_array.num_chunks} ä¸ªæ•°æ®å—ï¼Œç´¯è®¡ {total_tokens:,} tokens")

    logging.info(f"   æ‰€æœ‰æ•°æ®å—å¤„ç†å®Œæˆï¼Œæ€»è®¡ {total_tokens:,} tokens")

    # ä½¿ç”¨ PyArrow çš„ concatenate - è¿™æ˜¯çœŸæ­£çš„é›¶æ‹·è´æ“ä½œ
    logging.info("   æ­£åœ¨æ‰§è¡Œ PyArrow concatenateï¼ˆé›¶æ‹·è´ï¼‰...")
    all_tokens_arrow = pa.concat_arrays(flat_chunks)

    # è½¬ä¸º NumPy - å°½å¯èƒ½é›¶æ‹·è´
    logging.info("   æ­£åœ¨è½¬æ¢ä¸º NumPy æ•°ç»„...")
    all_tokens = all_tokens_arrow.to_numpy(zero_copy_only=False)

    # éªŒè¯æ•°æ®ç±»å‹
    if all_tokens.dtype != np.uint16:
        logging.warning(f"   ç±»å‹ä¸åŒ¹é…ï¼ˆ{all_tokens.dtype}ï¼‰ï¼Œæ‰§è¡Œè½¬æ¢...")
        all_tokens = all_tokens.astype(np.uint16)
    else:
        logging.info(f"   âœ… æ•°æ®å·²ä¸º np.uint16ï¼Œæ— éœ€è½¬æ¢")

    logging.info(f"   æ‹¼æ¥å®Œæˆï¼æ€» tokens æ•°: {len(all_tokens):,}")

    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    split_index = int(len(all_tokens) * SPLIT_RATIO)
    train_tokens = all_tokens[:split_index]
    val_tokens = all_tokens[split_index:]

    logging.info(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
    logging.info(f"   - è®­ç»ƒé›†: {len(train_tokens):,} tokens")
    logging.info(f"   - éªŒè¯é›†: {len(val_tokens):,} tokens")

    # ä¿å­˜
    logging.info(f"\nğŸ’¾ ä¿å­˜è®­ç»ƒé›†åˆ° '{train_file}'...")
    train_tokens.tofile(train_file)
    logging.info(f"ğŸ’¾ ä¿å­˜éªŒè¯é›†åˆ° '{val_file}'...")
    val_tokens.tofile(val_file)

    logging.info("\nğŸ‰ æ­¥éª¤äºŒå®Œæˆï¼å¯ä»¥å¼€å§‹è®­ç»ƒäº†ã€‚")


if __name__ == "__main__":
    build_bins()