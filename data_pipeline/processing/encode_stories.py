# FILE: data_pipeline/processing/encode_stories.py
"""
ã€æ­¥éª¤ä¸€ï¼šç¼–ç  - æ€§èƒ½ä¼˜åŒ–ç‰ˆã€‘
å•ä¸€èŒè´£ï¼šåŠ è½½åŸå§‹æ•°æ®é›†ï¼Œä½¿ç”¨æŒ‡å®šåˆ†è¯å™¨è¿›è¡Œå¤šè¿›ç¨‹å¹¶è¡Œç¼–ç ï¼Œ
å¹¶å°†ç»“æœä»¥é«˜æ•ˆçš„Arrowæ ¼å¼ä¿å­˜ä¸ºä¸­é—´æ–‡ä»¶ã€‚

ä¼˜åŒ–ç‚¹:
1. å¢å¤§ batch_size ä»¥æé«˜ map ååé‡ã€‚
2. è°ƒæ•´CPUæ ¸å¿ƒæ•°ï¼ˆä¾‹å¦‚ os.cpu_count() - 2ï¼‰ï¼Œå¯ä»¥æ ¹æ®ç³»ç»Ÿè°ƒæ•´ã€‚
3. (å…³é”®) åœ¨ä¿å­˜å‰ï¼Œå¹¶è¡Œåœ°å°† token ID å¼ºåˆ¶è½¬æ¢ä¸º uint16ï¼Œ
   è¿™æ˜¯ä¸ºäº†æå¤§åŠ é€Ÿä¸‹ä¸€æ­¥éª¤ (build_pretrain_bins.py)ï¼Œ
   é¿å…å…¶åœ¨ä¸»è¿›ç¨‹ä¸­è¿›è¡Œæ˜‚è´µçš„å†…å­˜å¤åˆ¶å’Œç±»å‹è½¬æ¢ã€‚
"""
import os
from pathlib import Path
from datasets import load_from_disk, Features, Value, Sequence
from tokenizers import Tokenizer
import logging
from multiprocessing import freeze_support

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def encode_stories():
    # --- é…ç½® ---
    TOKENIZER_NAME = "tinystories_project_vs4096.json"
    # ä¼˜åŒ–ç‚¹: ä¸º map æ“ä½œè®¾ç½®ä¸€ä¸ªæ›´å¤§çš„æ‰¹å¤„ç†å¤§å°ï¼Œ
    # å°¤å…¶æ˜¯å½“å¤„ç†åƒ TinyStories è¿™æ ·çš„çŸ­æ–‡æœ¬æ—¶ï¼Œ
    # è¿™å¯ä»¥æ˜¾è‘—å‡å°‘è¿›ç¨‹è°ƒåº¦çš„å¼€é”€ã€‚
    PROCESSING_BATCH_SIZE = 10_000

    # å®šä¹‰è·¯å¾„
    raw_data_path = Path(__file__).parent.parent / "downloaded_data" / "TinyStories"
    processed_data_path = Path(__file__).parent.parent / "processed_data"
    tokenizer_path = processed_data_path / TOKENIZER_NAME
    output_path = processed_data_path / "TinyStories_encoded"

    # --- æ£€æŸ¥ ---
    if not raw_data_path.exists():
        logging.error(f"åŸå§‹æ•°æ®ç›®å½• '{raw_data_path}' ä¸å­˜åœ¨ã€‚")
        return
    if not tokenizer_path.exists():
        logging.error(f"åˆ†è¯å™¨ '{tokenizer_path}' ä¸å­˜åœ¨ã€‚")
        return
    if output_path.exists():
        logging.info(f"âœ… å·²ç¼–ç çš„æ•°æ®é›† '{output_path}' å·²å­˜åœ¨ã€‚è·³è¿‡ç¼–ç æ­¥éª¤ã€‚")
        return

    # 1. åŠ è½½
    logging.info("ğŸ’¿ æ­£åœ¨ä»æœ¬åœ°ç£ç›˜åŠ è½½åŸå§‹æ•°æ®é›†...")
    dataset = load_from_disk(str(raw_data_path))

    logging.info(f"ğŸ¤– æ­£åœ¨åŠ è½½åˆ†è¯å™¨: {TOKENIZER_NAME}...")
    tokenizer = Tokenizer.from_file(str(tokenizer_path))
    eos_token_id = tokenizer.token_to_id("<|endoftext|>")
    if eos_token_id is None:
        logging.error("âŒ é”™è¯¯: åˆ†è¯å™¨ä¸­æœªæ‰¾åˆ° <|endoftext|> tokenã€‚")
        return
    # å‡è®¾è¯æ±‡è¡¨å¤§å°å°äº 65535 (uint16 çš„æœ€å¤§å€¼)
    if eos_token_id > 65535 or tokenizer.get_vocab_size() > 65536:
        logging.warning("âš ï¸ è­¦å‘Š: è¯æ±‡è¡¨å¤§å°æˆ– EOS token ID è¶…å‡º uint16 èŒƒå›´ã€‚")
        logging.warning("   å¦‚æœä¸‹ä¸€æ­¥çš„ 'build_pretrain_bins.py' å¤±è´¥ï¼Œ")
        logging.warning("   è¯·å°†æ­¤è„šæœ¬ä¸­çš„ 'uint16' æ”¹ä¸º 'uint32'ã€‚")


    # 2. å®šä¹‰ç¼–ç å‡½æ•°
    def encode_batch(batch):
        texts = batch['text']
        encodings = tokenizer.encode_batch(texts)
        # æ³¨æ„ï¼šè¿™é‡Œè¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å« token ID åˆ—è¡¨çš„åˆ—è¡¨
        all_ids = [encoding.ids + [eos_token_id] for encoding in encodings]
        return {"ids": all_ids}

    # 3. å¤šè¿›ç¨‹å¹¶è¡Œç¼–ç 
    logging.info("ğŸ“– æ­£åœ¨ä½¿ç”¨å¤šè¿›ç¨‹å¹¶è¡Œç¼–ç æ‰€æœ‰æ•…äº‹æ–‡æœ¬...")
    # ä¼˜åŒ–ç‚¹: è°ƒæ•´è¿›ç¨‹æ•°ã€‚
    # `os.cpu_count() // 2` æ˜¯ä¸€ä¸ªä¿å®ˆçš„é€‰æ‹©ã€‚
    # å°è¯• `os.cpu_count() - 2` (ç•™å‡ºç³»ç»Ÿå’ŒI/Oæ ¸å¿ƒ) å¯èƒ½ä¼šæ›´å¿«ã€‚
    num_proc = max(1, os.cpu_count() - 2 if os.cpu_count() > 2 else 1)
    logging.info(f"   ä½¿ç”¨ {num_proc} ä¸ªè¿›ç¨‹è¿›è¡Œå¤„ç† (batch_size={PROCESSING_BATCH_SIZE})ã€‚")

    encoded_dataset = dataset.map(
        encode_batch,
        batched=True,
        num_proc=num_proc,
        batch_size=PROCESSING_BATCH_SIZE, # ä¼˜åŒ–ç‚¹: å¢å¤§ batch_size
        remove_columns=dataset.column_names
    )

    # 4. å…³é”®ä¼˜åŒ–ï¼šåœ¨ä¿å­˜å‰è½¬æ¢ç±»å‹
    # è¿™æ˜¯ä¸ºäº†ä¼˜åŒ–ä¸‹ä¸€æ­¥ (build_pretrain_bins.py)ã€‚
    # åœ¨è¿™é‡Œä½¿ç”¨å¤šè¿›ç¨‹å°† 'ids' åˆ—è½¬æ¢ä¸º uint16ï¼Œ
    # è¿™æ ·ä¸‹ä¸€æ­¥è„šæœ¬åŠ è½½æ•°æ®æ—¶ï¼Œæ•°æ®å·²ç»æ˜¯æ­£ç¡®çš„ç±»å‹ (np.uint16)ï¼Œ
    # ä»è€Œé¿å…åœ¨ä¸»è¿›ç¨‹ä¸­æ‰§è¡Œä¸€ä¸ªå·¨å¤§ã€æ˜‚è´µã€ä¸”å ç”¨å¤§é‡å†…å­˜çš„ .astype() æ“ä½œã€‚
    logging.info(f"   ...ç¼–ç å®Œæˆã€‚æ­£åœ¨å¹¶è¡Œè½¬æ¢ä¸º uint16...")
    target_features = Features({
        'ids': Sequence(Value('uint16'))
    })
    encoded_dataset = encoded_dataset.cast(target_features, num_proc=num_proc)
    logging.info("   ...ç±»å‹è½¬æ¢å®Œæˆã€‚")


    # 5. ä¿å­˜ä¸ºArrowæ ¼å¼
    logging.info(f"ğŸ’¾ æ­£åœ¨å°†ç¼–ç åçš„ (uint16) æ•°æ®é›†ä»¥Arrowæ ¼å¼ä¿å­˜åˆ°: {output_path}")
    encoded_dataset.save_to_disk(str(output_path))

    logging.info("\nâœ… æ­¥éª¤ä¸€ï¼šç¼–ç ï¼ˆä¼˜åŒ–ç‰ˆï¼‰å®Œæˆï¼")
    logging.info(f"ä¸‹ä¸€æ­¥ï¼Œè¯·è¿è¡Œ 'build_pretrain_bins.py' æ¥æ„å»ºæœ€ç»ˆçš„ .bin è®­ç»ƒæ–‡ä»¶ã€‚")


if __name__ == "__main__":
    # åœ¨Windowsä¸Šï¼Œä¸ºäº†è®©å¤šè¿›ç¨‹æ­£å¸¸å·¥ä½œï¼Œéœ€è¦è¿™è¡Œä»£ç 
    freeze_support()
    encode_stories()
# END OF FILE: data_pipeline/processing/encode_stories.py