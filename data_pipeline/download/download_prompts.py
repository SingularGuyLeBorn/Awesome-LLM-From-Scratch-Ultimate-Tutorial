# FILE: data_pipeline/download/download_prompts.py
# -*- coding: utf-8 -*-
"""
[v2.1 - å¥å£®æ€§ä¿®å¤ç‰ˆ] ä» Hugging Face Hub ä¸‹è½½é«˜è´¨é‡ Prompt æ•°æ®é›†ã€‚
- åˆ‡æ¢åˆ°æ›´ç¨³å®šã€å®˜æ–¹ç»´æŠ¤çš„ HuggingFaceH4/instruction-dataset æ•°æ®é›†ã€‚
- å¢åŠ äº†æœ¬åœ°å¤„ç†æ­¥éª¤ï¼Œå°†ä¸‹è½½çš„JSONLè½¬æ¢ä¸ºç®€å•çš„txtæ–‡ä»¶ã€‚
"""
from pathlib import Path
import logging
import json
from tqdm import tqdm

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

try:
    from datasets import load_dataset
except ImportError:
    logging.error("datasets æœªå®‰è£…ã€‚è¯·è¿è¡Œ: pip install datasets")
    load_dataset = None


def download_and_process_h4_prompts(limit: int = 5000):
    """
    ä» HuggingFaceH4/instruction-dataset ä¸‹è½½æ•°æ®ï¼Œå¹¶æå– prompts ä¿å­˜ä¸º txt æ–‡ä»¶ã€‚
    """
    if not load_dataset:
        return

    repo_id = "HuggingFaceH4/instruction-dataset"
    # [æ ¸å¿ƒä¿®å¤] æ ¹æ®é”™è¯¯æ—¥å¿—ï¼Œå°†æ•°æ®é›†åˆ†ç‰‡åç§°ä» "helpful_base" ä¿®æ­£ä¸º "test"
    subset = "test"

    # å°†æ–‡ä»¶ä¿å­˜åˆ° data_pipeline/prompts/ ç›®å½•ä¸‹
    output_dir = Path(__file__).parent.parent / "prompts"
    output_dir.mkdir(exist_ok=True)
    output_path = output_dir / "h4_prompts.txt"
    # [åŒæ­¥ä¿®æ”¹] æ›´æ–°ç¼“å­˜æ–‡ä»¶åä»¥ä¿æŒä¸€è‡´æ€§
    jsonl_cache_path = output_dir / "h4_test.jsonl"

    if output_path.exists():
        logging.info(f"âœ… é«˜è´¨é‡Promptæ•°æ®é›†å·²å­˜åœ¨äº: '{output_path}'")
        return

    logging.info(f"ğŸ“¥ æ­£åœ¨ä» Hugging Face Hub ({repo_id}) ä¸‹è½½é«˜è´¨é‡Promptæ•°æ®é›†...")

    try:
        # ä½¿ç”¨æµå¼ä¸‹è½½ä»¥èŠ‚çœå†…å­˜ï¼Œå¹¶åªå–éœ€è¦çš„éƒ¨åˆ†
        # ä½¿ç”¨ä¿®æ­£åçš„ 'subset' å˜é‡
        dataset = load_dataset(repo_id, split=f"{subset}[:{limit}]", streaming=False)

        # å°†å…¶ä¿å­˜ä¸º jsonl æ–‡ä»¶ä½œä¸ºç¼“å­˜ï¼Œä¾¿äºè°ƒè¯•
        dataset.to_json(jsonl_cache_path)
        logging.info(f"å·²å°†åŸå§‹æ•°æ®ç¼“å­˜åˆ°: '{jsonl_cache_path}'")

        logging.info(f"âœï¸ æ­£åœ¨å¤„ç† JSONL æ–‡ä»¶å¹¶æå– prompts åˆ° '{output_path}'...")

        with open(jsonl_cache_path, 'r', encoding='utf-8') as f_in, \
                open(output_path, 'w', encoding='utf-8') as f_out:

            for line in tqdm(f_in, desc="Extracting prompts"):
                try:
                    data = json.loads(line)
                    prompt = data.get("prompt")
                    if prompt:
                        # å†™å…¥æ–‡ä»¶ï¼Œæ¯ä¸ªpromptå ä¸€è¡Œ
                        f_out.write(prompt.strip() + "\n")
                except json.JSONDecodeError:
                    logging.warning(f"è·³è¿‡æ— æ•ˆçš„JSONè¡Œ: {line.strip()}")

        logging.info(f"âœ… æ•°æ®é›†ä¸‹è½½ã€å¤„ç†å¹¶ä¿å­˜åˆ°: '{output_path}'")

    except Exception as e:
        logging.error(f"âŒ ä¸‹è½½æˆ–å¤„ç†å¤±è´¥: {e}")
        logging.error("è¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥æˆ– datasets ç‰ˆæœ¬ã€‚")
        logging.error("å¦‚æœé‡åˆ°ç½‘ç»œé—®é¢˜ï¼Œå¯ä»¥å°è¯•è®¾ç½® HF_ENDPOINT ç¯å¢ƒå˜é‡ã€‚")


if __name__ == "__main__":
    # æˆ‘ä»¬åªä¸‹è½½å‰5000æ¡ä½œä¸ºpromptæ± ï¼Œå¯¹äºCPUè®­ç»ƒæ¥è¯´å®Œå…¨è¶³å¤Ÿäº†
    download_and_process_h4_prompts(limit=5000)
# END OF FILE: data_pipeline/download/download_prompts.py