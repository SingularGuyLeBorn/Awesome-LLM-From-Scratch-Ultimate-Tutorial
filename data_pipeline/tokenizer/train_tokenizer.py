# FILE: data_pipeline/tokenizer/train_tokenizer.py
"""
ã€é¡¹ç›®ä¸»åˆ†è¯å™¨è®­ç»ƒè„šæœ¬ã€‘
ä½¿ç”¨HuggingFace Tokenizersåº“è®­ç»ƒBPEåˆ†è¯å™¨ï¼ˆRustå†…æ ¸ï¼Œé€Ÿåº¦æå¿«ï¼‰ã€‚

è¿™ä¸ªè„šæœ¬å°†ç”¨äºè®­ç»ƒæˆ‘ä»¬é¡¹ç›®ä¸­å®é™…ä½¿ç”¨çš„åˆ†è¯å™¨ã€‚
å®ƒç»è¿‡äº†å¢å¼ºï¼ŒåŠ å…¥äº†SFTï¼ˆæŒ‡ä»¤å¾®è°ƒï¼‰é˜¶æ®µå¿…éœ€çš„ç‰¹æ®Šè¯å…ƒã€‚

---
ç”¨æ³•ç¤ºä¾‹:
---

1.  âœ… å¿«é€Ÿæµ‹è¯• (4kè¯è¡¨, 20MBæ•°æ®):
    # ç”¨äºå¿«é€ŸéªŒè¯æµç¨‹æ˜¯å¦è·‘é€š
    python data_pipeline/tokenizer/train_tokenizer.py --vocab_size 4096 --data_limit_mb 20

2.  ğŸ”¥ SFTå¢å¼ºç‰ˆè®­ç»ƒ (æ¨è, 10kè¯è¡¨, 200MBæ•°æ®):
    # ä¸ºåç»­çš„SFTå’ŒDPOé˜¶æ®µåšå‡†å¤‡ï¼Œç”Ÿæˆä¸€ä¸ªé«˜è´¨é‡çš„åˆ†è¯å™¨
    # è€—æ—¶çº¦å‡ åˆ†é’Ÿ
    python data_pipeline/tokenizer/train_tokenizer.py --vocab_size 10000 --data_limit_mb 200

3.   marathon å®Œæ•´é¢„è®­ç»ƒç‰ˆ (16kè¯è¡¨, å…¨éƒ¨æ•°æ®):
    # å¦‚æœè¦ä»é›¶å¼€å§‹è¿›è¡Œå¤§è§„æ¨¡é¢„è®­ç»ƒï¼Œå¯ä»¥ä½¿ç”¨æ­¤é…ç½®
    # !!! è­¦å‘Š: ä¼šæ¶ˆè€—å¤§é‡å†…å­˜å’Œè¾ƒé•¿æ—¶é—´ !!!
    python data_pipeline/tokenizer/train_tokenizer.py --vocab_size 16384 --data_limit_mb 0
"""

import argparse
from pathlib import Path
import time
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, decoders
import sys

# å°†é¡¹ç›®æ ¹ç›®å½•æ·»åŠ åˆ°è·¯å¾„ï¼Œä»¥ä¾¿å¯¼å…¥utils
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)
from utils.file_utils import create_subset_file


def main():
    parser = argparse.ArgumentParser(
        description="ã€ä¸»è„šæœ¬ã€‘ä½¿ç”¨HuggingFace Tokenizersè®­ç»ƒä¸€ä¸ªç”¨äºé¡¹ç›®çš„é«˜æ€§èƒ½BPEåˆ†è¯å™¨ã€‚",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument("--vocab_size", type=int, default=10000, help="ç›®æ ‡è¯è¡¨å¤§å°")
    parser.add_argument("--data_limit_mb", type=int, default=200, help="ç”¨äºè®­ç»ƒçš„æ•°æ®é‡ä¸Šé™(MB)ï¼Œ0è¡¨ç¤ºæ— é™åˆ¶")
    args = parser.parse_args()

    # --- è·¯å¾„è®¾ç½® ---
    data_path = Path(__file__).parent.parent / "processed_data"
    train_file = data_path / "train.txt"
    output_file = data_path / f"tinystories_project_vs{args.vocab_size}.json"

    if not train_file.exists():
        print(f"âŒ é”™è¯¯: '{train_file}' ä¸å­˜åœ¨")
        print("è¯·å…ˆè¿è¡Œ 'data_pipeline/processing/process_tinystories.py'")
        return

    if output_file.exists():
        print(f"âŒ æ¨¡å‹å·²å­˜åœ¨: {output_file}")
        print("å¦‚æœä½ æƒ³é‡æ–°è®­ç»ƒï¼Œè¯·å…ˆæ‰‹åŠ¨åˆ é™¤è¯¥æ–‡ä»¶ï¼Œæˆ–æŒ‡å®šä¸€ä¸ªä¸åŒçš„ vocab_sizeã€‚")
        return

    print("=" * 80)
    print(f"ğŸš€ å¼€å§‹è®­ç»ƒé¡¹ç›®ä¸»åˆ†è¯å™¨ (ç›®æ ‡è¯è¡¨: {args.vocab_size})")
    print("=" * 80)

    # --- å‡†å¤‡è®­ç»ƒæ•°æ® ---
    temp_file = None
    if args.data_limit_mb > 0:
        print(f"ğŸ“š æ­£åœ¨åˆ›å»º {args.data_limit_mb}MB çš„æ•°æ®å­é›†ç”¨äºè®­ç»ƒ...")
        temp_file = data_path / "temp_train_subset.txt"
        create_subset_file(train_file, temp_file, args.data_limit_mb)
        print(f"   -> å·²åˆ›å»ºä¸´æ—¶å­é›†æ–‡ä»¶: {temp_file.name} ({temp_file.stat().st_size / 1e6:.2f} MB)")
        train_files = [str(temp_file)]

    else:
        print(f"ğŸ“š ä½¿ç”¨å®Œæ•´è®­ç»ƒæ•°æ®: {train_file.stat().st_size / 1e6:.1f}MB")
        train_files = [str(train_file)]

    # --- 1. åˆå§‹åŒ–åˆ†è¯å™¨æ¨¡å‹ ---
    print("\n1/3: åˆå§‹åŒ–BPEæ¨¡å‹...")
    tokenizer = Tokenizer(models.BPE())

    # --- 2. é…ç½®é¢„åˆ†è¯å™¨å’Œè§£ç å™¨ ---
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.decoder = decoders.ByteLevel()

    # --- 3. å®šä¹‰è®­ç»ƒå™¨å’Œç‰¹æ®Šè¯å…ƒ ---
    print(f"2/3: é…ç½®è®­ç»ƒå™¨åŠç‰¹æ®Šè¯å…ƒ...")
    special_tokens = [
        "<|endoftext|>",
        "<|pad|>",
        "<|im_start|>",
        "<|im_end|>",
    ]

    trainer = trainers.BpeTrainer(
        vocab_size=args.vocab_size,
        min_frequency=2,
        special_tokens=special_tokens,
        show_progress=True
    )

    # --- 4. å¼€å§‹è®­ç»ƒ ---
    print(f"3/3: å¼€å§‹è®­ç»ƒ...")
    t0 = time.time()
    tokenizer.train(train_files, trainer)
    t1 = time.time()

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼æ€»è€—æ—¶ {t1 - t0:.2f} ç§’")

    # --- 5. ä¿å­˜æ¨¡å‹ ---
    tokenizer.save(str(output_file))
    print(f"ğŸ’¾ åˆ†è¯å™¨æ¨¡å‹å·²ä¿å­˜åˆ°: {output_file}")

    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    if temp_file and temp_file.exists():
        temp_file.unlink()
        print(f"ğŸ—‘ï¸ å·²åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {temp_file.name}")

    # --- 6. éªŒè¯ ---
    print("\n" + "=" * 80)
    print("ğŸ§ª éªŒè¯åˆ†è¯å™¨åŠŸèƒ½")
    print("=" * 80)

    loaded_tokenizer = Tokenizer.from_file(str(output_file))
    test_text_simple = "This is a test sentence."
    encoding_simple = loaded_tokenizer.encode(test_text_simple)
    print(f"æ™®é€šæ–‡æœ¬: '{test_text_simple}'")
    print(f"  -> Tokens: {encoding_simple.tokens}")
    print(f"  -> è§£ç : '{loaded_tokenizer.decode(encoding_simple.ids)}'")
    assert test_text_simple == loaded_tokenizer.decode(encoding_simple.ids)

    print("-" * 40)

    test_text_special = "<|im_start|>Hello<|im_end|><|endoftext|>"
    encoding_special = loaded_tokenizer.encode(test_text_special)
    decoded_special = loaded_tokenizer.decode(encoding_special.ids, skip_special_tokens=False)

    print(f"å¸¦ç‰¹æ®Šè¯å…ƒçš„æ–‡æœ¬: '{test_text_special}'")
    print(f"  -> Tokens: {encoding_special.tokens}")
    print(f"  -> è§£ç  (ä¿ç•™ç‰¹æ®Šè¯å…ƒ): '{decoded_special}'")
    assert test_text_special == decoded_special

    print("\nâœ… éªŒè¯é€šè¿‡ï¼åˆ†è¯å™¨å·¥ä½œæ­£å¸¸ï¼Œä¸”èƒ½æ­£ç¡®å¤„ç†ç‰¹æ®Šè¯å…ƒã€‚")
    print(f"ğŸ“Š æœ€ç»ˆè¯è¡¨å¤§å°: {loaded_tokenizer.get_vocab_size()}")


if __name__ == "__main__":
    main()
# END OF FILE: data_pipeline/tokenizer/train_tokenizer.py