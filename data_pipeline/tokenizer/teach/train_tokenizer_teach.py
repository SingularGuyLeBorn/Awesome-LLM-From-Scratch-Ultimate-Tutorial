# FILE: data_pipeline/tokenizer/teach/train_tokenizer_teach.py
"""
ã€æ•™å­¦æ¼”ç¤ºè„šæœ¬ã€‘
è®­ç»ƒå¹¶æµ‹è¯•æ‰‹å†™çš„ã€çº¯Pythonçš„BPEåˆ†è¯å™¨ã€‚

!!! è­¦å‘Š !!!
è¿™æ˜¯ä¸€ä¸ªéå¸¸éå¸¸æ…¢çš„è„šæœ¬ï¼Œä»…ç”¨äºæ•™å­¦å’Œç®—æ³•æ¼”ç¤ºã€‚
å®ƒè¢«è®¾è®¡ä¸ºåœ¨æå°çš„æ•°æ®ä¸Šè¿è¡Œï¼Œä»¥ä¾¿èƒ½åœ¨åˆç†çš„æ—¶é—´å†…å®Œæˆã€‚

---
æ¨èç”¨æ³•ï¼ˆå‡ åˆ†é’Ÿå†…å®Œæˆï¼‰:
---
python data_pipeline/tokenizer/teach/train_tokenizer_teach.py
"""
import argparse
from pathlib import Path
import time
from bpe_teach import SimpleTokenizer  # å¯¼å…¥æˆ‘ä»¬æ‰‹å†™çš„æ•™å­¦ç‰ˆåˆ†è¯å™¨


def main():
    parser = argparse.ArgumentParser(
        description="ã€æ•™å­¦ç‰ˆã€‘è®­ç»ƒä¸€ä¸ªæ‰‹å†™çš„BPEåˆ†è¯å™¨ï¼ˆéå¸¸æ…¢ï¼‰ã€‚",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    # é»˜è®¤è¯è¡¨å¤§å°éå¸¸å°ï¼Œä»…ç”¨äºæ¼”ç¤º
    parser.add_argument("--vocab_size", type=int, default=300, help="ç›®æ ‡è¯è¡¨å¤§å° (>256)")
    # é»˜è®¤åªä½¿ç”¨1MBæ•°æ®ï¼Œä»¥ç¡®ä¿èƒ½å¿«é€Ÿå®Œæˆ
    parser.add_argument("--data_limit_mb", type=int, default=1, help="æ•°æ®é‡ä¸Šé™(MB)")
    args = parser.parse_args()

    # --- è·¯å¾„å®šä¹‰ ---
    # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œç¡®ä¿è„šæœ¬åœ¨ä»»ä½•ä½ç½®éƒ½èƒ½æ­£ç¡®æ‰¾åˆ°æ–‡ä»¶
    data_path = Path(__file__).parent.parent.parent / "processed_data"
    train_file = data_path / "train.txt"
    output_dir = Path(__file__).parent / "toy_tokenizer_model"
    output_dir.mkdir(exist_ok=True)  # åˆ›å»ºä¿å­˜æ¨¡å‹çš„ç›®å½•
    merges_file = output_dir / f"merges_vs{args.vocab_size}.txt"
    vocab_file = output_dir / f"vocab_vs{args.vocab_size}.json"

    if not train_file.exists():
        print(f"âŒ é”™è¯¯: è®­ç»ƒæ–‡ä»¶ '{train_file}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å…ˆè¿è¡Œ 'data_pipeline/processing/process_tinystories.py'ã€‚")
        return

    if merges_file.exists():
        print(f"âŒ é”™è¯¯: ç›®æ ‡æ¨¡å‹æ–‡ä»¶ '{merges_file}' å·²å­˜åœ¨ã€‚")
        print("å¦‚æœæƒ³é‡æ–°è®­ç»ƒï¼Œè¯·å…ˆæ‰‹åŠ¨åˆ é™¤ 'teach/toy_tokenizer_model' æ–‡ä»¶å¤¹ã€‚")
        return

    # --- 1. è®­ç»ƒ ---
    print("=" * 60)
    print("ğŸ“ å¼€å§‹è®­ç»ƒæ•™å­¦ç‰ˆBPEåˆ†è¯å™¨")
    print(f"   è¯è¡¨å¤§å°: {args.vocab_size}, æ•°æ®é™åˆ¶: {args.data_limit_mb} MB")
    print("   (é¢„è®¡è€—æ—¶: 1-5 åˆ†é’Ÿ)")
    print("=" * 60)

    with open(train_file, 'r', encoding='utf-8') as f:
        text = f.read(args.data_limit_mb * 1024 * 1024)

    tokenizer = SimpleTokenizer()
    t0 = time.time()
    tokenizer.train(text, args.vocab_size, verbose=True)
    t1 = time.time()

    print(f"\nâœ… è®­ç»ƒå®Œæˆï¼Œæ€»è€—æ—¶ {t1 - t0:.2f} ç§’ã€‚")

    # --- 2. ä¿å­˜æ¨¡å‹ ---
    # ä¸ºäº†æ¸…æ™°ï¼Œæˆ‘ä»¬å°†åˆå¹¶è§„åˆ™å’Œè¯æ±‡è¡¨åˆ†å¼€ä¿å­˜
    print("\nğŸ’¾ æ­£åœ¨ä¿å­˜ç©å…·æ¨¡å‹...")

    # ä¿å­˜åˆå¹¶è§„åˆ™
    with open(merges_file, 'w', encoding='utf-8') as f:
        for pair, idx in tokenizer.merges.items():
            f.write(f"{pair[0]} {pair[1]}\n")
    print(f"   - åˆå¹¶è§„åˆ™å·²ä¿å­˜åˆ°: {merges_file}")

    # ä¿å­˜è¯æ±‡è¡¨ (ä½¿ç”¨JSONä»¥ä¾¿é˜…è¯»)
    import json
    # bytesä¸èƒ½ç›´æ¥jsonåºåˆ—åŒ–ï¼Œéœ€è¦å…ˆè§£ç 
    decoded_vocab = {k: v.decode('utf-8', errors='replace') for k, v in tokenizer.vocab.items()}
    with open(vocab_file, 'w', encoding='utf-8') as f:
        json.dump(decoded_vocab, f, ensure_ascii=False, indent=2)
    print(f"   - è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {vocab_file}")

    # --- 3. éªŒè¯ ---
    print("\n" + "=" * 60)
    print("ğŸ§ª éªŒè¯ç©å…·åˆ†è¯å™¨")
    print("=" * 60)

    test_text = "Once upon a time, there was a tiny little dragon."
    encoded = tokenizer.encode(test_text)
    decoded = tokenizer.decode(encoded)

    print(f"åŸæ–‡: '{test_text}'")
    print(f"ç¼–ç åçš„Token IDåºåˆ—: {encoded}")
    print(f"è§£ç åçš„æ–‡æœ¬: '{decoded}'")

    assert test_text == decoded, "âŒ ç¼–è§£ç ä¸ä¸€è‡´ï¼"
    print("\nâœ… éªŒè¯æˆåŠŸï¼æ‰‹å†™åˆ†è¯å™¨å·¥ä½œæ­£å¸¸ã€‚")


if __name__ == "__main__":
    main()
# END OF FILE: data_pipeline/tokenizer/teach/train_tokenizer_teach.py