# FILE: finetune/peft/qlora/qlora.py
"""
[QLoRA Core] QLoRA ç»„è£…å·¥å‚ã€‚
åŠŸèƒ½ï¼š
1. éå†æ¨¡å‹ï¼Œå°†æ‰€æœ‰ nn.Linear æ›¿æ¢ä¸º Linear4bitã€‚
2. åœ¨ Linear4bit ä¹‹ä¸Šåº”ç”¨ LoRA é€‚é…å™¨ã€‚
"""
import torch
import torch.nn as nn
import math
from typing import List
from .linear4bit import Linear4bit


class QLoRALayer(nn.Module):
    """
    ä¸€ä¸ªç»„åˆå±‚ï¼ŒåŒ…å«ï¼š
    1. å†»ç»“çš„ 4-bit åŸºç¡€å±‚ (Linear4bit)
    2. å¯è®­ç»ƒçš„ LoRA åˆ†æ”¯ (Adapter)
    """

    def __init__(
            self,
            base_layer: Linear4bit,
            rank: int,
            alpha: int,
            dropout: float
    ):
        super().__init__()
        self.base_layer = base_layer
        # å†»ç»“åŸºç¡€å±‚ï¼ˆè™½ç„¶å®ƒæœ¬èº«ä¹Ÿæ²¡æœ‰ Parameterï¼Œæ˜¯ Bufferï¼Œä½† bias æ˜¯ Parameterï¼‰
        for param in self.base_layer.parameters():
            param.requires_grad = False

        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        in_features = base_layer.in_features
        out_features = base_layer.out_features

        # LoRA A: (in, r)
        # LoRA B: (r, out)
        # ä¸ºäº†åŒ¹é… F.linear(x, weight)ï¼Œæƒé‡å½¢çŠ¶é€šå¸¸æ˜¯ (out, in)
        # æ‰€ä»¥ nn.Linear(in, rank) çš„æƒé‡æ˜¯ (rank, in)
        # æˆ‘ä»¬éµå¾ª standard LoRA implementation: B @ A @ x

        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)
        self.lora_dropout = nn.Dropout(dropout)

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

        # ç¡®ä¿ LoRA å‚æ•°çš„æ•°æ®ç±»å‹ä¸è®¡ç®—ç±»å‹ä¸€è‡´
        self.lora_A.to(base_layer.compute_dtype)
        self.lora_B.to(base_layer.compute_dtype)

    def forward(self, x: torch.Tensor):
        # 1. Base output (Quantized path)
        # Linear4bit å†…éƒ¨ä¼šå¤„ç†è§£é‡åŒ–
        base_output = self.base_layer(x)

        # 2. LoRA output (Adapter path)
        # x -> Dropout -> A -> B -> Scale
        lora_output = self.lora_B(self.lora_A(self.lora_dropout(x))) * self.scaling

        return base_output + lora_output


def replace_linear_with_qlora(model: nn.Module, rank: int, alpha: int, dropout: float, target_modules: List[str],
                              compute_dtype=torch.bfloat16):
    """
    é€’å½’éå†æ¨¡å‹ï¼Œæ‰§è¡Œä¸¤æ­¥æ“ä½œï¼š
    1. å°†ç›®æ ‡ nn.Linear è½¬æ¢ä¸º Linear4bit (é‡åŒ–)ã€‚
    2. ç”¨ QLoRALayer åŒ…è£¹ Linear4bit (æ·»åŠ  LoRA)ã€‚
    """
    for name, module in model.named_children():
        if len(list(module.children())) > 0:
            # é€’å½’
            replace_linear_with_qlora(module, rank, alpha, dropout, target_modules, compute_dtype)

        if isinstance(module, nn.Linear):
            # æ£€æŸ¥æ˜¯å¦åœ¨ç›®æ ‡åˆ—è¡¨ä¸­
            # ä¾‹å¦‚ name æ˜¯ "wq"ï¼Œtarget_modules æ˜¯ ["wq", "wk"]
            # å¦‚æœæˆ‘ä»¬åªçœ‹ leaf nameï¼Œå¯èƒ½ä¸å¤Ÿç²¾ç¡®ï¼Œä½†åœ¨ç®€å• Transformer ä¸­é€šå¸¸å¤Ÿç”¨
            if any(t in name for t in target_modules):
                print(f"âš–ï¸ Quantizing & Adapting: {name} -> QLoRA (4-bit + Adapter)")

                # 1. è½¬æ¢ä¸º 4-bit
                linear4bit = Linear4bit.from_linear(module, block_size=64, compute_dtype=compute_dtype)

                # 2. åŒ…è£¹ LoRA
                qlora_layer = QLoRALayer(linear4bit, rank, alpha, dropout)

                # 3. æ›¿æ¢
                setattr(model, name, qlora_layer)

                # 4. é‡Šæ”¾åŸå§‹ fp32 æƒé‡çš„æ˜¾å­˜
                del module
                torch.cuda.empty_cache() if torch.cuda.is_available() else None


def prepare_model_for_qlora_training(model: nn.Module):
    """
    å‡†å¤‡è®­ç»ƒï¼š
    1. å†»ç»“æ‰€æœ‰é LoRA å‚æ•°ã€‚
    2. ç¡®ä¿åªæœ‰ LoRA å‚æ•° requires_grad=Trueã€‚
    3. æ‰“å°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡ã€‚
    """
    trainable_params = 0
    all_param = 0

    for name, param in model.named_parameters():
        all_param += param.numel()
        if "lora_" in name:
            param.requires_grad = True
            trainable_params += param.numel()
        else:
            param.requires_grad = False

    print(f"\nğŸ“Š QLoRA Model Statistics:")
    print(f"   - Total Params: {all_param:,}")
    print(f"   - Trainable Params: {trainable_params:,}")
    print(f"   - Trainable Ratio: {trainable_params / all_param:.4%}")

# END OF FILE: finetune/peft/qlora/qlora.py