# FILE: finetune/peft/lora/sft_lora_train.py
# -*- coding: utf-8 -*-
"""
[v1.9 - Config Fix] ä½¿ç”¨ LoRA è¿›è¡ŒSFTçš„è®­ç»ƒä¸»è„šæœ¬
- [ä¿®å¤] ä» cfg.checkpointing è¯»å– save_intervalï¼Œè€Œä¸æ˜¯ cfg.trainingã€‚
- [å¢å¼º] ä½¿ç”¨ getattr æä¾›é»˜è®¤å€¼ï¼Œé˜²æ­¢ AttributeErrorã€‚
"""
import torch
import argparse
from pathlib import Path
import time
import sys
import shutil

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent.parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model, build_optimizer, build_scheduler, build_loggers
from finetune.sft_data_loader import get_sft_loaders
from pretrain.components.checkpointing import CheckpointManager
from pretrain.components.training_loop import Trainer
from finetune.peft.lora.lora import apply_lora_to_model, freeze_base_model_for_lora
from utils.model_utils import find_all_linear_names

try:
    from torch.cuda.amp import GradScaler
except ImportError:
    GradScaler = None


def main():
    parser = argparse.ArgumentParser(description="[v1.9] [LoRA] ç›‘ç£å¾®è°ƒ (SFT) è„šæœ¬")
    parser.add_argument("--config_path", type=str, required=True, help="æŒ‡å‘SFT LoRAé…ç½®YAMLæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--fast_dev_run", action="store_true", help="å¯ç”¨å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šåç§°å¹¶æ¸…ç†æ—§ç›®å½•")
    args = parser.parse_args()

    # --- 0. é…ç½®ä¸æ—¥å¿— ---
    project_base_path = Path(__file__).parent.parent.parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    base_output_dir = Path(cfg.output_dir)
    if args.fast_dev_run:
        run_name = "fast-dev-run"
        output_dir = base_output_dir / "sft" / "peft" / "lora" / run_name
        if output_dir.exists():
            print(f"ğŸ§¹ fast_dev_run æ¨¡å¼: æ­£åœ¨æ¸…ç†æ—§çš„å¼€å‘ç›®å½• {output_dir}")
            shutil.rmtree(output_dir)
    else:
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        run_name = cfg.run_name.format(timestamp=timestamp)
        output_dir = base_output_dir / "sft" / "peft" / "lora" / run_name

    output_dir.mkdir(parents=True, exist_ok=True)

    logger = build_loggers(cfg, output_dir, run_name)

    # --- 1. æ¨¡å‹ ---
    cfg.model.use_activation_checkpointing = getattr(cfg.training, 'use_activation_checkpointing', False)
    model = build_model(cfg.model)

    # [è‡ªåŠ¨è·¯å¾„è¦†ç›–]
    ckpt_path = cfg.sft.base_model_checkpoint
    if args.fast_dev_run:
        pretrain_dev_ckpt_path = base_output_dir / "pretrain" / "fast-dev-run" / "checkpoints" / "ckpt_best.pth"
        print(f"ğŸ”© --fast_dev_run: è‡ªåŠ¨è¦†ç›–æ£€æŸ¥ç‚¹åŠ è½½è·¯å¾„ã€‚")
        print(f"   - YAMLä¸­è·¯å¾„ (å°†è¢«å¿½ç•¥): {ckpt_path}")
        print(f"   - è‡ªåŠ¨è§£æè·¯å¾„: {pretrain_dev_ckpt_path}")
        ckpt_path = str(pretrain_dev_ckpt_path)

    if ckpt_path and Path(ckpt_path).exists():
        print(f"æ­£åœ¨ä»åŸºç¡€æ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½æƒé‡: {ckpt_path}")
        checkpoint = torch.load(ckpt_path, map_location=cfg.device)
        model.load_state_dict(checkpoint['model_state_dict'], strict=False)
        print("âœ… é¢„è®­ç»ƒæƒé‡åŠ è½½æˆåŠŸã€‚")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šåŸºç¡€æ¨¡å‹æ£€æŸ¥ç‚¹ '{ckpt_path}' æœªæ‰¾åˆ°ã€‚LoRA å°†åœ¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹ä¸Šåº”ç”¨ã€‚")

    # [æ ¸å¿ƒå‡çº§] è‡ªåŠ¨æ£€æµ‹ Target Modules
    target_modules = getattr(cfg.lora, 'target_modules', None)

    if target_modules is None or target_modules == "auto":
        print("ğŸ” [LoRA] æ­£åœ¨è‡ªåŠ¨åˆ†ææ¨¡å‹ç»“æ„ä»¥å¯»æ‰¾ Linear å±‚...")
        auto_targets = find_all_linear_names(model)
        print(f"   -> è‡ªåŠ¨æ£€æµ‹åˆ°çš„ç›®æ ‡å±‚: {auto_targets}")
        target_modules = auto_targets
    else:
        print(f"   -> ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„ç›®æ ‡å±‚: {target_modules}")

    apply_lora_to_model(
        model,
        rank=cfg.lora.r,
        alpha=cfg.lora.alpha,
        dropout=cfg.lora.dropout,
        target_modules=target_modules
    )
    freeze_base_model_for_lora(model)

    model.to(cfg.device)
    print(f"æ¨¡å‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {cfg.device}")

    # --- 2. æ•°æ® ---
    train_loader, val_loader = get_sft_loaders(
        tokenizer_path=Path(cfg.data.tokenizer_name),
        sft_bin_file=Path(cfg.data.sft_data_path),
        block_size=cfg.model.max_seq_len,
        batch_size=cfg.training.batch_size
    )

    # --- 3. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨ ---
    optimizer = build_optimizer(model, cfg.training)
    max_iters = len(train_loader) * cfg.training.max_epochs
    scheduler = build_scheduler(optimizer, cfg.training, max_iters)
    scaler = GradScaler() if cfg.device == 'cuda' and GradScaler else None

    # --- 4. æ£€æŸ¥ç‚¹ ---
    print("\n--- 4. åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨ ---")
    sft_ckpt_dir = output_dir / "checkpoints"
    ckpt_manager = CheckpointManager(sft_ckpt_dir, model, optimizer, scheduler, scaler)

    # --- 5. è®­ç»ƒå™¨ ---
    # [æ ¸å¿ƒä¿®å¤] ä» checkpointing è¯»å– save_intervalï¼Œå¹¶æä¾›é»˜è®¤å€¼
    save_interval = getattr(getattr(cfg, 'checkpointing', None), 'save_interval',
                            getattr(cfg.training, 'save_interval', 1000))

    trainer = Trainer(
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        optimizer=optimizer,
        scheduler=scheduler,
        device=cfg.device,
        logger=logger,
        ckpt_manager=ckpt_manager,
        hooks=None,
        gradient_accumulation_steps=cfg.training.gradient_accumulation_steps,
        log_interval=getattr(cfg.logging, 'log_interval', 10),
        save_interval=save_interval,
        scaler=scaler,
        clip_grad_norm=getattr(cfg.training, 'clip_grad_norm', 1.0),
        loss_spike_threshold=getattr(cfg.training, 'loss_spike_threshold', 5.0),
        max_consecutive_spikes=getattr(cfg.training, 'max_consecutive_spikes', 5),
        grad_norm_history_size=getattr(cfg.training, 'grad_norm_history_size', 100),
        grad_clip_percentile=getattr(cfg.training, 'grad_clip_percentile', 0.9),
        dynamic_clip_factor=getattr(cfg.training, 'dynamic_clip_factor', 1.5)
    )
    trainer.run(cfg.training.max_epochs, 0)


if __name__ == "__main__":
    main()
# END OF FILE: finetune/peft/lora/sft_lora_train.py