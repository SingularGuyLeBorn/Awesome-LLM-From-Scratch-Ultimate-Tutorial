# FILE: align/rm_train.py
# -*- coding: utf-8 -*-
"""
[v1.7 - Best Checkpoint Assurance] å¥–åŠ±æ¨¡å‹ (RM) è®­ç»ƒä¸»è„šæœ¬ã€‚
- [æ–°å¢] è®­ç»ƒç»“æŸæ—¶ç¡®ä¿ ckpt_best.pth å­˜åœ¨ã€‚
"""
import torch
import torch.nn.functional as F
import argparse
from pathlib import Path
import time
import sys
import shutil

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_reward_model, build_optimizer, build_scheduler, build_loggers
from align.preference_data_loader import get_preference_loaders
from pretrain.components.checkpointing import CheckpointManager
from tqdm import tqdm


def rm_loss(chosen_rewards: torch.Tensor, rejected_rewards: torch.Tensor) -> torch.Tensor:
    return -F.logsigmoid(chosen_rewards - rejected_rewards).mean()


def main():
    parser = argparse.ArgumentParser(description="[v1.7] å¥–åŠ±æ¨¡å‹ (RM) è®­ç»ƒè„šæœ¬")
    parser.add_argument("--config_path", type=str, required=True, help="æŒ‡å‘RMé…ç½®YAMLæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--fast_dev_run", action="store_true", help="å¯ç”¨å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šåç§°å¹¶æ¸…ç†æ—§ç›®å½•")
    args = parser.parse_args()

    # --- 0. é…ç½®ä¸æ—¥å¿— ---
    cfg = load_config(args.config_path, Path(__file__).parent.parent.resolve())

    base_output_dir = Path(cfg.output_dir)
    if args.fast_dev_run:
        run_name = "fast-dev-run"
        output_dir = base_output_dir / "rlhf" / "rm" / run_name
        if output_dir.exists():
            print(f"ğŸ§¹ fast_dev_run æ¨¡å¼: æ­£åœ¨æ¸…ç†æ—§çš„å¼€å‘ç›®å½• {output_dir}")
            shutil.rmtree(output_dir)
    else:
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        run_name = cfg.run_name.format(timestamp=timestamp)
        output_dir = base_output_dir / "rlhf" / "rm" / run_name

    output_dir.mkdir(parents=True, exist_ok=True)

    logger = build_loggers(cfg, output_dir, "rm_run")

    # --- 1. åˆå§‹åŒ–æ¨¡å‹ ---
    cfg.model.use_activation_checkpointing = getattr(cfg.training, 'use_activation_checkpointing', False)
    reward_model = build_reward_model(cfg.model).to(cfg.device)

    # [æ ¸å¿ƒä¿®æ”¹] è¯»å–æ–°çš„é…ç½®å­—æ®µ
    ckpt_path = cfg.rm.sft_model_checkpoint
    if args.fast_dev_run:
        sft_dev_ckpt_path = base_output_dir / "sft" / "full" / "fast-dev-run" / "checkpoints" / "ckpt_best.pth"
        print(f"ğŸ”© --fast_dev_run: è‡ªåŠ¨è¦†ç›–SFTæ¨¡å‹æ£€æŸ¥ç‚¹åŠ è½½è·¯å¾„ã€‚")
        print(f"   - YAMLä¸­è·¯å¾„ (å°†è¢«å¿½ç•¥): {ckpt_path}")
        print(f"   - è‡ªåŠ¨è§£æè·¯å¾„: {sft_dev_ckpt_path}")
        ckpt_path = str(sft_dev_ckpt_path)

    if ckpt_path and Path(ckpt_path).exists():
        print(f"æ­£åœ¨ä»SFTæ£€æŸ¥ç‚¹åŠ è½½æƒé‡: {ckpt_path}")
        checkpoint = torch.load(ckpt_path, map_location=cfg.device)
        reward_model.transformer.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… RM Transformer æƒé‡åŠ è½½æˆåŠŸã€‚")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šSFTæ£€æŸ¥ç‚¹ '{ckpt_path}' æœªæ‰¾åˆ°ã€‚RM å°†ä»éšæœºåˆå§‹åŒ–çš„æƒé‡å¼€å§‹è®­ç»ƒã€‚")


    # --- 2. æ•°æ® ---
    train_loader = get_preference_loaders(
        data_dir=Path(cfg.data.data_dir),
        tokenizer_name=cfg.data.tokenizer_name,
        block_size=cfg.model.max_seq_len,
        batch_size=cfg.training.batch_size
    )

    # --- 3. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨ ---
    optimizer = build_optimizer(reward_model, cfg.training)
    max_iters = len(train_loader) * cfg.training.max_epochs
    scheduler = build_scheduler(optimizer, cfg.training, max_iters)

    # --- 4. æ£€æŸ¥ç‚¹ç®¡ç†å™¨ ---
    print("\n--- 4. åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨ ---")
    ckpt_dir = output_dir / "checkpoints"
    ckpt_manager = CheckpointManager(ckpt_dir, reward_model, optimizer, scheduler)

    # --- 5. è®­ç»ƒå¾ªç¯ ---
    print("\n--- å¼€å§‹å¥–åŠ±æ¨¡å‹ (RM) è®­ç»ƒ ---")
    global_step = 0
    best_accuracy = 0.0
    for epoch in range(cfg.training.max_epochs):
        pbar = tqdm(train_loader, desc=f"Epoch {epoch} [RM Training]")
        total_accuracy = 0
        for chosen_tokens, rejected_tokens, chosen_mask, rejected_mask in pbar:
            chosen_tokens, rejected_tokens = chosen_tokens.to(cfg.device), rejected_tokens.to(cfg.device)
            chosen_mask, rejected_mask = chosen_mask.to(cfg.device), rejected_mask.to(cfg.device)

            chosen_rewards = reward_model(chosen_tokens, attention_mask=chosen_mask)
            rejected_rewards = reward_model(rejected_tokens, attention_mask=rejected_mask)

            loss = rm_loss(chosen_rewards, rejected_rewards)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            accuracy = (chosen_rewards > rejected_rewards).float().mean().item()
            total_accuracy += accuracy

            pbar.set_postfix(loss=f"{loss.item():.4f}", acc=f"{accuracy:.2f}", lr=f"{scheduler.get_last_lr()[0]:.2e}")

            if global_step % cfg.logging.log_interval == 0:
                logger.log({'rm/loss': loss.item(), 'rm/accuracy_step': accuracy, 'lr': scheduler.get_last_lr()[0]},
                           step=global_step)

            global_step += 1

        epoch_accuracy = total_accuracy / len(train_loader)
        logger.log({'rm/accuracy_epoch': epoch_accuracy}, step=epoch)

        is_best = epoch_accuracy > best_accuracy
        if is_best:
            best_accuracy = epoch_accuracy

        ckpt_manager.save(epoch, 1 - epoch_accuracy, is_best)

    # [æ ¸å¿ƒä¿®æ”¹] ç¡®ä¿ best å­˜åœ¨
    ckpt_manager.ensure_best_exists()
    logger.finish()
    print("\n--- å¥–åŠ±æ¨¡å‹è®­ç»ƒå®Œæˆ ---")


if __name__ == "__main__":
    main()
# END OF FILE: align/rm_train.py