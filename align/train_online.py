# FILE: align/train_online.py
# -*- coding: utf-8 -*-
"""
[v3.4 - è¯­ä¹‰å‡€åŒ–] é€šç”¨åœ¨çº¿ RL å¯¹é½è®­ç»ƒè„šæœ¬ (PPO, GSPO, etc.)
- æ›´æ–°è„šæœ¬ä»¥ä½¿ç”¨æ–°çš„é…ç½®å­—æ®µå `sft_model_checkpoint`ã€‚
"""
import argparse
from pathlib import Path
import time
import sys
from tokenizers import Tokenizer
import torch
import shutil

project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import (
    build_model, build_value_model, build_reward_model,
    build_optimizer, build_loggers
)
from align.prompt_loader import get_prompt_loader
from pretrain.components.checkpointing import CheckpointManager
from align.trainer import AlignmentTrainer


def log_run_details(cfg, output_dir):
    group_size = getattr(cfg.rl, 'group_size', 1)
    prompt_file_path = Path(cfg.data.prompt_data_path)
    if prompt_file_path.exists():
        num_prompts = len(prompt_file_path.read_text(encoding='utf-8').strip().split('\n'))
        prompt_loader_len = (num_prompts + cfg.training.batch_size - 1) // cfg.training.batch_size
    else:
        prompt_loader_len = 0
    rollout_batches = min(cfg.rl.rollout_batches,
                          prompt_loader_len) if prompt_loader_len > 0 else cfg.rl.rollout_batches
    total_samples_per_epoch = rollout_batches * cfg.training.batch_size * group_size

    print("\n" + "=" * 50 + f"\n{'RLHF è®­ç»ƒå‚æ•°è¯¦æƒ…':^50}\n" + "=" * 50)
    print(f" â–¶ è¿è¡Œåç§°: {Path(output_dir).name}\n â–¶ è¾“å‡ºç›®å½•: {output_dir}\n â–¶ ä½¿ç”¨ç®—æ³•: {cfg.rl.algorithm.upper()}")
    print("-" * 50 + f"\n{'æ ¸å¿ƒè®­ç»ƒå‚æ•°':^50}\n" + "-" * 50)
    print(
        f"   - å­¦ä¹ ç‡: {cfg.training.learning_rate:.2e}\n   - æ‰¹æ¬¡å¤§å° (Prompt): {cfg.training.batch_size}\n   - æ€» Epochs: {cfg.training.max_epochs}")
    print("-" * 50 + f"\n{'RL æ ¸å¿ƒå‚æ•°':^50}\n" + "-" * 50)
    print(
        f"   - Rollout æ‰¹æ¬¡æ•°/Epoch: {rollout_batches}\n   - Update æ¬¡æ•°/Epoch: {cfg.rl.update_epochs}\n   - PPO è£å‰ª Epsilon: {cfg.rl.clip_epsilon}")
    print(f"   - KL æƒ©ç½šç³»æ•°: {cfg.rl.kl_coeff}")
    if group_size > 1: print(f"   - ç»„å¤§å° (Group Size): {group_size}")
    print(f" â„¹ æ€»è®¡æ¯ä¸ª Epoch å°†ç”Ÿæˆ {total_samples_per_epoch} ä¸ªæ ·æœ¬åºåˆ—ã€‚")
    print("=" * 50 + "\n")


def main():
    parser = argparse.ArgumentParser(description="[v3.4] é€šç”¨åœ¨çº¿ RL å¯¹é½è®­ç»ƒå¯åŠ¨å™¨")
    parser.add_argument("--config_path", type=str, required=True, help="æŒ‡å‘RLé…ç½®YAMLæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--fast_dev_run", action="store_true", help="å¯ç”¨å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šåç§°å¹¶æ¸…ç†æ—§ç›®å½•")
    args = parser.parse_args()

    cfg = load_config(args.config_path, Path(__file__).parent.parent.resolve())
    algorithm = cfg.rl.algorithm.lower()

    base_output_dir = Path(cfg.output_dir)
    if args.fast_dev_run:
        run_name = "fast-dev-run"
        output_dir = base_output_dir / "rlhf" / "online" / f"{algorithm}-{run_name}"
        if output_dir.exists():
            print(f"ğŸ§¹ fast_dev_run æ¨¡å¼: æ­£åœ¨æ¸…ç†æ—§çš„å¼€å‘ç›®å½• {output_dir}")
            shutil.rmtree(output_dir)
    else:
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        run_name = cfg.run_name.format(timestamp=timestamp)
        output_dir = base_output_dir / "rlhf" / "online" / run_name

    output_dir.mkdir(parents=True, exist_ok=True)

    logger = build_loggers(cfg, output_dir, "rl_run")
    log_run_details(cfg, output_dir)
    tokenizer = Tokenizer.from_file(cfg.data.tokenizer_name)

    print("\n--- 1. åˆå§‹åŒ–æ‰€æœ‰æ¨¡å‹ ---")
    cfg.model.use_activation_checkpointing = getattr(cfg.training, 'use_activation_checkpointing', False)
    policy_model = build_model(cfg.model).to(cfg.device)
    reference_model = build_model(cfg.model).to(cfg.device)
    value_model = build_value_model(cfg.model).to(cfg.device) if algorithm == 'ppo' else None
    reward_model = build_reward_model(cfg.model).to(cfg.device)

    print("\n--- 2. åŠ è½½æ£€æŸ¥ç‚¹æƒé‡ ---")
    # [æ ¸å¿ƒä¿®æ”¹] è¯»å–æ–°çš„é…ç½®å­—æ®µ
    sft_ckpt_path = cfg.rl.sft_model_checkpoint
    rm_ckpt_path = cfg.rl.reward_model_checkpoint
    if args.fast_dev_run:
        sft_dev_ckpt_path = base_output_dir / "sft" / "full" / "fast-dev-run" / "checkpoints" / "ckpt_best.pth"
        rm_dev_ckpt_path = base_output_dir / "rlhf" / "rm" / "fast-dev-run" / "checkpoints" / "ckpt_best.pth"
        print(f"ğŸ”© --fast_dev_run: è‡ªåŠ¨è¦†ç›–SFTå’ŒRMæ£€æŸ¥ç‚¹åŠ è½½è·¯å¾„ã€‚")
        sft_ckpt_path = str(sft_dev_ckpt_path)
        rm_ckpt_path = str(rm_dev_ckpt_path)
        print(f"   - SFT Ckpt -> {sft_ckpt_path}")
        print(f"   - RM Ckpt  -> {rm_ckpt_path}")

    # åŠ è½½ SFT æ£€æŸ¥ç‚¹
    if sft_ckpt_path and Path(sft_ckpt_path).exists():
        sft_ckpt = torch.load(sft_ckpt_path, map_location=cfg.device)
        policy_model.load_state_dict(sft_ckpt['model_state_dict'])
        reference_model.load_state_dict(sft_ckpt['model_state_dict'])
        if value_model:
            value_model.transformer.load_state_dict(sft_ckpt['model_state_dict'])
        print(f"âœ… Policy, Reference, Value (if any) æ¨¡å‹å·²ä» SFT æ£€æŸ¥ç‚¹ '{sft_ckpt_path}' åŠ è½½ã€‚")
    else:
        print(f"âš ï¸ è­¦å‘Š: SFT æ£€æŸ¥ç‚¹ '{sft_ckpt_path}' æœªæ‰¾åˆ°ï¼Œæ¨¡å‹å°†ä½¿ç”¨éšæœºæƒé‡ã€‚")

    # åŠ è½½ RM æ£€æŸ¥ç‚¹
    if rm_ckpt_path and Path(rm_ckpt_path).exists():
        rm_ckpt = torch.load(rm_ckpt_path, map_location=cfg.device)
        reward_model.load_state_dict(rm_ckpt['model_state_dict'])
        print(f"âœ… Reward æ¨¡å‹å·²ä»ä¸“ç”¨æ£€æŸ¥ç‚¹ '{rm_ckpt_path}' åŠ è½½ã€‚")
    else:
        print(f"âš ï¸ è­¦å‘Š: å¥–åŠ±æ¨¡å‹æ£€æŸ¥ç‚¹ '{rm_ckpt_path}' æœªæ‰¾åˆ°ï¼Œå¥–åŠ±æ¨¡å‹å°†ä½¿ç”¨éšæœºæƒé‡ã€‚")

    for param in reference_model.parameters():
        param.requires_grad = False

    print("\n--- 3. åˆå§‹åŒ– Prompt æ•°æ®åŠ è½½å™¨ ---")
    prompt_loader = get_prompt_loader(
        prompt_file_path=Path(cfg.data.prompt_data_path),
        tokenizer=tokenizer,
        batch_size=cfg.training.batch_size,
        max_prompt_len=cfg.rl.max_prompt_len
    )

    print("\n--- 4. åˆå§‹åŒ–ä¼˜åŒ–å™¨ ---")
    policy_optimizer, value_optimizer = None, None
    if algorithm == 'ppo':
        params_to_optimize = list(policy_model.parameters()) + list(value_model.parameters())
        ppo_optim_config = lambda: None
        setattr(ppo_optim_config, 'learning_rate', cfg.training.learning_rate)
        setattr(ppo_optim_config, 'weight_decay', cfg.training.weight_decay)
        combined_model = torch.nn.ModuleList([policy_model, value_model])
        policy_optimizer = build_optimizer(combined_model, ppo_optim_config)
        value_optimizer = None
    else:
        policy_optimizer = build_optimizer(policy_model, cfg.training)
        value_optimizer = None

    print("\n--- 5. æ„å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨ ---")
    ckpt_manager = CheckpointManager(output_dir / "checkpoints", policy_model, policy_optimizer, scheduler=None)

    trainer = AlignmentTrainer(
        cfg=cfg,
        policy_model=policy_model,
        reference_model=reference_model,
        value_model=value_model,
        reward_model=reward_model,
        tokenizer=tokenizer,
        train_loader=prompt_loader,
        logger=logger,
        ckpt_manager=ckpt_manager,
        policy_optimizer=policy_optimizer,
        value_optimizer=value_optimizer,
    )

    trainer.train()


if __name__ == "__main__":
    main()
# END OF FILE: align/train_online.py