# FILE: align/train_offline.py
# -*- coding: utf-8 -*-
"""
[v3.3 - ä¾èµ–è‡ªåŠ¨åŒ–] é€šç”¨ç¦»çº¿å¯¹é½è®­ç»ƒè„šæœ¬ (DPO, ORPO, etc.)
- åœ¨ fast_dev_run æ¨¡å¼ä¸‹ï¼Œè‡ªåŠ¨è¦†ç›–æ£€æŸ¥ç‚¹åŠ è½½è·¯å¾„ã€‚
"""
import argparse
from pathlib import Path
import time
import sys
from copy import deepcopy
from tokenizers import Tokenizer
import shutil

project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path: sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model, build_optimizer, build_scheduler, build_loggers
from align.preference_data_loader import get_preference_loaders
from pretrain.components.checkpointing import CheckpointManager
from align.trainer import AlignmentTrainer
import torch


def main():
    parser = argparse.ArgumentParser(description="[v3.3] é€šç”¨ç¦»çº¿å¯¹é½è®­ç»ƒå¯åŠ¨å™¨")
    parser.add_argument("--config_path", type=str, required=True, help="æŒ‡å‘ç¦»çº¿å¯¹é½é…ç½®YAMLæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--fast_dev_run", action="store_true", help="å¯ç”¨å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼ï¼Œä½¿ç”¨å›ºå®šåç§°å¹¶æ¸…ç†æ—§ç›®å½•")
    args = parser.parse_args()

    # --- 1. åŠ è½½é…ç½®å’Œåˆå§‹åŒ– ---
    cfg = load_config(args.config_path, Path(__file__).parent.parent.resolve())
    algorithm = cfg.offline.algorithm.lower()

    base_output_dir = Path(cfg.output_dir)
    if args.fast_dev_run:
        run_name = "fast-dev-run"
        # è·¯å¾„åä¸­åŠ å…¥ç®—æ³•åä»¥åŒºåˆ† DPO/ORPO çš„ dev run
        output_dir = base_output_dir / "rlhf" / "offline" / f"{algorithm}-{run_name}"
        if output_dir.exists():
            print(f"ğŸ§¹ fast_dev_run æ¨¡å¼: æ­£åœ¨æ¸…ç†æ—§çš„å¼€å‘ç›®å½• {output_dir}")
            shutil.rmtree(output_dir)
    else:
        timestamp = time.strftime('%Y%m%d-%H%M%S')
        run_name = cfg.run_name.format(timestamp=timestamp)
        output_dir = base_output_dir / "rlhf" / "offline" / run_name

    output_dir.mkdir(parents=True, exist_ok=True)


    logger = build_loggers(cfg, output_dir, f"{algorithm}_run")
    tokenizer = Tokenizer.from_file(cfg.data.tokenizer_name)

    # --- 2. æ„å»ºæ¨¡å‹ ---
    print("\n--- æ„å»º Policy å’Œ Reference æ¨¡å‹ ---")
    policy_model = build_model(cfg.model).to(cfg.device)
    reference_model = deepcopy(policy_model).to(cfg.device)
    for param in reference_model.parameters():
        param.requires_grad = False

    # [æ ¸å¿ƒä¿®æ”¹] è‡ªåŠ¨è·¯å¾„è¦†ç›–
    if args.fast_dev_run:
        sft_dev_ckpt_path = base_output_dir / "sft" / "full" / "fast-dev-run" / "checkpoints" / "ckpt_best.pth"
        print(f"ğŸ”© --fast_dev_run: è‡ªåŠ¨è¦†ç›–æ£€æŸ¥ç‚¹åŠ è½½è·¯å¾„ã€‚")
        print(f"   - YAMLä¸­è·¯å¾„ (å°†è¢«å¿½ç•¥): {cfg.offline.load_from_checkpoint}")
        print(f"   - è‡ªåŠ¨è§£æè·¯å¾„: {sft_dev_ckpt_path}")
        cfg.offline.load_from_checkpoint = str(sft_dev_ckpt_path)

    if cfg.offline.load_from_checkpoint and Path(cfg.offline.load_from_checkpoint).exists():
        print(f"æ­£åœ¨ä»SFTæ£€æŸ¥ç‚¹åŠ è½½æƒé‡: {cfg.offline.load_from_checkpoint}")
        checkpoint = torch.load(cfg.offline.load_from_checkpoint, map_location=cfg.device)
        policy_model.load_state_dict(checkpoint['model_state_dict'])
        reference_model.load_state_dict(checkpoint['model_state_dict'])
        print("âœ… Policy å’Œ Reference æ¨¡å‹æƒé‡åŠ è½½æˆåŠŸã€‚")
    else:
        print(f"âš ï¸ è­¦å‘Šï¼šSFTæ£€æŸ¥ç‚¹ '{cfg.offline.load_from_checkpoint}' æœªæ‰¾åˆ°ã€‚æ¨¡å‹å°†ä»éšæœºæƒé‡å¼€å§‹ã€‚")

    # --- 3. æ„å»ºæ•°æ®åŠ è½½å™¨ ---
    print("\n--- æ„å»ºåå¥½æ•°æ®åŠ è½½å™¨ ---")
    train_loader = get_preference_loaders(
        data_dir=Path(cfg.data.data_dir),
        tokenizer_name=cfg.data.tokenizer_name,
        block_size=cfg.model.max_seq_len,
        batch_size=cfg.training.batch_size
    )

    # --- 4. æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ---
    print("\n--- æ„å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ ---")
    optimizer = build_optimizer(policy_model, cfg.training)
    max_iters = len(train_loader) * cfg.training.max_epochs
    scheduler = build_scheduler(optimizer, cfg.training, max_iters)

    # --- 5. æ„å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨ ---
    ckpt_manager = CheckpointManager(output_dir / "checkpoints", policy_model, optimizer, scheduler)

    # --- 6. å®ä¾‹åŒ–å¹¶è¿è¡Œ AlignmentTrainer ---
    trainer = AlignmentTrainer(
        cfg=cfg,
        policy_model=policy_model,
        reference_model=reference_model,
        tokenizer=tokenizer,
        train_loader=train_loader,
        logger=logger,
        ckpt_manager=ckpt_manager,
        offline_optimizer=optimizer,
        offline_scheduler=scheduler,
    )

    trainer.train()


if __name__ == "__main__":
    main()
# END OF FILE: align/train_offline.py