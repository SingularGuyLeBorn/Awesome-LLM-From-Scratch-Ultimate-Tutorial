# FILE: tests/test_mla_optimization.py
"""
éªŒè¯ MLA (Multi-Head Latent Attention) çš„çŸ©é˜µå¸æ”¶ä¼˜åŒ–æ˜¯å¦æ­£ç¡®ã€‚
å¯¹æ¯” Naive å®ç°ï¼ˆè®­ç»ƒè·¯å¾„ï¼‰å’Œ Optimized å®ç°ï¼ˆæ¨ç†è·¯å¾„ï¼‰çš„è¾“å‡ºæ•°å€¼ã€‚
"""
import torch
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = str(Path(__file__).parent.parent)
sys.path.insert(0, project_root)

from models.blocks.attention.standard import MultiHeadLatentAttention
from models.config import ModelArgs
from models.blocks.positional_encoding.positional_encoding import RoPE, RoPEConfig
from inference.engine.kv_cache import LatentKVCache


def test_mla_correctness():
    print("ğŸ§ª Testing MLA Matrix Absorption Optimization...")

    # 1. é…ç½®
    args = ModelArgs(
        dim=128,
        n_heads=4,
        n_kv_heads=4,  # MLA typically ignored n_kv_heads as it's decoupled
        q_lora_rank=64,
        kv_lora_rank=32,
        nope_head_dim=16,
        rope_head_dim=8,
        v_head_dim=32,
        max_seq_len=128,
        dropout=0.0,
        norm_eps=1e-5,
        vocab_size=4096  # [ä¿®å¤] å¿…é¡»æä¾› vocab_sizeï¼Œå°½ç®¡åœ¨æ­¤æµ‹è¯•ä¸­æœªè¢«ä½¿ç”¨
    )

    device = "cpu"
    mla = MultiHeadLatentAttention(args).to(device).eval()

    rope_config = RoPEConfig(head_dim=args.rope_head_dim, max_seq_len=args.max_seq_len)
    rope = RoPE(rope_config).to(device)

    # 2. æ¨¡æ‹Ÿè¾“å…¥æ•°æ®
    batch_size = 1
    history_len = 10

    # å†å²æ•°æ® (History)
    x_history = torch.randn(batch_size, history_len, args.dim, device=device)
    # å½“å‰æ•°æ® (Current Token)
    x_current = torch.randn(batch_size, 1, args.dim, device=device)

    # 3. è¿è¡Œ Naive Forward (æ¨¡æ‹Ÿè®­ç»ƒæ¨¡å¼ï¼Œä¸€æ¬¡æ€§è¾“å…¥å…¨éƒ¨åºåˆ—)
    # æ‹¼æ¥ history å’Œ current
    x_full = torch.cat([x_history, x_current], dim=1)

    with torch.no_grad():
        output_naive = mla(x_full, rope, layer_idx=0)
        # æˆ‘ä»¬åªå…³å¿ƒæœ€åä¸€ä¸ª token çš„è¾“å‡º
        last_token_naive = output_naive[:, -1:, :]

    print("âœ… Naive forward pass completed.")

    # 4. è¿è¡Œ Optimized Inference (æ¨¡æ‹Ÿ KV Cache æ¨¡å¼)
    kv_cache = LatentKVCache(
        max_batch_size=batch_size,
        max_seq_len=args.max_seq_len,
        n_layers=1,
        kv_lora_rank=args.kv_lora_rank,
        rope_head_dim=args.rope_head_dim,
        device=device,
        dtype=torch.float32
    )

    with torch.no_grad():
        # Phase 1: Prefill (å¤„ç† History)
        # ä¸ºäº†æµ‹è¯•ä¸¥è°¨æ€§ï¼Œæˆ‘ä»¬è¿™é‡Œç”¨å¾ªç¯æ¨¡æ‹Ÿé€æ­¥ç”Ÿæˆï¼Œæˆ–è€…ç›´æ¥ Hack è¿› Cache
        # è¿™é‡Œæˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨ inference æ¨¡å¼å¤„ç† history (é€ä¸ª token)
        # åœ¨å®é™…ä¸­ Prefill é€šå¸¸èµ° Naive æ¨¡å¼ç”Ÿæˆ Cacheï¼Œè¿™é‡Œä¸ºäº†æµ‹è¯• forward_optimizedï¼Œæˆ‘ä»¬é€ä¸ªå¡è¿›å»

        for i in range(history_len):
            token = x_history[:, i:i + 1, :]
            _ = mla(token, rope, layer_idx=0, kv_cache=kv_cache, start_pos=i)

        # Phase 2: Decode (å¤„ç† Current Token)
        # è¿™æ˜¯æˆ‘ä»¬è¦éªŒè¯çš„å…³é”®æ­¥éª¤
        last_token_opt = mla(x_current, rope, layer_idx=0, kv_cache=kv_cache, start_pos=history_len)

    print("âœ… Optimized inference pass completed.")

    # 5. å¯¹æ¯”ç»“æœ
    print("\nğŸ“Š Results Comparison:")
    print(f"Naive Output Shape: {last_token_naive.shape}")
    print(f"Optimized Output Shape: {last_token_opt.shape}")

    diff = (last_token_naive - last_token_opt).abs().max().item()
    print(f"Max Difference: {diff:.8f}")

    if diff < 1e-4:
        print("\nğŸ‰ SUCCESS: Optimized MLA implementation matches Naive implementation!")
    else:
        print("\nâŒ FAILURE: Outputs do not match.")


if __name__ == "__main__":
    test_mla_correctness()
# END OF FILE: tests/test_mla_optimization.py