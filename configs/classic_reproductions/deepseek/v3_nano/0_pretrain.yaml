run_name: pretrain-deepseek-v3_nano-{timestamp}
output_dir: ./runs/
device: cpu
console:
  verbose: true
data:
  tokenizer_name: ./data_pipeline/processed_data/tinystories_project_vs4096.json
  data_dir: ./data_pipeline/processed_data/
  train_data_limit: 5000
  val_data_limit: 200
model:
  dim: 64
  n_layers: 2
  n_heads: 4
  n_kv_heads: 4
  vocab_size: 4096
  multiple_of: 16
  norm_eps: 1.0e-05
  max_seq_len: 128
  dropout: 0.0
  attention_variant: mla
  q_lora_rank: 16
  kv_lora_rank: 16
  v_head_dim: 16
  rope_head_dim: 8
  nope_head_dim: 8
  num_experts: 4
  num_shared_experts: 1
  num_experts_per_tok: 2
  use_aux_free_lb: true
  use_activation_checkpointing: true
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  max_epochs: 2
  weight_decay: 0.01
  clip_grad_norm: 1.0
  loss_spike_threshold: 10.0
  use_activation_checkpointing: true
  learning_rate: 0.0005
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
logging:
  log_interval: 5
checkpointing:
  save_interval: 200
  resume_from: none
