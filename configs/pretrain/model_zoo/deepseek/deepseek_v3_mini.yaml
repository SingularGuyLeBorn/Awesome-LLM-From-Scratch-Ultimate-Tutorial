# FILE: configs/pretrain/model_zoo/deepseek/deepseek_v3_mini.yaml
# ==============================================================================
# ã€æ¨¡å‹æ¡£æ¡ˆã€‘ DeepSeek-V3 mini (High-Fidelity / é«˜ä¿çœŸå¤åˆ»ç‰ˆ)
# ==============================================================================
#
# ğŸ¯ è®¾è®¡ç›®æ ‡ (Design Philosophy):
#    è¿™æ˜¯ä¸€ä¸ªä¸“ä¸º "éªŒè¯ç¨€ç–é€»è¾‘" è®¾è®¡çš„çº³ç±³çº§æ¨¡å‹ (çº¦ 1.6M å‚æ•°)ã€‚
#    å®ƒä¸ä»…ä¿ç•™äº† MLA å’Œ DeepSeekMoEï¼Œè¿˜é€šè¿‡ç‰¹æ„è°ƒæ•´çš„ Context/Block æ¯”ä¾‹ï¼Œ
#    å¼ºåˆ¶æ¨¡å‹åœ¨ CPU ä¸Šæ‰§è¡ŒçœŸå®çš„ç¨€ç–æ³¨æ„åŠ›è®¡ç®— (NSA/MoBA)ï¼Œé˜²æ­¢é€€åŒ–ä¸ºæ ‡å‡† Attentionã€‚
#
# ğŸ—ï¸ æ ¸å¿ƒå‚æ•°è§£æ„ (Critical Configuration):
#
#    [1] å¼ºåˆ¶ç¨€ç–åŒ– (Forced Sparsity):
#        - Context: 1024 tokens
#        - Block Size: 64 tokens
#        - ç»“æœ: 16 ä¸ª Blocksã€‚æ¨¡å‹å¿…é¡»ä½¿ç”¨ Top-K (k=2) ç®—æ³•åœ¨ 16 ä¸ªå—ä¸­é€‰æ‹© 2 ä¸ªã€‚
#        - éªŒè¯: å¦‚æœä»£ç é€»è¾‘æœ‰è¯¯ï¼Œè¿™é‡Œä¼šç«‹å³æŠ¥é”™ (Tensor Shape Mismatch)ã€‚
#
#    [2] MLA (æè‡´å‹ç¼©):
#        - KV Compression: 32 dim. æå°çš„ KV Cacheï¼Œæ¨¡æ‹Ÿ V3 çš„æ ¸å¿ƒä¼˜åŠ¿ã€‚
#
#    [3] DeepSeekMoE (Aux-free):
#        - 4 Routed + 1 Sharedã€‚å¯ç”¨ Bias-based è´Ÿè½½å‡è¡¡ã€‚
#
# ==============================================================================

run_name: "deepseek-v3-mini-sparse-check-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true # å¼€å¯è¯¦ç»†æ‰“å°ï¼Œè§‚å¯Ÿå‚æ•°é‡å’Œæ¶æ„

data:
  data_dir: "./data_pipeline/processed_data/"
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  # [å…³é”®ä¿®æ”¹] å¢åŠ æ•°æ®é‡ä»¥æ”¯æŒ Packing åˆ° 1024 é•¿åº¦
  train_data_limit: 2000 
  val_data_limit: 100

model:
  # --- åŸºç¡€è§„æ¨¡ (~1.6M Params) ---
  dim: 96               # éšè—å±‚ç»´åº¦ (96 = 6 heads * 16 head_dim)
  n_layers: 3           # 3å±‚è¶³ä»¥éªŒè¯æ¢¯åº¦ä¼ æ’­
  n_heads: 6            # æ³¨æ„åŠ›å¤´æ•°
  vocab_size: 4096      # è¯è¡¨
  multiple_of: 16       # FFN å¯¹é½
  norm_eps: 1.0e-5
  
  # [å…³é”®ä¿®æ”¹] é•¿åºåˆ—é…ç½®
  max_seq_len: 1024     # è¶³å¤Ÿé•¿ï¼Œå…è®¸åˆ‡åˆ†ä¸ºå¤šä¸ª Block
  dropout: 0.0

  # --- [æ ¸å¿ƒ] Attention: MLA + Sparse å˜ä½“ ---
  attention_variant: "mla" # ä¹Ÿå¯ä»¥æ”¹ä¸º 'moba' æˆ– 'nsa' è¿›è¡Œå•ç‹¬æµ‹è¯•
  
  # MLA å‹ç¼©å‚æ•°
  q_lora_rank: 64
  kv_lora_rank: 32
  v_head_dim: 16
  rope_head_dim: 8
  nope_head_dim: 8

  # [å…³é”®ä¿®æ”¹] MoBA/NSA ç¨€ç–å‚æ•° (å¼ºåˆ¶å° Block)
  moba_block_size: 64    # 64 < 1024ï¼Œç¡®ä¿äº§ç”Ÿ 16 ä¸ªå—
  moba_topk: 2           # åªå…³æ³¨ 2 ä¸ªå— (ç¨€ç–åº¦ ~87.5%)
  
  # NSA å‚æ•° (åŒç†)
  nsa_compression_block_size: 32
  nsa_selection_block_size: 64
  nsa_selected_blocks: 2
  nsa_sliding_window_size: 128

  # --- [æ ¸å¿ƒ] DeepSeekMoE é…ç½® ---
  num_experts: 4          # è·¯ç”±ä¸“å®¶
  num_shared_experts: 1   # å…±äº«ä¸“å®¶
  num_experts_per_tok: 2  # Top-2
  
  # å¼€å¯æ— è¾…åŠ©æŸå¤±è´Ÿè½½å‡è¡¡
  use_aux_free_lb: true   

  # CPU è°ƒè¯•å»ºè®®å…³é—­ Checkpointing
  use_activation_checkpointing: false

training:
  # Batch Size å‡å°ä»¥é€‚åº” Sequence Length å¢åŠ  (8 -> 4)
  batch_size: 4
  gradient_accumulation_steps: 4 # ä¿æŒæ€» Batch Size è¶³å¤Ÿå¤§
  max_epochs: 1
  
  learning_rate: 3.0e-3
  weight_decay: 0.01
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
  clip_grad_norm: 1.0
  
  loss_spike_threshold: 5.0
  dynamic_clip_factor: 1.5

logging:
  log_interval: 5 # æ‰“å°æ›´é¢‘ç¹ä¸€ç‚¹
  swanlab:
    enable: false

checkpointing:
  save_interval: 500
  resume_from: "none"

# END OF FILE: configs/pretrain/model_zoo/deepseek/deepseek_v3_mini.yaml