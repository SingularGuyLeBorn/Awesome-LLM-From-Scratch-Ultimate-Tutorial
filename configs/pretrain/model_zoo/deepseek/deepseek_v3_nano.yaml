# FILE: configs/pretrain/model_zoo/deepseek/deepseek_v3_nano.yaml
# ==============================================================================
# ã€æ¨¡å‹æ¡£æ¡ˆã€‘ DeepSeek-V3 Nano (Pico-Scale / æé€ŸéªŒè¯ç‰ˆ)
# ==============================================================================
#
# ğŸ§ª å®šä½ä¸ç”¨é€” (Purpose):
#    - å‚æ•°é‡: ~0.2M (äºŒåä¸‡å‚æ•°)
#    - æ ¸å¿ƒç”¨é€”: CI/CD é›†æˆæµ‹è¯•ã€ç®—æ³•é€»è¾‘å†’çƒŸæµ‹è¯•ã€ä»£ç é‡æ„åçš„å¿«é€Ÿå›å½’ã€‚
#    - è¿è¡Œé€Ÿåº¦: å•æ ¸ CPU ä¹Ÿèƒ½â€œç§’è§£â€ã€‚
#
# ğŸ”¬ æ¶æ„ç¼©æ”¾ (Scaling Strategy):
#    å°½ç®¡å°åˆ°æè‡´ï¼Œä½†å®ƒä¾ç„¶ä¿ç•™äº† DeepSeek-V3 çš„å®Œæ•´æ‹“æ‰‘ç»“æ„ï¼š
#    1. MLA: ä¾ç„¶æ‰§è¡Œ KV å‹ç¼©ä¸è§£å‹ (Rank=16)ã€‚
#    2. MoE: ä¾ç„¶æ‰§è¡Œ è·¯ç”±ä¸å…±äº«ä¸“å®¶è®¡ç®— (4 Routed + 1 Shared)ã€‚
#    3. RoPE: ä¾ç„¶æ‰§è¡Œ è§£è€¦çš„ä½ç½®ç¼–ç ã€‚
#
# ==============================================================================

run_name: "deepseek-v3-nano-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true # å¼€å¯è¯¦ç»†æ‰“å°ï¼Œè§‚å¯Ÿå‚æ•°é‡

data:
  data_dir: "./data_pipeline/processed_data/"
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  # æ•°æ®é‡æå°ï¼Œåªä¸ºäº†è·‘é€šæµç¨‹
  train_data_limit: 200
  val_data_limit: 20

model:
  # --- Pico Scale (~0.2M Params) ---
  dim: 64               # æçª„éšè—å±‚ (64 = 4 heads * 16 dim)
  n_layers: 2           # ææµ…ç½‘ç»œ (2å±‚ï¼š1å±‚å­¦ç‰¹å¾ï¼Œ1å±‚æ•´åˆ)
  n_heads: 4            # 4 ä¸ªå¤´
  vocab_size: 4096      # è¯è¡¨ä¸å˜
  multiple_of: 16       # å¯¹é½
  norm_eps: 1.0e-5
  max_seq_len: 128      # æçŸ­åºåˆ— (128 tokens)
  dropout: 0.0

  # --- [MLA] å¾®å‹å‹ç¼© ---
  attention_variant: "mla"
  q_lora_rank: 16       # Query å‹ç¼©åˆ° 16 ç»´
  kv_lora_rank: 16      # KV å‹ç¼©åˆ° 16 ç»´
  v_head_dim: 16        # Value ç»´åº¦
  rope_head_dim: 8      # RoPE ç»´åº¦
  nope_head_dim: 8      # Non-RoPE ç»´åº¦

  # --- [DeepSeekMoE] å¾®å‹ä¸“å®¶ ---
  num_experts: 4          # 4 ä¸ªè·¯ç”±ä¸“å®¶
  num_shared_experts: 1   # 1 ä¸ªå…±äº«ä¸“å®¶
  num_experts_per_tok: 2  # Top-2 è·¯ç”±

  use_aux_free_lb: true   # ä¾ç„¶å¼€å¯ Bias è´Ÿè½½å‡è¡¡

  use_activation_checkpointing: false

training:
  batch_size: 8
  gradient_accumulation_steps: 1
  max_epochs: 1
  learning_rate: 5.0e-3 # æå°æ¨¡å‹éœ€è¦å¾ˆå¤§çš„å­¦ä¹ ç‡æ‰èƒ½çœ‹åˆ° Loss å˜åŒ–
  weight_decay: 0.0
  clip_grad_norm: 1.0

  # ç¨³å®šæ€§ç›‘æ§æ”¾å®½
  loss_spike_threshold: 10.0

logging:
  log_interval: 1     # æ¯æ­¥éƒ½æ‰“å°
  swanlab:
    enable: false

checkpointing:
  save_interval: 50   # ä¸æ€ä¹ˆéœ€è¦ä¿å­˜ï¼Œæˆ–è€…æœ€åä¿å­˜ä¸€ä¸‹
  resume_from: "none"

# END OF FILE: configs/pretrain/model_zoo/deepseek/deepseek_v3_nano.yaml