# FILE: configs/pretrain/1.4M_moe_fast.yaml
# ===================================================================
# MoE 预训练配置 - 1.4M (Active) / ~5M (Total) - 快速测试版
# ===================================================================

run_name: "moe-pretrain-fast-test-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true

data:
  data_dir: "./data_pipeline/processed_data/"
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  train_data_limit: 1000
  val_data_limit: 100

model:
  # 基础维度 (保持小巧以便 CPU 快速跑通)
  dim: 128
  n_layers: 4          # 加深一点，让 MoE 有更多发挥空间
  n_heads: 4
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 256
  dropout: 0.0

  # [核心新增: MoE 配置]
  # 总共有 4 个专家
  num_experts: 4
  # 每个 Token 选择 2 个专家进行计算 (Top-2 Gating)
  num_experts_per_tok: 2
  # (可选) 如果你想指定只有中间层是 MoE，可以取消注释下面这行：
  # moe_layers_indices: [1, 2]

training:
  batch_size: 4
  gradient_accumulation_steps: 1
  max_epochs: 1
  learning_rate: 5.0e-4  # MoE 通常能容忍稍大的学习率
  weight_decay: 0.01
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
  clip_grad_norm: 1.0

  # 开启梯度检查点以节省内存 (MoE 参数量大，这很有用)
  use_activation_checkpointing: true

  loss_spike_threshold: 5.0
  max_consecutive_spikes: 5
  grad_norm_history_size: 100
  grad_clip_percentile: 0.9
  dynamic_clip_factor: 1.5

logging:
  log_interval: 1
  swanlab:
    enable: true
    project: "LLM-From-Scratch-MoE-Tests"
    experiment_name: "{run_name}"
  wandb:
    enable: false

checkpointing:
  save_interval: 10
  resume_from: "none"

# END OF FILE: configs/pretrain/1.4M_moe_fast.yaml