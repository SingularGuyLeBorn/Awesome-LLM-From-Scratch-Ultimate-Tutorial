# FILE: configs/pretrain/variants/deepseek_v2_nano.yaml
# ===================================================================
# DeepSeek-V2 Nano 架构验证配置
# 核心特性:
# 1. MLA (Multi-Head Latent Attention): 极低显存占用的注意力机制
# 2. DeepSeekMoE: 细粒度混合专家模型
# ===================================================================

run_name: "deepseek-v2-nano-{timestamp}"
output_dir: "./runs/"
device: "cpu"

# 控制台输出详细程度
console:
  verbose: true

# 数据路径配置
data:
  data_dir: "./data_pipeline/processed_data/"
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  # 训练数据限制 (Nano级测试用)
  train_data_limit: 200
  val_data_limit: 20

# 模型架构配置
model:
  # --- 基础维度 ---
  dim: 128              # 隐藏层维度
  n_layers: 4           # 层数
  n_heads: 4            # 注意力头数
  vocab_size: 4096      # 词表大小
  multiple_of: 32       # 维度对齐倍数
  norm_eps: 1.0e-5      # RMSNorm Epsilon
  max_seq_len: 512      # 最大序列长度
  dropout: 0.0          # Dropout 概率

  # --- [核心] Attention 变体选择 ---
  attention_variant: "mla"  # 启用 DeepSeek-V2 的 MLA

  # --- [MLA] 参数配置 (压缩与解耦) ---
  # 1. Query 压缩: 将 128 维压缩到 64
  q_lora_rank: 64
  # 2. KV 压缩 (核心): 将 128 维压缩到 64 (KV Cache 显著减小)
  kv_lora_rank: 64
  # 3. 头部维度拆分 (Pe/Nope)
  # Value Head Dim: 32 (4 heads * 32 = 128 dim)
  v_head_dim: 32
  # RoPE 部分 (位置敏感): 16
  rope_head_dim: 16
  # NoPE 部分 (内容敏感): 16
  nope_head_dim: 16

  # --- [MoE] 混合专家配置 ---
  num_experts: 4          # 专家总数
  num_experts_per_tok: 2  # 每次激活 2 个专家 (Top-2 Routing)
  # moe_layers_indices: [1, 2, 3] # (可选) 指定哪些层是 MoE，默认除了第一层外全都是

  # --- 训练优化 ---
  use_activation_checkpointing: true # 强烈建议开启，尤其是 MoE 模型

# 训练超参数
training:
  batch_size: 4
  gradient_accumulation_steps: 4 # 累计梯度，模拟更大 Batch Size
  max_epochs: 1
  learning_rate: 5.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
  clip_grad_norm: 1.0

  # 动态梯度裁剪与尖峰检测 (增强稳定性)
  loss_spike_threshold: 5.0
  max_consecutive_spikes: 5
  grad_norm_history_size: 100
  grad_clip_percentile: 0.9
  dynamic_clip_factor: 1.5

# 日志记录
logging:
  log_interval: 1
  swanlab:
    enable: true
    project: "DeepSeek-V2-Nano-Experiments"
    experiment_name: "{run_name}"
  wandb:
    enable: false

# 检查点保存
checkpointing:
  save_interval: 100 # 每 100 步保存一次
  resume_from: "none"

# END OF FILE: configs/pretrain/variants/deepseek_v2_nano.yaml