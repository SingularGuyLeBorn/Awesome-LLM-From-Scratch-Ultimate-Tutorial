# FILE: configs/rlhf/rm/deepseek_v3_nano_rm.yaml
# ==============================================================================
# 【RM 配置】 DeepSeek-V3 Nano 专用
# 目的：验证在 MLA + MoE 架构上训练奖励模型 (Reward Model)。
# 注意：模型架构参数必须与预训练配置 (deepseek_v3_nano.yaml) 严格一致。
# ==============================================================================

run_name: "rm-deepseek-v3-nano-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  data_dir: "./data_pipeline/processed_data/"

model:
  # --- 必须与预训练模型完全一致 ---
  dim: 64
  n_layers: 2
  n_heads: 4
  vocab_size: 4096
  multiple_of: 16
  norm_eps: 1.0e-5
  max_seq_len: 128
  dropout: 0.0

  # --- MLA ---
  attention_variant: "mla"
  q_lora_rank: 16
  kv_lora_rank: 16
  v_head_dim: 16
  rope_head_dim: 8
  nope_head_dim: 8

  # --- DeepSeekMoE ---
  num_experts: 4
  num_shared_experts: 1
  num_experts_per_tok: 2
  use_aux_free_lb: true

  use_activation_checkpointing: true

rm:
  # 在 fast_dev_run 模式下，这个路径会被自动覆盖为 SFT 产出的最佳检查点
  sft_model_checkpoint: "will_be_overridden_by_fast_dev_run"

training:
  batch_size: 2  # RM 需要成对数据 (chosen/rejected)，batch_size 实际上是 pair 的数量
  max_epochs: 1
  learning_rate: 1.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.1
  min_lr_ratio: 0.1

logging:
  log_interval: 1

# END OF FILE: configs/rlhf/rm/deepseek_v3_nano_rm.yaml