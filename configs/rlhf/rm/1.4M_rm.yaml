# FILE: configs/rlhf/rm/1.4M_rm.yaml
# ===================================================================
# 奖励模型 (RM) 训练配置 - 1.4M 尺寸 (v1.1 - 修复版)
# ===================================================================

run_name: "rm-1.4M-test-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true

data:
  data_dir: "./data_pipeline/processed_data/"
  # [核心修复] 为 preference_data_loader 提供 tokenizer_name 以获取 pad_token_id
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"

model:
  dim: 128
  n_layers: 2
  n_heads: 4
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 256
  dropout: 0.0

rm:
  load_from_checkpoint: "./runs/sft/full/sft-full-1.4M-test-20251116-161123/checkpoints/ckpt_best.pth"

training:
  batch_size: 2
  max_epochs: 4
  learning_rate: 1.0e-5
  weight_decay: 0.0
  warmup_ratio: 0.1
  min_lr_ratio: 0.1

logging:
  log_interval: 1
# END OF FILE: configs/rlhf/rm/1.4M_rm.yaml