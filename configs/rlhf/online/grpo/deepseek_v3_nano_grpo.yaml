# FILE: configs/rlhf/online/grpo/deepseek_v3_nano_grpo.yaml
# ==============================================================================
# 【GRPO 配置】 DeepSeek-V3 Nano 专用
# 验证 Group Relative Policy Optimization 在线对齐流程。
# 核心：不需要 Critic 模型，通过 Group 内对比计算 Advantage。
# ==============================================================================

run_name: "grpo-deepseek-v3-nano-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: true

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  # 使用之前下载的高质量 Prompt
  prompt_data_path: "./data_pipeline/prompts/h4_prompts.txt"

model:
  # --- 必须与预训练模型完全一致 ---
  dim: 64
  n_layers: 2
  n_heads: 4
  vocab_size: 4096
  multiple_of: 16
  norm_eps: 1.0e-5
  max_seq_len: 128
  dropout: 0.0

  # --- MLA ---
  attention_variant: "mla"
  q_lora_rank: 16
  kv_lora_rank: 16
  v_head_dim: 16
  rope_head_dim: 8
  nope_head_dim: 8

  # --- DeepSeekMoE ---
  num_experts: 4
  num_shared_experts: 1
  num_experts_per_tok: 2
  use_aux_free_lb: true

  use_activation_checkpointing: true

rl:
  algorithm: "grpo"
  # 在 fast_dev_run 模式下，这两个路径都会被自动覆盖
  sft_model_checkpoint: "will_be_overridden"
  reward_model_checkpoint: "will_be_overridden"

  kl_coeff: 0.04

  # Group Size: 每个 prompt 生成多少个回复进行对比
  # 必须能被 update_batch_size 整除
  group_size: 4

  # Rollout/Generate 控制
  max_prompt_len: 32
  max_gen_len: 64
  generate:
    temperature: 0.7
    top_k: 20

  # RL 训练流程控制
  rollout_batches: 2  # 只要生成几批数据验证流程
  update_epochs: 1
  update_batch_size: 4 # = group_size * 1

  # GRPO 核心参数
  clip_epsilon: 0.2

training:
  # 此处 batch_size 指的是 prompt 的 batch size
  batch_size: 1
  max_epochs: 1
  learning_rate: 1.0e-6
  weight_decay: 0.0

# END OF FILE: configs/rlhf/online/grpo/deepseek_v3_nano_grpo.yaml