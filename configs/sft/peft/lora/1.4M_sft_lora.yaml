# FILE: configs/sft/peft/lora/1.4M_sft_lora.yaml
# ===================================================================
# SFT LoRA 微调配置 - 1.4M 尺寸模型 (v2 - 字段语义净化)
# ===================================================================

run_name: "sft-lora-1.4M-test-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console:
  verbose: false

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  sft_data_path: "./data_pipeline/processed_data/sft_data.bin"

model:
  dim: 128
  n_layers: 2
  n_heads: 4
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 256
  dropout: 0.0

sft:
  # [核心修改] 字段名 "load_from_checkpoint" -> "base_model_checkpoint"，语义更清晰
  base_model_checkpoint: "./runs/pretrain/pretrain-1.4M-fast-test-CHANGEME/checkpoints/ckpt_best.pth"

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: [ "wq", "wk", "wv", "wo" ]

training:
  batch_size: 2
  gradient_accumulation_steps: 2
  max_epochs: 5
  learning_rate: 3.0e-4
  weight_decay: 0.0
  clip_grad_norm: 1.0
  save_interval: 3
  loss_spike_threshold: 5.0
  max_consecutive_spikes: 5
  grad_norm_history_size: 100
  grad_clip_percentile: 0.9
  dynamic_clip_factor: 1.5
  use_activation_checkpointing: true

logging:
  log_interval: 1