# FILE: configs/sft/peft/qlora/1.4M_sft_qlora.yaml
# ===================================================================
# [占位符] SFT QLoRA 微调配置 - 1.4M 尺寸模型
# ===================================================================

run_name: "sft-qlora-1.4M-test-{timestamp}"
output_dir: "./runs/"
device: "cuda" # QLoRA 强依赖于 CUDA

console:
  verbose: false

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  sft_data_path: "./data_pipeline/processed_data/sft_data.bin"

model:
  dim: 128
  n_layers: 2
  n_heads: 4
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 256
  dropout: 0.0

sft:
  load_from_checkpoint: "./runs/pretrain/pretrain-1.4M-fast-test-CHANGEME/checkpoints/ckpt_best.pth"

qlora:
  # LoRA 参数
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules: [ "wq", "wk", "wv", "wo", "w_gate", "w_up", "w_down" ] # 几乎所有线性层

  # 量化参数 (bitsandbytes)
  bits: 4
  quant_type: "nf4" # 4-bit NormalFloat
  double_quant: true
  compute_dtype: "bfloat16" # 计算时使用的更高精度类型

training:
  batch_size: 1 # QLoRA 内存占用极低，可以适当调大
  gradient_accumulation_steps: 4
  max_epochs: 3
  learning_rate: 2.0e-4
  weight_decay: 0.0
  clip_grad_norm: 0.3 # QLoRA 推荐的梯度裁剪值
  save_interval: 3

logging:
  log_interval: 1

# END FILE: configs/sft/peft/qlora/1.4M_sft_qlora.yaml