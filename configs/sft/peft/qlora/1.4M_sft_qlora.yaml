# FILE: configs/sft/peft/qlora/1.4M_sft_qlora.yaml
# ===================================================================
# SFT QLoRA 微调配置 - 1.4M 尺寸模型
# ===================================================================

run_name: "sft-qlora-1.4M-test-{timestamp}"
output_dir: "./runs/"
device: "cpu" # QLoRA on CPU for testing logic, change to 'cuda' if available

console:
  verbose: true

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  sft_data_path: "./data_pipeline/processed_data/sft_data.bin"

model:
  dim: 128
  n_layers: 2
  n_heads: 4
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 256
  dropout: 0.0

sft:
  # [修正] 字段名统一为 base_model_checkpoint
  base_model_checkpoint: "./runs/pretrain/pretrain-1.4M-fast-test-CHANGEME/checkpoints/ckpt_best.pth"

qlora:
  # LoRA 参数
  r: 8
  alpha: 16
  dropout: 0.05
  # 几乎所有线性层都应该被替换
  target_modules: [ "wq", "wk", "wv", "wo", "w_gate", "w_up", "w_down" ]

  # 计算精度
  compute_dtype: "float32" # CPU 建议 float32，GPU 建议 bfloat16

training:
  batch_size: 2
  gradient_accumulation_steps: 2
  max_epochs: 3
  learning_rate: 2.0e-4
  weight_decay: 0.0
  clip_grad_norm: 0.3 # QLoRA 推荐值
  save_interval: 10

logging:
  log_interval: 1

# END OF FILE: configs/sft/peft/qlora/1.4M_sft_qlora.yaml