# FILE: configs/distillation/1.4M_to_0.5M_distill.yaml
run_name: "distill-1.4M-to-0.5M-{timestamp}"
output_dir: "./runs/"
device: "cpu"

console: { verbose: true }

data:
  tokenizer_name: "./data_pipeline/processed_data/tinystories_project_vs4096.json"
  data_dir: "./data_pipeline/processed_data/"
  train_data_limit: 1000
  val_data_limit: 100

# --- Teacher Model Config (DeepSeek-V2 Nano) ---
teacher_model:
  dim: 128
  n_layers: 4
  n_heads: 4
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 512
  attention_variant: "mla"
  q_lora_rank: 64
  kv_lora_rank: 64
  v_head_dim: 32
  rope_head_dim: 16
  nope_head_dim: 16
  num_experts: 4
  num_experts_per_tok: 2

# --- Student Model Config (Tiny Dense) ---
student_model:
  dim: 64            # 更小的维度
  n_layers: 2        # 更浅的层数
  n_heads: 2
  n_kv_heads: 2
  vocab_size: 4096
  multiple_of: 32
  norm_eps: 1.0e-5
  max_seq_len: 512
  dropout: 0.0
  attention_variant: "mha" # 普通注意力
  num_experts: 0     # 普通 FFN

distillation:
  teacher_checkpoint: "will_be_overridden_by_fast_dev_run"
  temperature: 2.0
  alpha: 0.5 # 50% CE Loss, 50% KD Loss

training:
  batch_size: 4
  gradient_accumulation_steps: 1
  max_epochs: 1
  learning_rate: 5.0e-4
  weight_decay: 0.01
  warmup_ratio: 0.1
  min_lr_ratio: 0.1
  clip_grad_norm: 1.0

logging: { log_interval: 1 }
# END OF FILE: configs/distillation/1.4M_to_0.5M_distill.yaml