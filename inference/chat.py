# FILE: inference/chat.py
# -*- coding: utf-8 -*-
"""
[v1.6 - è¾“å‡ºå‡€åŒ–] äº¤äº’å¼å‘½ä»¤è¡ŒèŠå¤©è„šæœ¬ã€‚
- å¢åŠ è¾“å‡ºå‡€åŒ–é€»è¾‘ï¼Œå°†æ¨¡å‹ç”Ÿæˆçš„æ¢è¡Œç¬¦æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œä»¥ä¿è¯ç»ˆç«¯å•è¡Œæ‰“å­—æœºæ•ˆæœã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from inference.generate import generate_stream
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="ä¸ä½ è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºæ¨¡å‹ç»“æ„ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ã€‚")
    parser.add_argument("--top_p", type=float, default=0.9, help="ç”Ÿæˆæ—¶çš„top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ä¸€æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ã€‚")
    args = parser.parse_args()

    # --- 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')

    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)
    model.load_state_dict(checkpoint['model_state_dict'])
    tokenizer_path = cfg.data.tokenizer_name

    model.eval()
    device = 'cpu'
    model.to(device)
    try:
        model = model.to(torch.bfloat16)
        print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16 ä»¥åŠ é€Ÿæ¨ç†ã€‚")
    except Exception:
        print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    tokenizer = Tokenizer.from_file(tokenizer_path)
    im_start_id = tokenizer.token_to_id("<|im_start|>")
    im_end_id = tokenizer.token_to_id("<|im_end|>")
    eos_id = tokenizer.token_to_id("<|endoftext|>")

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("\n--- å¼€å§‹èŠå¤© (è¾“å…¥ '/quit' é€€å‡º, '/clear' æ¸…ç©ºå†å²) ---")

    history = []

    while True:
        try:
            prompt_text = input("ğŸ˜€ > ")
            if prompt_text.lower() == '/quit':
                break
            if prompt_text.lower() == '/clear':
                history = []
                print("\n--- å†å²å·²æ¸…ç©º ---")
                continue

            # --- 2. æ ¼å¼åŒ–è¾“å…¥ ---
            full_prompt_text = ""
            for q, a in history:
                full_prompt_text += f"<|im_start|>{q}<|im_end|>{a}<|endoftext|>"
            full_prompt_text += f"<|im_start|>{prompt_text}<|im_end|>"

            prompt_tokens = tokenizer.encode(full_prompt_text).ids
            prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)

            # --- 3. æµå¼ç”Ÿæˆ ---
            print("ğŸ¤– > ", end="", flush=True)
            response_tokens = []
            start_time = time.perf_counter()

            token_stream = generate_stream(
                model,
                prompt_tensor,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                eos_id=eos_id
            )

            generated_text = ""
            for token_id in token_stream:
                if token_id in [im_end_id, im_start_id, eos_id]:
                    break

                response_tokens.append(token_id)
                new_text = tokenizer.decode(response_tokens)

                newly_generated_part = new_text[len(generated_text):]

                # [æ ¸å¿ƒä¿®å¤] å‡€åŒ–è¾“å‡ºä»¥è·å¾—å¹²å‡€çš„å•è¡Œæ‰“å­—æœºæ•ˆæœ
                # æ¨¡å‹å¯èƒ½ä¼šç”Ÿæˆ \n, \r ç­‰å­—ç¬¦ï¼Œç›´æ¥æ‰“å°ä¼šå¯¼è‡´ç»ˆç«¯æ˜¾ç¤ºæ··ä¹±ã€‚
                # æˆ‘ä»¬å°†å…¶æ›¿æ¢ä¸ºç©ºæ ¼ï¼Œç¡®ä¿å…‰æ ‡å§‹ç»ˆåœ¨åŒä¸€è¡Œã€‚
                sanitized_part = newly_generated_part.replace('\n', ' ').replace('\r', '')

                print(sanitized_part, end="", flush=True)

                generated_text = new_text

            # --- 4. ç»“æŸä¸ç»Ÿè®¡ ---
            end_time = time.perf_counter()
            duration = end_time - start_time
            num_tokens = len(response_tokens)
            tokens_per_sec = num_tokens / duration if duration > 0 else float('inf')

            # æœ€ç»ˆçš„å®Œæ•´å›ç­”ä¹Ÿéœ€è¦è¢«å‡€åŒ–ï¼Œä»¥ä¾¿åœ¨å†å²è®°å½•ä¸­ä¿æŒæ•´æ´
            final_response = generated_text.replace('\n', ' ').replace('\r', ' ').strip()

            # ç”±äºæˆ‘ä»¬æ˜¯å¢é‡æ‰“å°ï¼Œæœ€åéœ€è¦æ‰“å°ä¸€ä¸ªæ¢è¡Œç¬¦æ¥ç»“æŸè¿™ä¸€è¡Œ
            print()
            print(f"   (ç”Ÿæˆ {num_tokens} tokens, è€—æ—¶ {duration:.2f}s, é€Ÿåº¦: {tokens_per_sec:.2f} tok/s)")

            # æ›´æ–°å†å²
            history.append((prompt_text, final_response))

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å‘Šè¾ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
# END OF FILE: inference/chat.py