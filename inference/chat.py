# FILE: inference/chat.py
# -*- coding: utf-8 -*-
"""
[v2.0 - Universal Chat Interface] ÈÄöÁî®ÂØπËØùÁªàÁ´Ø
ÊîØÊåÅÂä†ËΩΩÊú¨È°πÁõÆÂÖ®ÁîüÂëΩÂë®ÊúüÁöÑÊ®°Âûã‰∫ßÁâ©Ôºö
1. Full Weights: Pretrain, Full SFT, RM, DPO, PPO, GRPO, GSPO
2. Adapters: LoRA, QLoRA (Ëá™Âä®Âä†ËΩΩ Base + Adapter)
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- Ë∑ØÂæÑ‰øÆÂ§ç ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model, load_qlora_model_for_inference
from inference.generate import generate_stream
from inference.quantization import Quantizer
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="‰∏é‰Ω†ËÆ≠ÁªÉÁöÑÊ®°ÂûãËøõË°å‰∫§‰∫íÂºèËÅäÂ§©„ÄÇ")
    parser.add_argument("--config_path", type=str, required=True, help="Ê®°ÂûãÈÖçÁΩÆÊñá‰ª∂ (.yaml)ÔºåÁî®‰∫éÊûÑÂª∫Ê®°ÂûãÈ™®Êû∂„ÄÇ")
    parser.add_argument("--checkpoint_path", type=str, required=True,
                        help="Ê®°ÂûãÊùÉÈáçË∑ØÂæÑ (Base Model Êàñ Full Finetuned Model)„ÄÇ")
    parser.add_argument("--adapter_path", type=str, default=None, help="[ÂèØÈÄâ] LoRA/QLoRA ÈÄÇÈÖçÂô®ÊùÉÈáçË∑ØÂæÑ„ÄÇ")
    parser.add_argument("--temperature", type=float, default=0.7, help="ÁîüÊàêÊ∏©Â∫¶ (0.0-1.0)„ÄÇ")
    parser.add_argument("--top_p", type=float, default=0.9, help="Top-P ÈááÊ†∑ÂèÇÊï∞„ÄÇ")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ÊúÄÂ§ßÁîüÊàêÈïøÂ∫¶„ÄÇ")
    parser.add_argument("--quantize", action="store_true", help="[‰ªÖÈùûQLoRA] ÂêØÁî® Int8 Âä®ÊÄÅÈáèÂåñ‰ª•Âä†ÈÄü CPU Êé®ÁêÜ„ÄÇ")
    args = parser.parse_args()

    # --- 0. ÁéØÂ¢ÉÂàùÂßãÂåñ ---
    print("\n" + "=" * 60)
    print(f"{'üöÄ LLM Chat Terminal':^60}")
    print("=" * 60)

    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)
    device = 'cpu'  # Âº∫Âà∂ CPU Êé®ÁêÜÔºåÁ°Æ‰øùÂÖºÂÆπÊÄß

    # --- 1. Ê®°ÂûãÂä†ËΩΩÈÄªËæë ---
    model = None

    if args.adapter_path:
        # [Ê®°Âºè A] Base + Adapter (LoRA/QLoRA)
        print(f"üõ†Ô∏è  Mode: [Adapter Fusion]")
        print(f"    Base Model:    {Path(args.checkpoint_path).name}")
        print(f"    Adapter:       {Path(args.adapter_path).name}")

        model = load_qlora_model_for_inference(
            config=cfg,
            base_ckpt_path=args.checkpoint_path,
            adapter_ckpt_path=args.adapter_path,
            device=device
        )

        if args.quantize:
            print("‚ÑπÔ∏è  Info: QLoRA Ê®°ÂºèÂ∑≤ÂåÖÂê´ 4-bit ÈáèÂåñÔºåÂøΩÁï• --quantize ÂèÇÊï∞„ÄÇ")

    else:
        # [Ê®°Âºè B] Full Weights (Pretrain/SFT/RLHF)
        print(f"üì¶  Mode: [Full Weights]")
        print(f"    Checkpoint:    {Path(args.checkpoint_path).name}")

        print("    -> Building model architecture...")
        model = build_model(cfg.model)

        print(f"    -> Loading state dictionary...")
        checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
        # ÂÖºÂÆπ‰øùÂ≠ò‰∫ÜÂÆåÊï¥ checkpoint ÁöÑÊÉÖÂÜµ
        state_dict = checkpoint['model_state_dict'] if 'model_state_dict' in checkpoint else checkpoint
        model.load_state_dict(state_dict, strict=False)

        model.eval()
        model.to(device)

        # Âä®ÊÄÅÈáèÂåñ‰∏éÁ≤æÂ∫¶Â§ÑÁêÜ
        if args.quantize:
            print("    -> Applying Dynamic Int8 Quantization (CPU)...")
            model = Quantizer.quantize_dynamic(model)
        else:
            try:
                model = model.to(torch.bfloat16)
                print("    -> Converted to bfloat16 for inference.")
            except Exception:
                print("    -> CPU does not support bfloat16, using float32.")

    # --- 2. Âä†ËΩΩÂàÜËØçÂô® ---
    tokenizer_path = cfg.data.tokenizer_name
    print(f"üìñ  Tokenizer:     {Path(tokenizer_path).name}")
    tokenizer = Tokenizer.from_file(tokenizer_path)

    # ÁâπÊÆä Token ID
    im_start_id = tokenizer.token_to_id("<|im_start|>")
    im_end_id = tokenizer.token_to_id("<|im_end|>")
    eos_id = tokenizer.token_to_id("<|endoftext|>")

    if im_start_id is None:
        print("‚ö†Ô∏è  Warning: Chat tokens not found. Standard completion mode.")

    print("=" * 60)
    print("üí° Tips: ËæìÂÖ• '/quit' ÈÄÄÂá∫, '/clear' Ê∏ÖÁ©∫ÂéÜÂè≤")
    print("-" * 60)

    # --- 3. ‰∫§‰∫íÂæ™ÁéØ ---
    history = []

    while True:
        try:
            prompt_text = input("\nüòÄ User > ")
            if prompt_text.strip().lower() == '/quit':
                print("üëã Bye!")
                break
            if prompt_text.strip().lower() == '/clear':
                history = []
                print("üßπ History cleared.")
                continue
            if not prompt_text.strip():
                continue

            # ÊûÑÂª∫ Chat Prompt
            # Ê†ºÂºè: <|im_start|>user\n{msg}<|im_end|>\n<|im_start|>assistant\n
            full_prompt_text = ""
            for q, a in history:
                if im_start_id is not None:
                    full_prompt_text += f"<|im_start|>user\n{q}<|im_end|>\n<|im_start|>assistant\n{a}<|im_end|>\n"
                else:
                    full_prompt_text += f"{q}\n{a}\n"

            if im_start_id is not None:
                full_prompt_text += f"<|im_start|>user\n{prompt_text}<|im_end|>\n<|im_start|>assistant\n"
            else:
                full_prompt_text += f"{prompt_text}"

            # ÁºñÁ†Å
            encoded = tokenizer.encode(full_prompt_text)
            prompt_tokens = torch.tensor([encoded.ids], dtype=torch.long, device=device)

            # ÊµÅÂºèÁîüÊàê
            print("ü§ñ AI   > ", end="", flush=True)
            response_tokens = []
            start_time = time.perf_counter()

            # Á°Æ‰øùÊ®°ÂûãÂú® eval Ê®°Âºè
            model.eval()

            token_stream = generate_stream(
                model,
                prompt_tokens,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                eos_id=eos_id
            )

            generated_text = ""
            for token_id in token_stream:
                # ÈÅáÂà∞ÁâπÊÆä token ÂÅúÊ≠¢
                if token_id in [im_end_id, eos_id]:
                    break
                # Â¶ÇÊûúÊòØ im_startÔºåÈÄöÂ∏∏‰∏çÂ∫îËØ•ÁîüÊàêÂá∫Êù•Ôºå‰ΩÜ‰πü‰Ωú‰∏∫ÂÅúÊ≠¢Á¨¶Â§ÑÁêÜ
                if im_start_id is not None and token_id == im_start_id:
                    break

                response_tokens.append(token_id)
                new_text = tokenizer.decode(response_tokens)

                # Â¢ûÈáèÊâìÂç∞
                newly_generated_part = new_text[len(generated_text):]

                # ÁÆÄÂçïÁöÑÊµÅÂºèËæìÂá∫Ê∏ÖÊ¥ó
                print(newly_generated_part, end="", flush=True)
                generated_text = new_text

            # ÁªüËÆ°
            end_time = time.perf_counter()
            duration = end_time - start_time
            num_tokens = len(response_tokens)
            tps = num_tokens / duration if duration > 0 else 0

            print(f"\n\n[Speed: {tps:.2f} tok/s | Time: {duration:.2f}s]")

            history.append((prompt_text, generated_text.strip()))

        except KeyboardInterrupt:
            print("\n‚õî Interrupted.")
            break
        except Exception as e:
            print(f"\n‚ùå Error: {e}")
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
# END OF FILE: inference/chat.py