# FILE: inference/chat.py
# -*- coding: utf-8 -*-
"""
[v1.3 - è·¯å¾„ä¿®å¤ç‰ˆ] äº¤äº’å¼å‘½ä»¤è¡ŒèŠå¤©è„šæœ¬ã€‚
- ä¿®å¤äº†åŠ è½½é…ç½®æ–‡ä»¶æ—¶ï¼Œç›¸å¯¹è·¯å¾„è§£æé”™è¯¯çš„é—®é¢˜ã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from inference.generate import generate_stream
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="ä¸ä½ è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºæ¨¡å‹ç»“æ„ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ã€‚")
    parser.add_argument("--top_p", type=float, default=0.9, help="ç”Ÿæˆæ—¶çš„top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ä¸€æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ã€‚")
    args = parser.parse_args()

    # --- 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')

    # [æ ¸å¿ƒä¿®å¤] ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä½œä¸ºåŸºç¡€è·¯å¾„æ¥åŠ è½½é…ç½®
    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)
    model.load_state_dict(checkpoint['model_state_dict'])
    tokenizer_path = cfg.data.tokenizer_name

    model.eval()
    device = 'cpu'
    model.to(device)
    try:
        model = model.to(torch.bfloat16)
        print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16 ä»¥åŠ é€Ÿæ¨ç†ã€‚")
    except Exception:
        print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    tokenizer = Tokenizer.from_file(tokenizer_path)
    im_start_id = tokenizer.token_to_id("<|im_start|>")
    im_end_id = tokenizer.token_to_id("<|im_end|>")
    eos_id = tokenizer.token_to_id("<|endoftext|>")

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("\n--- å¼€å§‹èŠå¤© (è¾“å…¥ '/quit' é€€å‡º, '/clear' æ¸…ç©ºå†å²) ---")

    history = []

    while True:
        try:
            prompt_text = input("ğŸ˜€ > ")
            if prompt_text.lower() == '/quit':
                break
            if prompt_text.lower() == '/clear':
                history = []
                print("\n--- å†å²å·²æ¸…ç©º ---")
                continue

            # --- 2. æ ¼å¼åŒ–è¾“å…¥ ---
            full_prompt_text = ""
            for q, a in history:
                full_prompt_text += f"<|im_start|>{q}<|im_end|>{a}<|endoftext|>"
            full_prompt_text += f"<|im_start|>{prompt_text}<|im_end|>"

            prompt_tokens = tokenizer.encode(full_prompt_text).ids
            prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)

            # --- 3. æµå¼ç”Ÿæˆ ---
            print("ğŸ¤– > ", end="", flush=True)
            response_tokens = []
            start_time = time.perf_counter()

            token_stream = generate_stream(
                model,
                prompt_tensor,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                eos_id=eos_id
            )

            for token_id in token_stream:
                if token_id in [im_end_id, im_start_id, eos_id]:
                    break
                response_tokens.append(token_id)
                decoded_text = tokenizer.decode(response_tokens)
                print("\r" + "ğŸ¤– > " + decoded_text, end="", flush=True)

            # --- 4. ç»“æŸä¸ç»Ÿè®¡ ---
            end_time = time.perf_counter()
            duration = end_time - start_time
            num_tokens = len(response_tokens)
            tokens_per_sec = num_tokens / duration if duration > 0 else float('inf')

            final_response = tokenizer.decode(response_tokens).strip()
            print("\r" + "ğŸ¤– > " + final_response, flush=True)
            print(f"\n   (ç”Ÿæˆ {num_tokens} tokens, è€—æ—¶ {duration:.2f}s, é€Ÿåº¦: {tokens_per_sec:.2f} tok/s)")

            # æ›´æ–°å†å²
            history.append((prompt_text, final_response))

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å‘Šè¾ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
# END OF FILE: inference/chat.py