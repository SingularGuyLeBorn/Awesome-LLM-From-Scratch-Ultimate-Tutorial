# FILE: inference/chat.py
# -*- coding: utf-8 -*-
"""
[v1.7 - é›†æˆé‡åŒ–] äº¤äº’å¼å‘½ä»¤è¡ŒèŠå¤©è„šæœ¬ã€‚
- æ–°å¢ `--quantize` å‚æ•°ï¼Œæ”¯æŒ Int8 åŠ¨æ€é‡åŒ–åŠ é€Ÿ CPU æ¨ç†ã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from inference.generate import generate_stream
from inference.quantization import Quantizer
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="ä¸ä½ è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºæ¨¡å‹ç»“æ„ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ã€‚")
    parser.add_argument("--top_p", type=float, default=0.9, help="ç”Ÿæˆæ—¶çš„top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ä¸€æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ã€‚")
    parser.add_argument("--quantize", action="store_true", help="[New] æ˜¯å¦å¯¹æ¨¡å‹è¿›è¡Œ Int8 åŠ¨æ€é‡åŒ–ä»¥åŠ é€Ÿ CPU æ¨ç†ã€‚")
    args = parser.parse_args()

    # --- 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')

    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)
    model.load_state_dict(checkpoint['model_state_dict'])
    tokenizer_path = cfg.data.tokenizer_name

    model.eval()
    device = 'cpu' # åŠ¨æ€é‡åŒ–ç›®å‰ä¸»è¦åœ¨ CPU ä¸Šæœ‰æ•ˆ
    model.to(device)

    # é‡åŒ–å¤„ç†é€»è¾‘
    if args.quantize:
        print("\nâš–ï¸ æ­£åœ¨åº”ç”¨ Int8 åŠ¨æ€é‡åŒ– (Dynamic Quantization)...")
        print("   è¿™ä¼šæ˜¾è‘—é™ä½å†…å­˜å ç”¨å¹¶åŠ é€Ÿ CPU æ¨ç†ï¼Œä½†å¯èƒ½ä¼šå¸¦æ¥å¾®å°çš„ç²¾åº¦æŸå¤±ã€‚")
        model = Quantizer.quantize_dynamic(model)
        print("âœ… æ¨¡å‹å·²é‡åŒ–ã€‚")
    else:
        try:
            model = model.to(torch.bfloat16)
            print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16 ä»¥åŠ é€Ÿæ¨ç†ã€‚")
        except Exception:
            print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    tokenizer = Tokenizer.from_file(tokenizer_path)
    im_start_id = tokenizer.token_to_id("<|im_start|>")
    im_end_id = tokenizer.token_to_id("<|im_end|>")
    eos_id = tokenizer.token_to_id("<|endoftext|>")

    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")
    print("\n--- å¼€å§‹èŠå¤© (è¾“å…¥ '/quit' é€€å‡º, '/clear' æ¸…ç©ºå†å²) ---")

    history = []

    while True:
        try:
            prompt_text = input("ğŸ˜€ > ")
            if prompt_text.lower() == '/quit':
                break
            if prompt_text.lower() == '/clear':
                history = []
                print("\n--- å†å²å·²æ¸…ç©º ---")
                continue

            # --- 2. æ ¼å¼åŒ–è¾“å…¥ ---
            full_prompt_text = ""
            for q, a in history:
                full_prompt_text += f"<|im_start|>{q}<|im_end|>{a}<|endoftext|>"
            full_prompt_text += f"<|im_start|>{prompt_text}<|im_end|>"

            prompt_tokens = tokenizer.encode(full_prompt_text).ids
            prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)

            # --- 3. æµå¼ç”Ÿæˆ ---
            print("ğŸ¤– > ", end="", flush=True)
            response_tokens = []
            start_time = time.perf_counter()

            token_stream = generate_stream(
                model,
                prompt_tensor,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                eos_id=eos_id
            )

            generated_text = ""
            for token_id in token_stream:
                if token_id in [im_end_id, im_start_id, eos_id]:
                    break

                response_tokens.append(token_id)
                new_text = tokenizer.decode(response_tokens)

                newly_generated_part = new_text[len(generated_text):]

                # å‡€åŒ–è¾“å‡ºä»¥è·å¾—å¹²å‡€çš„å•è¡Œæ‰“å­—æœºæ•ˆæœ
                sanitized_part = newly_generated_part.replace('\n', ' ').replace('\r', '')

                print(sanitized_part, end="", flush=True)

                generated_text = new_text

            # --- 4. ç»“æŸä¸ç»Ÿè®¡ ---
            end_time = time.perf_counter()
            duration = end_time - start_time
            num_tokens = len(response_tokens)
            tokens_per_sec = num_tokens / duration if duration > 0 else float('inf')

            final_response = generated_text.replace('\n', ' ').replace('\r', ' ').strip()

            print()
            print(f"   (ç”Ÿæˆ {num_tokens} tokens, è€—æ—¶ {duration:.2f}s, é€Ÿåº¦: {tokens_per_sec:.2f} tok/s)")

            history.append((prompt_text, final_response))

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å‘Šè¾ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºç°é”™è¯¯: {e}")


if __name__ == "__main__":
    main()
# END OF FILE: inference/chat.py