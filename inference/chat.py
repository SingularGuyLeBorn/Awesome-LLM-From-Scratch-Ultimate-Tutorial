# FILE: inference/chat.py
# -*- coding: utf-8 -*-
"""
[v1.8 - QLoRA Inference Support] äº¤äº’å¼å‘½ä»¤è¡ŒèŠå¤©è„šæœ¬ã€‚
- æ–°å¢ `--adapter_path` å‚æ•°ï¼Œæ”¯æŒåŠ è½½ QLoRA é€‚é…å™¨ã€‚
- è‡ªåŠ¨è¯†åˆ«æ˜¯å¦éœ€è¦è¿›è¡Œ QLoRA ç»„è£…ã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model, load_qlora_model_for_inference
from inference.generate import generate_stream
from inference.quantization import Quantizer
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="ä¸ä½ è®­ç»ƒçš„æ¨¡å‹è¿›è¡Œäº¤äº’å¼èŠå¤©ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="åŸºåº§æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--adapter_path", type=str, default=None, help="[QLoRA] LoRA é€‚é…å™¨æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ï¼Œç”¨äºæ„å»ºæ¨¡å‹ç»“æ„ã€‚")
    parser.add_argument("--temperature", type=float, default=0.7, help="ç”Ÿæˆæ—¶çš„æ¸©åº¦å‚æ•°ã€‚")
    parser.add_argument("--top_p", type=float, default=0.9, help="ç”Ÿæˆæ—¶çš„top-p (nucleus) é‡‡æ ·å‚æ•°ã€‚")
    parser.add_argument("--max_new_tokens", type=int, default=256, help="ä¸€æ¬¡ç”Ÿæˆçš„æœ€å¤§tokenæ•°ã€‚")
    parser.add_argument("--quantize", action="store_true",
                        help="æ˜¯å¦å¯¹æ¨¡å‹è¿›è¡Œ Int8 åŠ¨æ€é‡åŒ– (ä»…å½“æœªä½¿ç”¨ QLoRA æ—¶æœ‰æ•ˆ)ã€‚")
    args = parser.parse_args()

    # --- 0. åŠ è½½é…ç½® ---
    print("ğŸš€ åˆå§‹åŒ–èŠå¤©ç¯å¢ƒ...")
    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)
    device = 'cpu'  # é»˜è®¤ CPU æ¨ç†

    # --- 1. åŠ è½½æ¨¡å‹ ---
    if args.adapter_path:
        print(f"ğŸ› ï¸ æ£€æµ‹åˆ° Adapter è·¯å¾„ï¼Œå¯åŠ¨ QLoRA æ¨¡å¼...")
        print(f"   Base: {args.checkpoint_path}")
        print(f"   Adapter: {args.adapter_path}")

        model = load_qlora_model_for_inference(
            config=cfg,
            base_ckpt_path=args.checkpoint_path,
            adapter_ckpt_path=args.adapter_path,
            device=device
        )
        if args.quantize:
            print("âš ï¸ æç¤º: QLoRA æ¨¡å¼ä¸‹æ¨¡å‹å·²ç»æ˜¯ 4-bit é‡åŒ–ï¼Œå¿½ç•¥ --quantize å‚æ•°ã€‚")

    else:
        print("ğŸ“¦ åŠ è½½æ ‡å‡†æ¨¡å‹...")
        checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
        model = build_model(cfg.model)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()
        model.to(device)

        # æ ‡å‡†æ¨¡å¼ä¸‹çš„å¯é€‰é‡åŒ–
        if args.quantize:
            print("\nâš–ï¸ æ­£åœ¨åº”ç”¨ Int8 åŠ¨æ€é‡åŒ– (Dynamic Quantization)...")
            model = Quantizer.quantize_dynamic(model)
            print("âœ… æ¨¡å‹å·²é‡åŒ–ã€‚")
        else:
            try:
                model = model.to(torch.bfloat16)
                print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16 ä»¥åŠ é€Ÿæ¨ç†ã€‚")
            except Exception:
                print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    # --- 2. åŠ è½½åˆ†è¯å™¨ ---
    tokenizer_path = cfg.data.tokenizer_name
    tokenizer = Tokenizer.from_file(tokenizer_path)
    im_start_id = tokenizer.token_to_id("<|im_start|>")
    im_end_id = tokenizer.token_to_id("<|im_end|>")
    eos_id = tokenizer.token_to_id("<|endoftext|>")

    print("âœ… ç³»ç»Ÿå‡†å¤‡å°±ç»ªï¼")
    print("\n--- å¼€å§‹èŠå¤© (è¾“å…¥ '/quit' é€€å‡º, '/clear' æ¸…ç©ºå†å²) ---")

    history = []

    while True:
        try:
            prompt_text = input("ğŸ˜€ > ")
            if prompt_text.lower() == '/quit':
                break
            if prompt_text.lower() == '/clear':
                history = []
                print("\n--- å†å²å·²æ¸…ç©º ---")
                continue

            # --- 3. æ ¼å¼åŒ–è¾“å…¥ ---
            full_prompt_text = ""
            for q, a in history:
                full_prompt_text += f"<|im_start|>{q}<|im_end|>{a}<|endoftext|>"
            full_prompt_text += f"<|im_start|>{prompt_text}<|im_end|>"

            prompt_tokens = tokenizer.encode(full_prompt_text).ids
            prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)

            # --- 4. æµå¼ç”Ÿæˆ ---
            print("ğŸ¤– > ", end="", flush=True)
            response_tokens = []
            start_time = time.perf_counter()

            # ç¡®ä¿æ¨¡å‹åœ¨ eval æ¨¡å¼
            model.eval()

            token_stream = generate_stream(
                model,
                prompt_tensor,
                max_new_tokens=args.max_new_tokens,
                temperature=args.temperature,
                top_p=args.top_p,
                eos_id=eos_id
            )

            generated_text = ""
            for token_id in token_stream:
                if token_id in [im_end_id, im_start_id, eos_id]:
                    break

                response_tokens.append(token_id)
                new_text = tokenizer.decode(response_tokens)

                newly_generated_part = new_text[len(generated_text):]

                # å‡€åŒ–è¾“å‡ºä»¥è·å¾—å¹²å‡€çš„å•è¡Œæ‰“å­—æœºæ•ˆæœ
                sanitized_part = newly_generated_part.replace('\n', ' ').replace('\r', '')

                print(sanitized_part, end="", flush=True)

                generated_text = new_text

            # --- 5. ç»“æŸä¸ç»Ÿè®¡ ---
            end_time = time.perf_counter()
            duration = end_time - start_time
            num_tokens = len(response_tokens)
            tokens_per_sec = num_tokens / duration if duration > 0 else float('inf')

            final_response = generated_text.replace('\n', ' ').replace('\r', ' ').strip()

            print()
            print(f"   (ç”Ÿæˆ {num_tokens} tokens, è€—æ—¶ {duration:.2f}s, é€Ÿåº¦: {tokens_per_sec:.2f} tok/s)")

            history.append((prompt_text, final_response))

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å‘Šè¾ï¼")
            break
        except Exception as e:
            print(f"\nâŒ å‡ºç°é”™è¯¯: {e}")
            # æ‰“å°æ›´è¯¦ç»†çš„é”™è¯¯å †æ ˆä»¥ä¾¿è°ƒè¯•
            import traceback
            traceback.print_exc()


if __name__ == "__main__":
    main()
# END OF FILE: inference/chat.py