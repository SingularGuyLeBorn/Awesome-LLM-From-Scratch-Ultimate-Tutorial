# FILE: inference/quantization.py
# -*- coding: utf-8 -*-
"""
[v1.0 - åŠ¨æ€é‡åŒ–å®ç°] CPUæ¨ç†åŠ é€Ÿæ¨¡å—
åŠŸèƒ½: æä¾›å°† PyTorch æ¨¡å‹åŠ¨æ€é‡åŒ–ä¸º Int8 çš„å·¥å…·ã€‚
è¿™å¯¹äºåœ¨å†…å­˜å—é™ï¼ˆå¦‚ 16GB RAMï¼‰çš„ CPU è®¾å¤‡ä¸Šè¿è¡Œè¾ƒå¤§æ¨¡å‹ï¼ˆ>1Bï¼‰è‡³å…³é‡è¦ã€‚
"""
import torch
import torch.nn as nn
import time
import logging


class Quantizer:
    """
    åŠ¨æ€é‡åŒ–å™¨ã€‚
    ç›®å‰æ”¯æŒ PyTorch åŸç”Ÿçš„åŠ¨æ€é‡åŒ– (Dynamic Quantization)ã€‚
    å®ƒå°† nn.Linear å±‚çš„æƒé‡è½¬æ¢ä¸º int8ï¼Œä½†åœ¨è®¡ç®—æ—¶ä¼šå°†æ¿€æ´»å€¼ä¿æŒä¸ºæµ®ç‚¹æ•°ï¼ˆæˆ–åŠ¨æ€é‡åŒ–ï¼‰ï¼Œ
    ä»è€Œæ˜¾è‘—å‡å°‘æ¨¡å‹å¤§å°å¹¶åŠ é€Ÿ CPU æ¨ç†ã€‚
    """

    @staticmethod
    def quantize_dynamic(model: nn.Module, dtype=torch.qint8) -> nn.Module:
        """
        å¯¹æ¨¡å‹åº”ç”¨åŠ¨æ€é‡åŒ–ã€‚

        Args:
            model: å¾…é‡åŒ–çš„ PyTorch æ¨¡å‹ (é€šå¸¸æ˜¯ float32 æˆ– bfloat16)ã€‚
            dtype: ç›®æ ‡é‡åŒ–ç±»å‹ï¼Œé€šå¸¸æ˜¯ torch.qint8ã€‚

        Returns:
            nn.Module: é‡åŒ–åçš„æ¨¡å‹ã€‚
        """
        start_time = time.perf_counter()
        logging.info(f"âš–ï¸ æ­£åœ¨å¯¹æ¨¡å‹åº”ç”¨åŠ¨æ€é‡åŒ– (Target: {dtype})...")

        # PyTorch åŠ¨æ€é‡åŒ–ä¸»è¦é’ˆå¯¹ Linear å’Œ LSTM/GRU/RNN å±‚
        # æˆ‘ä»¬ä¸»è¦å…³æ³¨ Linear å±‚
        quantized_model = torch.quantization.quantize_dynamic(
            model,
            {nn.Linear},  # åªé‡åŒ–çº¿æ€§å±‚
            dtype=dtype
        )

        end_time = time.perf_counter()
        logging.info(f"âœ… åŠ¨æ€é‡åŒ–å®Œæˆã€‚è€—æ—¶: {end_time - start_time:.2f}s")
        return quantized_model

    @staticmethod
    def print_model_size(model: nn.Module, name: str = "Model"):
        """
        æ‰“å°æ¨¡å‹çš„å‚æ•°å¤§å°ï¼ˆä»¥ MB ä¸ºå•ä½ï¼‰ã€‚
        """
        torch.save(model.state_dict(), "temp.p")
        size_mb = os.path.getsize("temp.p") / 1e6
        print(f"ğŸ“¦ {name} Size: {size_mb:.2f} MB")
        os.remove("temp.p")


if __name__ == "__main__":
    # ç®€å•çš„æµ‹è¯•ç”¨ä¾‹
    import os

    print("--- æµ‹è¯•åŠ¨æ€é‡åŒ– ---")
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(512, 1024)
            self.relu = nn.ReLU()
            self.fc2 = nn.Linear(1024, 256)

        def forward(self, x):
            return self.fc2(self.relu(self.fc1(x)))

    model = SimpleModel()
    print("åŸå§‹æ¨¡å‹ç»“æ„:")
    print(model)
    Quantizer.print_model_size(model, "Original Model")

    # åº”ç”¨é‡åŒ–
    q_model = Quantizer.quantize_dynamic(model)
    print("\né‡åŒ–åæ¨¡å‹ç»“æ„:")
    print(q_model)
    Quantizer.print_model_size(q_model, "Quantized Model")

    # éªŒè¯æ¨ç†
    input_tensor = torch.randn(1, 512)
    with torch.no_grad():
        out_orig = model(input_tensor)
        out_quant = q_model(input_tensor)

    # æ³¨æ„ï¼šé‡åŒ–ä¼šæœ‰ç²¾åº¦æŸå¤±ï¼Œæ‰€ä»¥ assert allclose å¯èƒ½ä¼šå¤±è´¥ï¼Œè¿™é‡Œåªæ‰“å°å·®å€¼
    diff = (out_orig - out_quant).abs().mean().item()
    print(f"\nå¹³å‡è¾“å‡ºå·®å¼‚: {diff:.6f}")
    print("âœ… æµ‹è¯•å®Œæˆã€‚")

# END OF FILE: inference/quantization.py