# FILE: inference/paged_engine_demo.py
# -*- coding: utf-8 -*-
"""
[æ–°å¢] PagedAttention æ¨ç†å¼•æ“çš„æ¼”ç¤ºè„šæœ¬ã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from utils.config_loader import load_config
from utils.builders import build_model
from inference.engine.paged_engine import PagedInferenceEngine
from tokenizers import Tokenizer


def main():
    parser = argparse.ArgumentParser(description="æ¼”ç¤º PagedAttention æ¨ç†å¼•æ“ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ã€‚")
    args = parser.parse_args()

    # --- 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')

    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)
    model.load_state_dict(checkpoint['model_state_dict'])
    tokenizer_path = cfg.data.tokenizer_name

    device = 'cpu'
    model.to(device)
    try:
        model = model.to(torch.bfloat16)
        print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16 ä»¥åŠ é€Ÿæ¨ç†ã€‚")
    except Exception:
        print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    tokenizer = Tokenizer.from_file(tokenizer_path)
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    # --- 2. åˆå§‹åŒ– PagedInferenceEngine ---
    # å‡è®¾æˆ‘ä»¬æœ‰ 256 ä¸ªç‰©ç†å—ï¼Œæ¯ä¸ªå—å¤§å°ä¸º 16 tokens
    engine = PagedInferenceEngine(model, tokenizer, block_size=16, num_blocks=256)
    print("\nğŸ”¥ PagedInferenceEngine åˆå§‹åŒ–å®Œæˆï¼")
    print(f"   - ç‰©ç†å—æ€»æ•°: {engine.block_manager.num_blocks}")
    print(f"   - æ¯å—å¤§å°: {engine.block_size} tokens")

    # --- 3. æ·»åŠ æ¨ç†è¯·æ±‚ ---
    prompts = [
        "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
        "ä»€ä¹ˆæ˜¯å¤§å‹è¯­è¨€æ¨¡å‹ï¼Ÿ",
        "è¯·å†™ä¸€é¦–å…³äºå®‡å®™çš„çŸ­è¯—ã€‚",
        "ä»å‰æœ‰åº§å±±ï¼Œ",
    ]
    for i, prompt in enumerate(prompts):
        engine.add_request(prompt, seq_id=i)

    print(f"\nğŸ“¥ å·²æ·»åŠ  {len(prompts)} ä¸ªæ¨ç†è¯·æ±‚ã€‚")

    # --- 4. è¿è¡Œæ¨ç†å¾ªç¯ ---
    print("\nâ³ å¼€å§‹æ‰§è¡Œæ¨ç†å¾ªç¯ (step-by-step)...")
    start_time = time.perf_counter()
    step = 0
    all_outputs = {}

    while engine.has_unfinished_requests():
        step_start_time = time.perf_counter()

        finished_this_step = engine.step()

        step_end_time = time.perf_counter()

        # æ‰“å°å½“å‰æ­¥çš„çŠ¶æ€
        num_running = len(engine.scheduler.running)
        num_waiting = len(engine.scheduler.waiting)
        num_finished = len(engine.scheduler.finished)
        free_blocks = engine.block_manager.get_num_free_blocks()

        print(
            f"Step {step:>3}: "
            f"Running: {num_running}, Waiting: {num_waiting}, Finished: {num_finished}, "
            f"Free Blocks: {free_blocks:>3}, "
            f"Time: {(step_end_time - step_start_time) * 1000:.2f} ms"
        )

        if finished_this_step:
            all_outputs.update(finished_this_step)
            for seq_id, text in finished_this_step.items():
                print(f"  âœ¨ åºåˆ— {seq_id} å·²å®Œæˆï¼")

        step += 1

    end_time = time.perf_counter()
    print(f"\nâœ… æ‰€æœ‰è¯·æ±‚å¤„ç†å®Œæ¯•ï¼æ€»è€—æ—¶: {end_time - start_time:.2f} s")

    # --- 5. æ‰“å°æœ€ç»ˆç»“æœ ---
    print("\n" + "=" * 20 + " æ¨ç†ç»“æœ " + "=" * 20)
    for i in range(len(prompts)):
        print(f"\n--- Prompt {i} ---\n{prompts[i]}")
        print(f"\n--- Completion {i} ---\n{all_outputs.get(i, 'Error: No output generated')}")
    print("\n" + "=" * 50)


if __name__ == "__main__":
    main()
# END OF FILE: inference/paged_engine_demo.py