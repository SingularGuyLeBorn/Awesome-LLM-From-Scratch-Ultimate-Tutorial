# FILE: inference/api_server.py
# -*- coding: utf-8 -*-
"""
[v2.5 - Final Cleaned Version]

- ç§»é™¤æ‰€æœ‰è°ƒè¯•ç”¨çš„ print è¯­å¥ï¼Œæä¾›ä¸€ä¸ªå¹²å‡€çš„ã€å¯éƒ¨ç½²çš„ç‰ˆæœ¬ã€‚
- æœ€ç»ˆç¡®è®¤ï¼šæœåŠ¡å™¨é€»è¾‘æ­£ç¡®ï¼Œå®¢æˆ·ç«¯æ¥æ”¶é—®é¢˜ç”±å®¢æˆ·ç«¯å·¥å…·ï¼ˆå¦‚ PowerShellï¼‰
  çš„å¤„ç†æ–¹å¼å¯¼è‡´ã€‚æœ¬æœåŠ¡å™¨æ­£ç¡®åœ°è¿”å›äº†æ¨¡å‹çš„åŸå§‹ã€æœªç»ä¿®æ”¹çš„è¾“å‡ºã€‚
"""
import torch
import argparse
from pathlib import Path
import sys
import time
import asyncio
from typing import List, Dict, Any
from contextlib import asynccontextmanager
import uvicorn
from dataclasses import dataclass

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from fastapi import FastAPI
from pydantic import BaseModel, Field
from tokenizers import Tokenizer

from utils.config_loader import load_config
from utils.builders import build_model
from inference.engine.paged_engine import PagedInferenceEngine


# --- 1. ç”Ÿäº§è€…-æ¶ˆè´¹è€…é˜Ÿåˆ—ä¸è¯·æ±‚å¯¹è±¡ ---

@dataclass
class APIRequest:
    """å°è£…ä¸€ä¸ªAPIè¯·æ±‚çš„æ‰€æœ‰ä¿¡æ¯"""
    seq_id: int
    prompt: str
    prompt_tokens: List[int]
    future: asyncio.Future


# å…¨å±€è¯·æ±‚é˜Ÿåˆ—
request_queue: asyncio.Queue = None


# --- 2. Lifespan ä¸Šä¸‹æ–‡ç®¡ç†å™¨ ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    print("ğŸš€ æœåŠ¡å™¨å¯åŠ¨ä¸­... æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆå§‹åŒ–æ¨ç†å¼•æ“...")
    global engine, request_queue

    request_queue = asyncio.Queue()
    args = app.state.args
    checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    model = build_model(cfg.model)
    model.load_state_dict(checkpoint['model_state_dict'])
    tokenizer = Tokenizer.from_file(cfg.data.tokenizer_name)

    device = 'cpu'
    model.to(device)
    try:
        model = model.to(torch.bfloat16)
        print("   -> æ¨¡å‹å·²è½¬æ¢ä¸º bfloat16ã€‚")
    except Exception:
        print("   -> CPU ä¸æ”¯æŒ bfloat16ï¼Œå°†ä½¿ç”¨ float32ã€‚")

    engine = PagedInferenceEngine(model, tokenizer, block_size=16, num_blocks=256)
    print("âœ… PagedInferenceEngine åˆå§‹åŒ–å®Œæˆï¼")

    loop = asyncio.get_running_loop()
    app.state.inference_task = loop.create_task(inference_loop())
    print("ğŸ”¥ æ¨ç†åå°ä»»åŠ¡å·²å¯åŠ¨ã€‚æœåŠ¡å™¨å‡†å¤‡å°±ç»ªï¼")

    yield

    print("ğŸ‘‹ æœåŠ¡å™¨æ­£åœ¨å…³é—­... æ­£åœ¨å–æ¶ˆæ¨ç†ä»»åŠ¡...")
    app.state.inference_task.cancel()
    try:
        await app.state.inference_task
    except asyncio.CancelledError:
        print("   -> æ¨ç†ä»»åŠ¡å·²æˆåŠŸå–æ¶ˆã€‚")
    print("âœ… æœåŠ¡å™¨å·²å…³é—­ã€‚")


# --- 3. FastAPI åº”ç”¨ä¸ Pydantic æ•°æ®æ¨¡å‹ ---
app = FastAPI(lifespan=lifespan)


class ChatCompletionRequest(BaseModel):
    model: str = "llm-from-scratch"
    messages: List[Dict[str, str]]


class ChatCompletionResponseChoice(BaseModel):
    index: int
    message: Dict[str, str]
    finish_reason: str


class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionResponseChoice]


# --- 4. æ ¸å¿ƒåå°ä»»åŠ¡ï¼šæ¨ç†å¾ªç¯ ---
async def inference_loop():
    global engine, request_queue
    active_requests: Dict[int, APIRequest] = {}

    while True:
        try:
            while not request_queue.empty():
                new_request = await request_queue.get()
                engine.add_request(prompt=new_request.prompt, seq_id=new_request.seq_id)
                active_requests[new_request.seq_id] = new_request

            if engine.has_unfinished_requests():
                finished_sequences_tokens = engine.step()
                for seq_id, output_tokens in finished_sequences_tokens.items():
                    if seq_id in active_requests:
                        request = active_requests.pop(seq_id)
                        request.future.set_result(output_tokens)
            else:
                await asyncio.sleep(0.01)

        except asyncio.CancelledError:
            break
        except Exception as e:
            print(f"ğŸ”¥ æ¨ç†å¾ªç¯ä¸­å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            for request in active_requests.values():
                request.future.set_exception(e)
            active_requests.clear()
            await asyncio.sleep(1)


# --- 5. API ç«¯ç‚¹å®ç° ---
@app.post("/v1/chat/completions")
async def create_chat_completion(request: ChatCompletionRequest):
    global request_queue

    user_message = next((msg["content"] for msg in reversed(request.messages) if msg["role"] == "user"), None)
    if user_message is None:
        return {"error": "No user message found."}

    formatted_prompt = f"<|im_start|>{user_message}<|im_end|>"

    loop = asyncio.get_running_loop()
    future = loop.create_future()

    seq_id = int(time.time() * 1000)

    prompt_tokens = engine.tokenizer.encode(formatted_prompt).ids

    api_request = APIRequest(
        seq_id=seq_id,
        prompt=formatted_prompt,
        prompt_tokens=prompt_tokens,
        future=future
    )

    await request_queue.put(api_request)
    output_tokens = await future

    completion_tokens = output_tokens

    if completion_tokens and completion_tokens[-1] == engine.eos_id:
        completion_tokens = completion_tokens[:-1]

    if len(completion_tokens) >= len(prompt_tokens) and completion_tokens[:len(prompt_tokens)] == prompt_tokens:
        completion_tokens = completion_tokens[len(prompt_tokens):]

    # ç›´æ¥è§£ç ï¼Œä¸è¿›è¡Œä»»ä½•å­—ç¬¦ä¸²å¤„ç†
    completion_text = engine.tokenizer.decode(completion_tokens)

    response = ChatCompletionResponse(
        id=f"chatcmpl-{seq_id}",
        model=request.model,
        choices=[
            ChatCompletionResponseChoice(
                index=0,
                message={"role": "assistant", "content": completion_text},
                finish_reason="stop"
            )
        ]
    )
    return response


# --- 6. å¯åŠ¨å™¨ ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯åŠ¨ç¬¦åˆOpenAIæ ‡å‡†çš„FastAPIæ¨ç†æœåŠ¡å™¨ã€‚")
    parser.add_argument("--host", type=str, default="127.0.0.1", help="æœåŠ¡å™¨ç›‘å¬çš„ä¸»æœºåœ°å€ã€‚")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡å™¨ç›‘å¬çš„ç«¯å£ã€‚")
    parser.add_argument("--checkpoint_path", type=str, required=True, help="æ¨¡å‹æ£€æŸ¥ç‚¹ (.pth) çš„è·¯å¾„ã€‚")
    parser.add_argument("--config_path", type=str, required=True, help="æ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml) çš„è·¯å¾„ã€‚")
    args = parser.parse_args()

    app.state.args = args
    uvicorn.run(app, host=args.host, port=args.port)

# END FILE: inference/api_server.py