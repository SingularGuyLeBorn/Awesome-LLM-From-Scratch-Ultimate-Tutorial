# FILE: pretrain/train.py
# -*- coding: utf-8 -*-
"""
ã€v4.3 - é²æ£’æ€§å·…å³°ç‰ˆã€‘ç»Ÿä¸€é¢„è®­ç»ƒ/ç»§ç»­é¢„è®­ç»ƒè„šæœ¬ (DDP + Compile + Auto-Fallback)
- [è‡ªä¸¾] Windows UTF-8 ç¼–ç è‡ªåŠ¨ä¿®å¤ã€‚
- [é¢„æ£€] è‡ªåŠ¨æ£€æµ‹ C++ ç¼–è¯‘å™¨ã€‚å¦‚æœæ²¡æœ‰å®‰è£… VS Build Toolsï¼Œè‡ªåŠ¨å…³é—­ç¼–è¯‘ä»¥å…å´©æºƒã€‚
- [å…¼å®¹] MoE æ¶æ„è‡ªåŠ¨é€‚é… DDP å‚æ•°ã€‚
"""
import os
import sys
import subprocess
import shutil

# --- [Windows å…¼å®¹æ€§è¡¥ä¸: å¿…é¡»åœ¨ä»»ä½•é€»è¾‘æ‰§è¡Œå‰è¿è¡Œ] ---
if os.name == 'nt' and os.environ.get('PYTHONUTF8') != '1':
    print("ğŸ”„ [ç³»ç»Ÿè‡ªä¸¾] Windows ç¯å¢ƒæ£€æµ‹: æ­£åœ¨è®¾ç½® PYTHONUTF8=1 å¹¶é‡å¯è®­ç»ƒè¿›ç¨‹...")
    env = os.environ.copy()
    env['PYTHONUTF8'] = '1'
    try:
        ret = subprocess.call([sys.executable] + sys.argv, env=env)
        sys.exit(ret)
    except Exception as e:
        print(f"âŒ è‡ªä¸¾å¤±è´¥: {e}")
        sys.exit(1)
# -----------------------------------------------------

import torch
import argparse
from pathlib import Path
import time
import shutil

# --- è·¯å¾„ä¿®å¤ ---
project_root = str(Path(__file__).parent.parent)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from torch.nn.parallel import DistributedDataParallel as DDP
from utils.ddp_utils import setup_ddp, cleanup_ddp, get_rank, get_world_size, is_main_process

from utils.config_loader import load_config
from utils.builders import build_model, build_optimizer, build_scheduler, build_loggers
from pretrain.data_loader import get_pretrain_loaders
from pretrain.components.checkpointing import CheckpointManager
from pretrain.components.training_loop import Trainer
from pretrain.components.hooks import register_hooks

try:
    from torch.cuda.amp import GradScaler
except ImportError:
    GradScaler = None


def check_cxx_compiler() -> bool:
    """
    æ£€æŸ¥ç³»ç»Ÿä¸­æ˜¯å¦å­˜åœ¨ C++ ç¼–è¯‘å™¨ (cl.exe for Windows, g++ for Linux/others)ã€‚
    torch.compile(backend='inductor') å¼ºä¾èµ–äº C++ ç¼–è¯‘å™¨ã€‚
    """
    if os.name == 'nt':
        # Windows éœ€è¦ Visual Studio Build Tools (cl.exe)
        # æˆ–è€… MinGW (g++)ï¼Œä½† inductor å¯¹ MSVC æ”¯æŒæœ€å¥½
        if shutil.which('cl') is not None:
            return True
        if shutil.which('g++') is not None:
            return True
        return False
    else:
        # Linux/Mac é€šå¸¸é¢„è£… g++ æˆ– clang
        return shutil.which('c++') is not None or shutil.which('g++') is not None or shutil.which('clang++') is not None


def main():
    parser = argparse.ArgumentParser(description="[v4.3] ç»Ÿä¸€é¢„è®­ç»ƒè„šæœ¬")
    parser.add_argument("--config_path", type=str, required=True, help="æŒ‡å‘é…ç½®YAMLæ–‡ä»¶çš„è·¯å¾„")
    parser.add_argument("--fast_dev_run", action="store_true", help="å¯ç”¨å¿«é€Ÿå¼€å‘è¿è¡Œæ¨¡å¼")
    parser.add_argument("--compile", action="store_true", help="å¯ç”¨ torch.compile (PyTorch 2.0+) åŠ é€Ÿ")
    args = parser.parse_args()

    setup_ddp()
    world_size = get_world_size()

    # --- 0. é…ç½®ä¸æ—¥å¿— ---
    project_base_path = Path(__file__).parent.parent.resolve()
    cfg = load_config(args.config_path, project_base_path)

    output_dir = None
    run_name = ""
    if is_main_process():
        base_output_dir = Path(cfg.output_dir)
        if args.fast_dev_run:
            run_name = "fast-dev-run"
            output_dir = base_output_dir / "pretrain" / run_name
            if output_dir.exists():
                print(f"ğŸ§¹ fast_dev_run æ¨¡å¼ (ä¸»è¿›ç¨‹): æ­£åœ¨æ¸…ç†æ—§çš„å¼€å‘ç›®å½• {output_dir}")
                shutil.rmtree(output_dir)
        else:
            timestamp = time.strftime('%Y%m%d-%H%M%S')
            run_name = cfg.run_name.format(timestamp=timestamp)
            output_dir = base_output_dir / "pretrain" / run_name
        output_dir.mkdir(parents=True, exist_ok=True)

    logger = build_loggers(cfg, output_dir, run_name)
    if is_main_process():
        print(f"é…ç½®åŠ è½½è‡ª: {args.config_path}")
        print(f"æ‰€æœ‰è¾“å‡ºå°†ä¿å­˜åˆ°: {output_dir}")

    # --- 1. æ¨¡å‹ ---
    cfg.model.use_activation_checkpointing = getattr(cfg.training, 'use_activation_checkpointing', False)
    model = build_model(cfg.model).to(cfg.device)

    # [æ€§èƒ½ä¼˜åŒ–] torch.compile æ™ºèƒ½å¤„ç†
    if args.compile:
        can_compile = True
        # 1. æ£€æŸ¥ç¼–è¯‘å™¨ç¯å¢ƒ
        if not check_cxx_compiler():
            if is_main_process():
                print("\nâš ï¸  [è­¦å‘Š] æœªæ£€æµ‹åˆ° C++ ç¼–è¯‘å™¨ (cl.exe æˆ– g++)ï¼")
                print("   torch.compile éœ€è¦ C++ ç¯å¢ƒæ‰èƒ½å·¥ä½œã€‚")
                print("   -> Windows ç”¨æˆ·è¯·å®‰è£…: 'Visual Studio Build Tools' (å‹¾é€‰ C++ æ¡Œé¢å¼€å‘)ã€‚")
                print("   -> æ­£åœ¨è‡ªåŠ¨é™çº§å› Eager æ¨¡å¼ (æ— ç¼–è¯‘) ç»§ç»­è¿è¡Œ...\n")
            can_compile = False

        # 2. æ‰§è¡Œç¼–è¯‘
        if can_compile:
            if is_main_process():
                print("ğŸš€ æ­£åœ¨ç¼–è¯‘æ¨¡å‹ (torch.compile)... é¦–æ¬¡è¿­ä»£å¯èƒ½ä¼šå˜æ…¢ã€‚")
            try:
                # Windows ä¸‹ inductor å¶å°”ä¼šæœ‰è·¯å¾„é—®é¢˜ï¼ŒåŠ ä¸ªä¿æŠ¤
                model = torch.compile(model, backend="inductor")
            except Exception as e:
                if is_main_process():
                    print(f"âŒ ç¼–è¯‘å¤±è´¥: {e}")
                    print("   -> å›é€€åˆ° Eager æ¨¡å¼è¿è¡Œã€‚")

    if world_size > 1:
        has_moe = cfg.model.num_experts > 1
        find_unused = has_moe

        if is_main_process() and has_moe:
            print("âš ï¸ æ£€æµ‹åˆ° MoE æ¶æ„ï¼Œå·²å¯ç”¨ DDP(find_unused_parameters=True)ã€‚")

        model = DDP(
            model,
            device_ids=None if cfg.device == 'cpu' else [int(os.environ["LOCAL_RANK"])],
            find_unused_parameters=find_unused
        )
        if is_main_process():
            print(f"æ¨¡å‹å·²ç”¨ DDP åŒ…è£… (Rank {get_rank()})ã€‚")

    # --- 2. æ•°æ®ã€ä¼˜åŒ–å™¨ã€è°ƒåº¦å™¨ã€æ··åˆç²¾åº¦ ---
    train_limit = getattr(cfg.data, 'train_data_limit', None)
    val_limit = getattr(cfg.data, 'val_data_limit', None)

    train_loader, val_loader = get_pretrain_loaders(
        tokenizer_name=cfg.data.tokenizer_name, data_dir=Path(cfg.data.data_dir),
        block_size=cfg.model.max_seq_len, batch_size=cfg.training.batch_size,
        train_data_limit=train_limit, val_data_limit=val_limit,
        ddp_rank=get_rank(), ddp_world_size=world_size
    )

    model_for_optimizer = model.module if world_size > 1 else model
    optimizer = build_optimizer(model_for_optimizer, cfg.training)
    max_iters = len(train_loader) * cfg.training.max_epochs
    scheduler = build_scheduler(optimizer, cfg.training, max_iters)
    scaler = GradScaler() if cfg.device == 'cuda' and GradScaler else None

    # --- 3. æ£€æŸ¥ç‚¹ç®¡ç†å™¨ä¸åŠ è½½ ---
    if is_main_process():
        print("\n--- 4. åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨ (ä»…ä¸»è¿›ç¨‹) ---")

    ckpt_dir = output_dir / "checkpoints" if is_main_process() else None
    ckpt_manager = CheckpointManager(ckpt_dir, model_for_optimizer, optimizer, scheduler, scaler)
    start_epoch = 0

    load_ckpt_path = getattr(cfg.training, 'load_from_checkpoint', "none")
    load_only_model = getattr(cfg.training, 'load_only_model', False)

    if load_ckpt_path != "none":
        if is_main_process():
            print(f"æ£€æµ‹åˆ°åŠ è½½è¯·æ±‚: {load_ckpt_path}")
        start_epoch = ckpt_manager.load(load_ckpt_path, load_only_model=load_only_model)

    if world_size > 1:
        torch.distributed.barrier()

    # --- 4. é’©å­ä¸è®­ç»ƒå™¨ ---
    if is_main_process():
        print("--- 1.1. ä¸ºæ¨¡å‹æ³¨å†Œç›‘æ§é’©å­ ---")
        try:
            # æ³¨æ„ï¼šç¼–è¯‘åçš„æ¨¡å‹æ³¨å†Œ hook å¯èƒ½ä¼šå—é™ï¼Œè¿™é‡Œå°½åŠ›è€Œä¸º
            hooks = register_hooks(model_for_optimizer)
            print(f"âœ… å·²æˆåŠŸæ³¨å†Œ {len(hooks)} ä¸ªé’©å­ç”¨äºç›‘æ§å†…éƒ¨çŠ¶æ€ã€‚")
        except Exception as e:
            print(f"âš ï¸ é’©å­æ³¨å†Œå¤±è´¥ (å¯èƒ½å— torch.compile å½±å“): {e}")
            hooks = None
    else:
        hooks = None

    trainer = Trainer(
        model=model, train_loader=train_loader, val_loader=val_loader,
        optimizer=optimizer, scheduler=scheduler, device=cfg.device,
        logger=logger, ckpt_manager=ckpt_manager,
        hooks=hooks,
        gradient_accumulation_steps=cfg.training.gradient_accumulation_steps,
        log_interval=getattr(cfg.logging, 'log_interval', 10),
        save_interval=getattr(cfg.checkpointing, 'save_interval', 1000),
        scaler=scaler,
        clip_grad_norm=getattr(cfg.training, 'clip_grad_norm', 1.0),
        loss_spike_threshold=getattr(cfg.training, 'loss_spike_threshold', 5.0),
        max_consecutive_spikes=getattr(cfg.training, 'max_consecutive_spikes', 5),
        grad_norm_history_size=getattr(cfg.training, 'grad_norm_history_size', 100),
        grad_clip_percentile=getattr(cfg.training, 'grad_clip_percentile', 0.9),
        dynamic_clip_factor=getattr(cfg.training, 'dynamic_clip_factor', 1.5)
    )
    trainer.run(cfg.training.max_epochs, start_epoch)

    if world_size > 1:
        torch.distributed.barrier()

    # --- DDP æ¸…ç† ---
    cleanup_ddp()


if __name__ == "__main__":
    main()
# END OF FILE: pretrain/train.py